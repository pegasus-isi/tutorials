<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial">
  <title>Tutorial</title>

  <section id="tutorial_introduction">
    <title>Getting Started</title>

    <para>This tutorial will take you through the steps of running simple
    workflows using Pegasus Workflow Management System that are executed on
    the OLCF Summit system. In this tutorial, we will walk through setting up
    a workflow submit node for Pegasus in the OLCF Kubernetes cluster and then
    submit workflows from their to SUMMIT.</para>

    <para> Pegasus allows scientists to</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Automate</emphasis> their scientific
        computational work, as portable workflows. Pegasus enables scientists
        to construct workflows in abstract terms without worrying about the
        details of the underlying execution environment or the particulars of
        the low-level specifications required by the middleware (Condor,
        Globus, or Amazon EC2). It automatically locates the necessary input
        data and computational resources necessary for workflow execution. It
        cleans up storage as the workflow is executed so that data-intensive
        workflows have enough space to execute on storage-constrained
        resources.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Recover</emphasis> from failures at
        runtime. When errors occur, Pegasus tries to recover when possible by
        retrying tasks, and when all else fails, provides a rescue workflow
        containing a description of only the work that remains to be done. It
        also enables users to move computations from one resource to another.
        Pegasus keeps track of what has been done (provenance) including the
        locations of data used and produced, and which software was used with
        which parameters.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Debug</emphasis> failures in their
        computations using a set of system provided debugging tools and an
        online workflow monitoring dashboard.</para>
      </listitem>
    </orderedlist>

    <para>This tutorial is intended for new users who want to get a quick
    overview of Pegasus concepts and usage. The accompanying tutorial VM comes
    pre-configured to run the example workflows. The instructions listed here
    refer mainly to the population modelling workflow example. The tutorial
    covers</para>

    <itemizedlist>
      <listitem>
        <para>submission of an already generated example workflow with
        Pegasus.</para>
      </listitem>

      <listitem>
        <para>the command line tools for monitoring, debugging and generating
        statistics.</para>
      </listitem>

      <listitem>
        <para>creation of workflow using system provided API</para>
      </listitem>

      <listitem>
        <para>information catalogs configuration.</para>
      </listitem>

      <listitem>
        <para>cluster short running tasks</para>
      </listitem>

      <listitem>
        <para>recovery from failures</para>
      </listitem>
    </itemizedlist>

    <para>More information about the topics covered in this tutorial can be
    found in later chapters of this user's guide.</para>

    <para>All of the steps in this tutorial are performed on the command-line.
    The convention we will use for command-line input and output is to put
    things that you should type in bold, monospace font, and to put the output
    you should get in a normal weight, monospace font, like this:</para>

    <programlisting>[user@host dir]$ <emphasis role="bold">you type this</emphasis>
you get this</programlisting>

    <para>Where <literal>[user@host dir]$</literal> is the terminal prompt,
    the text you should type is “<literal>you type this</literal>”, and the
    output you should get is "<literal>you get this</literal>". The terminal
    prompt will be abbreviated as <literal>$</literal>. Because some of the
    outputs are long, we don’t always include everything. Where the output is
    truncated we will add an ellipsis '...' to indicate the omitted
    output.</para>

    <para><emphasis role="bold">If you are having trouble with this tutorial,
    or anything else related to Pegasus, you can contact the Pegasus Users
    mailing list at <email>pegasus-users@isi.edu</email> to get help.
    </emphasis></para>
  </section>

  <section id="tutorial_prereq">
    <title>Prerequisites</title>

    <para>Before you start this tutorial, you need to make sure of the
    following.</para>

    <orderedlist>
      <listitem>
        <para>You have a user account at OLCF with access to SUMMIT.</para>
      </listitem>

      <listitem>
        <para>A working RSA Token to access OLCF's systems</para>
      </listitem>

      <listitem>
        <para>You have an automation user associated with your user account
        that can login to Kubernetes Marble Cluster.</para>
      </listitem>

      <listitem>
        <para>Openshift's origin client
        https://github.com/openshift/origin/releases </para>
      </listitem>

      <listitem>
        <para>Allocation on OLCF's Kubernetes Cluster</para>
      </listitem>
    </orderedlist>
  </section>

  <section id="tutorial_deployment">
    <title>Deployment Overview</title>

    <para>As part of this tutorial, you will prepare a container that can run
    on OLCF's Kubernetes infrastructure. The associated GitHUB repo
    (https://github.com/pegasus-isi/pegasus-olcf-kubernetes) provides yaml pod
    specification templates, that can be used to spawn pods that mount OLCF's
    GPFS filesystem and provide access to the batch schedulers of Summit, RHEA
    and the DTN. The container spawned is a fully functional Pegasus WMS
    workflow submit node that has Pegasus and HTCondor + BOSCO
    installed.</para>

    <figure>
      <title>Pegasus Deployment in OLCF Kubernetes </title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="70%"
                     fileref="images/pegasus-kubernetes-deployment.png"/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section id="tutorial_setup">
    <title>Setup</title>

    <para>Open a command line terminal on your laptop/desktop and clone the
    GitHUB repo.</para>

    <programlisting><emphasis role="bold">$ git clone https://github.com/pegasus-isi/pegasus-olcf-kubernetes.git</emphasis>
Cloning into 'pegasus-olcf-kubernetes'...
remote: Enumerating objects: 3, done.
remote: Counting objects: 100% (3/3), done.
remote: Compressing objects: 100% (3/3), done.
remote: Total 102 (delta 0), reused 1 (delta 0), pack-reused 99
Receiving objects: 100% (102/102), 167.86 KiB | 4.66 MiB/s, done.
Resolving deltas: 100% (30/30), don</programlisting>

    <para>In this repository you will find following bash script and
    files</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">bootstrap.sh </emphasis>- This script
        generates the personalized Dockerfile and Kubernetes pod and service
        specifications for your deployment. It updates the template files with
        your automation user acount details, and saves them under the Docker
        and the Specs folders.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Docker/Dockerfile</emphasis> - Dockerfile.
        used to prepare a container with Pegasus and Condor, ommiting Pegasus'
        R support.</para>
      </listitem>

      <listitem>
        <para><emphasis
        role="bold">Specs/pegasus-submit-service.yml</emphasis> - Contains
        Kubernetes service specification that can be used to spawn a Nodeport
        service that exposes the HTCondor Gridmanager Service running in your
        submit pod, to outside world.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Specs/pegasus-submit-pod.yml</emphasis> -
        Contains Kubernetes pod specification that can be used to spawn a
        pegasus/condor pod that has access to Summits's GPFS filesystem and
        its batch scheduler.</para>
      </listitem>
    </itemizedlist>

    <para>You now need to execute a series of steps. It is important to
    execute them in order. If you make a mistake at any point, we recommend
    starting afresh from step.</para>

    <orderedlist>
      <listitem>
        <para>Update bootstrap.sh</para>

        <para>In bootstrap.sh update the section "ENV Variables For User and
        Group" with your automation user's name, id, group name, group id and
        the Gridmanager Service Port, which must be in the range 30000-32767.
        More specifically replace: </para>

        <itemizedlist>
          <listitem>
            <para>USER, with the username of your automation user (eg.
            csc001_auser) </para>
          </listitem>

          <listitem>
            <para>USER_ID, with the user id of your automation user (eg.
            20001) </para>
          </listitem>

          <listitem>
            <para>USER_GROUP, with the group name your automation user belongs
            to (eg. csc001) </para>
          </listitem>

          <listitem>
            <para>USER_GROUP_ID, with the group id your automation user
            belongs to (eg. 10001) </para>
          </listitem>

          <listitem>
            <para>GRIDMANAGER_SERVICE_PORT, with the Kubernetes Nodeport port
            number the Gridmanager Service should use (eg. 32752)</para>
          </listitem>
        </itemizedlist>
      </listitem>

      <listitem>
        <para>Run bootstrap.sh</para>

        <para>This generates the Dockerfile and the Spec files for your
        deployment.</para>

        <programlisting><emphasis role="bold">$ bash bootstrap.sh</emphasis></programlisting>
      </listitem>

      <listitem>
        <para>Login to OLCF Marble Kubernetes Cluster</para>

        <programlisting><emphasis role="bold">$ oc login -u YOUR_USERNAME https://marble.ccs.ornl.gov/

</emphasis>Username: olcf_user
Password: 
Login successful.

You have one project on this server: "csc001"

Using project "csc001".
</programlisting>

        <note>
          <para>You are logging in as your user . Not as the automation user
          in this step</para>
        </note>
      </listitem>

      <listitem>
        <para>Create a new build and build the image</para>

        <programlisting>$ oc new-build --name=pegasus-olcf -D - &lt; Docker/Dockerfile

--&gt; Found image 9f38484 (7 months old) in image stream "cscXXX/centos" under tag "centos7" for "centos:centos7"

    * A Docker build using a predefined Dockerfile will be created
      * The resulting image will be pushed to image stream tag "pegasus-olcf:latest"
      * Use 'start-build' to trigger a new build

--&gt; Creating resources with label build=pegasus-olcf ...
    buildconfig.build.openshift.io "pegasus-olcf" created
--&gt; Success
</programlisting>

        <para>You can trace the log of the build by running: </para>

        <programlisting>$ oc logs -f build/pegasus-olcf-1

...
Step 27/30 : RUN echo "source /opt/bosco/bosco_setenv" &gt;&gt; ${HOME}/.bashrc
 ---&gt; Using cache
 ---&gt; 7f7aa07a8aac
Step 28/30 : ENTRYPOINT /opt/entrypoint.sh
 ---&gt; Using cache
 ---&gt; 891b14ab3533
Step 29/30 : ENV "OPENSHIFT_BUILD_NAME" "pegasus-olcf-1" "OPENSHIFT_BUILD_NAMESPACE" "cscXXX"
 ---&gt; Using cache
 ---&gt; 95a163ea8f5e
Step 30/30 : LABEL "io.openshift.build.name" "pegasus-olcf-1" "io.openshift.build.namespace" "cscXXX"
 ---&gt; Using cache
 ---&gt; ed0f4341ff43
Successfully built ed0f4341ff43
Pushing image docker-registry.default.svc:5000/cscXXX/pegasus-olcf:latest ...
Pushed 2/14 layers, 14% complete
Pushed 3/14 layers, 21% complete
Pushed 4/14 layers, 29% complete
Pushed 5/14 layers, 36% complete
Pushed 6/14 layers, 43% complete
Pushed 7/14 layers, 50% complete
Pushed 8/14 layers, 57% complete
Pushed 9/14 layers, 64% complete
Pushed 10/14 layers, 71% complete
Pushed 11/14 layers, 79% complete
Pushed 12/14 layers, 86% complete
Pushed 13/14 layers, 93% complete
Pushed 14/14 layers, 100% complete
Push successful


</programlisting>
      </listitem>

      <listitem>
        <para>In case you have to update the Docker file after creating the
        image. You can start a new build using the command</para>

        <programlisting>$oc start-build pegasus-olcf --from-file=Docker/Dockerfile</programlisting>
      </listitem>

      <listitem>
        <para>Start a Kubernetes Service that will expose your pod
        services</para>

        <programlisting>$ oc create -f Specs/pegasus-submit-service.yml
service/pegasus-submit-service created</programlisting>
      </listitem>

      <listitem>
        <para>Start a Kubernetes pod with batch job submission
        capabilities</para>

        <programlisting>$ oc create -f Specs/pegasus-submit-pod.yml

pod/pegasus-submit created</programlisting>

        <para>Wait a couple of minutes for the POD to spin up</para>
      </listitem>

      <listitem>
        <para>Logon to the POD and get an interactive shell</para>

        <programlisting>$  oc exec -it pegasus-submit /bin/bash
[csc001_auser@pegasus-submit /]$ 
</programlisting>
      </listitem>

      <listitem>
        <para>Configuring for batch submissions </para>

        <para>Execute this step in the shell you got in Step 8. All tutorial
        exercises are also done in this shell, </para>

        <programlisting>$ bash /opt/remote_bosco_setup.sh
rm: cannot remove '/home/cscXXX_auser/.bosco/.clusterlist': No such file or directory
... 
****************************************************************************
scp: /tmp/tmp_bosco_probe.5jcAPMLF/bosco_run: No such file or directory
****************************************************************************
                          NOTICE TO USERS

This is a Federal computer system and is the property of the United States
Government.  It is for authorized use only.  Users (authorized or
unauthorized) have no explicit or implicit expectation of privacy.

Any or all uses of this system and all files on this system may be
intercepted, monitored, recorded, copied, audited, inspected, and disclosed
to authorized site, Department of Energy, and law enforcement personnel, as
well as authorized officials of other agencies, both domestic and foreign.
By using this system, the user consents to such interception, monitoring,
recording, copying, auditing, inspection, and disclosure at the discretion
of authorized site or Department of Energy personnel.

Unauthorized or improper use of this system may result in administrative
disciplinary action and civil and criminal penalties.  By continuing to use
this system you indicate your awareness of and consent to these terms and
conditions of use.  LOG OFF IMMEDIATELY if you do not agree to the
conditions stated in this warning.
****************************************************************************
scp: /tmp/tmp_bosco_probe.iStWyBpW/bosco_run: No such file or directory
Downloading for csc001_auser@dtn.ccs.ornl.gov..........


ls: cannot access /tmp/tmp.ED9ZfqKl/condor*: No such file or directory
Unable to download and prepare BOSCO for remote installation.
Download URL: ftp://ftp.cs.wisc.edu/condor/bosco/1.2/bosco-1.2-x86_64_RedHat7.tar.gz
Aborting installation to cscXXX_auser@dtn.ccs.ornl.gov.
Installing lsf_local_submit_attributes.sh
Installing lsf_status.sh
Installing moab_cancel.sh
Installing moab_hold.sh
Installing moab_local_submit_attributes.sh
Installing moab_resume.sh
Installing moab_status.sh
Installing moab_submit.sh
Installing pbs_local_submit_attributes.sh
Installing sge_local_submit_attributes.sh
Installing slurm_local_submit_attributes.sh
Installing lsf_status.py
Adding moab support to batch_gahp.config
slurm_cancel.sh
slurm_cluster.patch
slurm_hold.sh
slurm_resume.sh
slurm_status.py
slurm_status.sh
slurm_submit.sh
</programlisting>
      </listitem>
    </orderedlist>

    <para>You can now check the status of everything on a terminal on your
    laptop by </para>

    <programlisting>$oc status
In project cscXXX on server https://marble.ccs.ornl.gov:443

svc/pegasus-submit-service (all nodes):32753 -&gt; 11000
  pod/pegasus-submit runs docker-registry.default.svc:5000/cscXXX/pegasus-olcf:latest

bc/pegasus-olcf docker builds Dockerfile on istag/centos:centos7
  -&gt; istag/pegasus-olcf:latest
  build #1 succeeded 15 minutes ago


1 info identified, use 'oc status --suggest' to see details.
</programlisting>

    <para>In case there is a mistake in your configuration and you need to
    restart. Here are some helpful commands to delete.</para>

    <para>In order to delete the pod, exit the interactive shell by typing
    "exit" and then use the following command.</para>

    <programlisting>$  oc delete pod pegasus-submit
</programlisting>

    <para>To delete the service use:</para>

    <programlisting>$ oc delete svc pegasus-submit-service</programlisting>

    <para>To delete the container image</para>

    <programlisting>$  oc delete bc/pegasus-olcf
</programlisting>
  </section>

  <section id="tutorial_scientific_workflows">
    <title>What are Scientific Workflows?</title>

    <para>Scientific workflows allow users to easily express multi-step
    computational tasks, for example retrieve data from an instrument or a
    database, reformat the data, and run an analysis. A scientific workflow
    describes the dependencies between the tasks and in most cases the
    workflow is described as a directed acyclic graph (DAG), where the nodes
    are tasks and the edges denote the task dependencies. A defining property
    for a scientific workflow is that it manages data flow. The tasks in a
    scientific workflow can be everything from short serial tasks to very
    large parallel tasks (MPI for example) surrounded by a large number of
    small, serial tasks used for pre- and post-processing.</para>

    <para>Workflows can vary from simple to complex. Below are some examples.
    In the figures below, the task are designated by circles/ellipses while
    the files created by the tasks are indicated by rectangles. Arrows
    indicate task dependencies.</para>

    <para><emphasis role="bold">Process Workflow</emphasis></para>

    <para>It consists of a single task that runs the <literal>ls</literal>
    command and generates a listing of the files in the `/` directory.</para>

    <figure>
      <title>Process Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="20%"
                     fileref="images/tutorial-single-job-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para><emphasis role="bold">Pipeline of Tasks</emphasis></para>

    <para>The pipeline workflow consists of two tasks linked together in a
    pipeline. The first job runs the `curl` command to fetch the Pegasus home
    page and store it as an HTML file. The result is passed to the `wc`
    command, which counts the number of lines in the HTML file. <figure>
        <title>Pipeline of Tasks</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="25%"
                       fileref="images/tutorial-pipeline-tasks-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Split Workflow</emphasis></para>

    <para>The split workflow downloads the Pegasus home page using the `curl`
    command, then uses the `split` command to divide it into 4 pieces. The
    result is passed to the `wc` command to count the number of lines in each
    piece.<figure>
        <title>Split Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/tutorial-split-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Merge Workflow</emphasis></para>

    <para>The merge workflow runs the `ls` command on several */bin
    directories and passes the results to the `cat` command, which merges the
    files into a single listing. The merge workflow is an example of a
    parameter sweep over arguments.<figure>
        <title>Merge Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/tutorial-merge-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Diamond Workflow</emphasis></para>

    <para>The diamond workflow runs combines the split and merge workflow
    patterns to create a more complex workflow.</para>

    <figure>
      <title>Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="70%"
                     fileref="images/tutorial-diamond-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para><emphasis role="bold">Complex Workflows</emphasis></para>

    <para>The above examples can be used as building blocks for much complex
    workflows. Some of these are showcased on the <ulink
    url="https://pegasus.isi.edu/applications">Pegasus Applications
    page</ulink>.</para>
  </section>

  <section id="tutorial_submitting_wf">
    <title>Submitting Our First Workflow</title>

    <para>All of the example workflows described in the previous section can
    be generated with the <literal>pegasus-init</literal> command. For this
    tutorial we will be using the split workflow, which can be created like
    this:</para>

    <para>Please replace XXX in cscXXX with your project number.</para>

    <programlisting>$ <emphasis role="bold">mkdir cd /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial</emphasis>
$ <emphasis role="bold">cd $WORK/tutorial</emphasis>
$ <emphasis role="bold">pegasus-init diamond</emphasis> 
pegasus-init diamond
Do you want to generate a tutorial workflow? (y/n) [n]: y
1: Local Machine Condor Pool
2: USC HPCC Cluster
3: OSG from ISI submit node
4: XSEDE, with Bosco
5: Bluewaters, with Glite
6: TACC Wrangler with Glite
7: OLCF TITAN with Glite
<emphasis role="bold">8: OLCF Summit from Kubernetes using BOSCO</emphasis>
What environment is tutorial to be setup for? (1-8) [1]: 8
What project your jobs should run under. For example on TACC there are like : TG-DDM160003 ?: CSCXXX
1: Process
2: Pipeline
3: Split
4: Merge
5: EPA (requires R)
6: Population Modeling using Containers
<emphasis role="bold">7: Diamond</emphasis>
8: MPI Hello World
What tutorial workflow do you want? (1-8) [1]: 7
<emphasis role="bold">Pegasus Tutorial setup for example workflow - diamond for execution on summit-kub-bosco in directory /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond
$ </emphasis>cd diamond
$ <emphasis role="bold">ls</emphasis>
bin  daxgen.py  generate_dax.sh  input  output  pegasus.properties  plan_cluster_dax.sh 
 plan_dax.sh  rc.txt  README.md  sites.xml  tc.txt
</programlisting>

    <tip>
      <para>The <literal>pegasus-init</literal> tool can be used to generate
      workflow skeletons from templates by asking the user questions. It is
      easier to use pegasus-init than to start a new workflow from
      scratch.</para>
    </tip>

    <para>The diamond workflow looks like this:</para>

    <figure>
      <title>Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="70%"
                     fileref="images/tutorial-diamond-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The input workflow description for Pegasus is called the DAX. It can
    be generated by running the <filename>generate_dax.sh</filename> script
    from the split directory, like this:</para>

    <programlisting>$ <emphasis role="bold">./generate_dax.sh diamond.dax</emphasis>
Generated dax diamond.dax
    </programlisting>

    <para>This script will run a small Python program
    (<filename>daxgen.py</filename>) that generates a file with a .dax
    extension using the Pegasus Python API. We will cover the details of
    creating a DAX programmatically <link
    linkend="tutorial_wf_generation">later in the tutorial</link>. Pegasus
    reads the DAX and generates an executable HTCondor workflow that is run on
    an execution site.</para>

    <para>The <literal>pegasus-plan</literal> command is used to submit the
    workflow through Pegasus. The pegasus-plan command reads the input
    workflow (DAX file specified by --dax option), maps the abstract DAX to
    one or more execution sites, and submits the generated executable workflow
    to HTCondor. Among other things, the options to pegasus-plan tell
    Pegasus</para>

    <itemizedlist>
      <listitem>
        <para>the workflow to run</para>
      </listitem>

      <listitem>
        <para>where (what site) to run the workflow</para>
      </listitem>

      <listitem>
        <para>the input directory where the inputs are placed</para>
      </listitem>

      <listitem>
        <para>the output directory where the outputs are placed</para>
      </listitem>
    </itemizedlist>

    <para>By default, the workflow is setup to run on the compute sites (i.e
    sites with handle other than "local") defined in the sites.xml file. In
    our example, the workflow will run on a site named "summit" in the
    sites.xml file.</para>

    <para>To plan the split workflow invoke the pegasus-plan command using the
    <filename>plan_dax.sh</filename> wrapper script as follows:</para>

    <programlisting>$ <emphasis role="bold">./plan_dax.sh diamond.dax</emphasis>
2019.10.24 23:50:23.203 UTC:    
2019.10.24 23:50:23.208 UTC:   ----------------------------------------------------------------------- 
2019.10.24 23:50:23.213 UTC:   File for submitting this DAG to HTCondor           : diamond-0.dag.condor.sub 
2019.10.24 23:50:23.219 UTC:   Log of DAGMan debugging messages                 : diamond-0.dag.dagman.out 
2019.10.24 23:50:23.224 UTC:   Log of HTCondor library output                     : diamond-0.dag.lib.out 
2019.10.24 23:50:23.229 UTC:   Log of HTCondor library error messages             : diamond-0.dag.lib.err 
2019.10.24 23:50:23.234 UTC:   Log of the life of condor_dagman itself          : diamond-0.dag.dagman.log 
2019.10.24 23:50:23.240 UTC:    
2019.10.24 23:50:23.245 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: 
2019.10.24 23:50:23.255 UTC:   ----------------------------------------------------------------------- 
2019.10.24 23:50:23.711 UTC:   Created Pegasus database in: sqlite:////home/cscXXX_auser/.pegasus/workflow.db 
2019.10.24 23:50:23.716 UTC:   Your database is compatible with Pegasus version: 4.9.3dev 
2019.10.24 23:50:23.768 UTC:   Submitting to condor diamond-0.dag.condor.sub 
2019.10.24 23:50:23.782 UTC:   Submitting job(s). 
2019.10.24 23:50:23.787 UTC:   1 job(s) submitted to cluster 1. 
2019.10.24 23:50:23.792 UTC:    
2019.10.24 23:50:23.798 UTC:   Your workflow has been started and is running in the base directory: 
2019.10.24 23:50:23.803 UTC:    
2019.10.24 23:50:23.808 UTC:     /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0001 
2019.10.24 23:50:23.813 UTC:    
2019.10.24 23:50:23.818 UTC:   *** To monitor the workflow you can run *** 
2019.10.24 23:50:23.824 UTC:    
2019.10.24 23:50:23.829 UTC:     pegasus-status -l /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0001 
2019.10.24 23:50:23.834 UTC:    
2019.10.24 23:50:23.839 UTC:   *** To remove your workflow run *** 
2019.10.24 23:50:23.844 UTC:    
2019.10.24 23:50:23.850 UTC:     pegasus-remove /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0001 
2019.10.24 23:50:23.855 UTC:    
2019.10.24 23:50:24.828 UTC:   Time taken to execute is 1.172 seconds 
</programlisting>

    <note>
      <para>The line in the output that starts with
      <literal>pegasus-status</literal>, contains the command you can use to
      monitor the status of the workflow. The path it contains is the path to
      the submit directory where all of the files required to submit and
      monitor the workflow are stored.</para>
    </note>

    <para>This is what the diamond workflow looks like after Pegasus has
    finished planning the DAX:</para>

    <figure>
      <title>DIAMOND DAG</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/tutorial-blackdiamond-dag.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>For this workflow the only jobs Pegasus needs to add are a directory
    creation job, a stage-in job , and stage-out jobs (for the analyze job).
    The cleanup job removes data from the scratch directory.</para>
  </section>

  <section id="tutorial_monitoring_cmd_tools">
    <title>Command line tools for Monitoring and Debugging</title>

    <para>Pegasus also comes with a series of command line tools that users
    can use to monitor and debug their workflows.</para>

    <itemizedlist>
      <listitem>
        <para>pegasus-status : monitor the status of the workflow</para>
      </listitem>

      <listitem>
        <para>pegasus-analyzer : debug a failed workflow</para>
      </listitem>

      <listitem>
        <para>pegasus-statistics : generate statistics from a workflow
        run.</para>
      </listitem>
    </itemizedlist>

    <para>We will run pegasus-statistics in a later section.</para>

    <section>
      <title>pegasus-status - monitoring the workflow</title>

      <para>After the workflow has been submitted you can monitor it using the
      <literal>pegasus-status</literal> command:</para>

      <programlisting><emphasis role="bold">$ </emphasis> <emphasis
          role="bold">pegasus-status -l submit/cscXXX_auser/pegasus/diamond/run0001/</emphasis>
STAT  IN_STATE  JOB                                                                                                                
Run      00:53  diamond-0 ( /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0001 )
Idle     00:19   \_preprocess_ID0000001                                                                                            
Summary: 2 Condor jobs total (I:1 R:1)

UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    5     0     0     1     0     2     0  25.0 Running *diamond-0.dag                          
Summary: 1 DAG total (Running:1)


</programlisting>

      <para>This command shows the workflow (population-0) and the running
      jobs . It also gives statistics on the number of jobs in each state and
      the percentage of the jobs in the workflow that have finished
      successfully.</para>

      <para>Use the <literal>watch</literal> option to continuously monitor
      the workflow:</para>

      <programlisting>$ <emphasis role="bold">pegasus-status -w $<emphasis
            role="bold">submit/cscXXX_auser/pegasus/diamond/run0001/</emphasis></emphasis>
...</programlisting>

      <programlisting>(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    0     0     0     0     0     8     0 100.0 Success *diamond-0.dag                          
Summary: 1 DAG total (Success:1)
      </programlisting>

      <para>That means the workflow is finished successfully.</para>
    </section>

    <section>
      <title>pegasus-analyzer - debug a failed workflow</title>

      <para>In our case one or more jobs will fail, and the output of the
      <literal>pegasus-status</literal> command above will have a non-zero
      value in the <literal>FAILURE</literal> column.</para>

      <programlisting>$<emphasis role="bold"> pegasus-status -l $WORK/tutorial/diamond/submit/trainXX/pegasus/diamond/run0002
</emphasis>(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    6     0     0     0     0     1     1  12.5 Failure *diamond-0.dag                          
Summary: 1 DAG total (Failure:1)
</programlisting>

      <para>You can debug the failure using the
      <literal>pegasus-analyzer</literal> command. This command will identify
      the jobs that failed and show their output.</para>

      <programlisting><emphasis role="bold">$ </emphasis> <emphasis
          role="bold">pegasus-analyzer submit/cscXXX_auser/pegasus/diamond/run0002
</emphasis>
************************************Summary*************************************

 Submit Directory   : submit/cscXXX_auser/pegasus/diamond/run0002
 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      1 (12.50%)
 # jobs failed      :      1 (12.50%)
 # jobs held        :      0 (0.00%)
 # jobs unsubmitted :      6 (75.00%)

******************************Failed jobs' details******************************

===========================stage_in_local_summit_0_0============================

 last state: POST_SCRIPT_FAILED
       site: local
submit file: 00/00/stage_in_local_summit_0_0.sub
output file: 00/00/stage_in_local_summit_0_0.out.001
 error file: 00/00/stage_in_local_summit_0_0.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : pegasus-submit
executable  : /opt/pegasus-4.9.3dev/bin/pegasus-transfer
arguments   :   --threads   2   -m   1  
exitcode    : 1
working dir : /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0002

------------------Task #1 - pegasus::transfer - None - stdout-------------------

2019-10-24 23:56:31,654    INFO:  Reading URL pairs from stdin
2019-10-24 23:56:31,655    INFO:  1 transfers loaded
2019-10-24 23:56:31,655    INFO:  PATH=/usr/bin:/bin
2019-10-24 23:56:31,655    INFO:  LD_LIBRARY_PATH=
2019-10-24 23:56:31,681    INFO:  --------------------------------------------------------------------------------
2019-10-24 23:56:31,681    INFO:  Starting transfers - attempt 1
2019-10-24 23:56:33,684   ERROR:  Expected local file does not exist: /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/input/f.a
2019-10-24 23:56:33,685    INFO:  --------------------------------------------------------------------------------
2019-10-24 23:56:33,685    INFO:  Stats: Total 1 transfers, 0.0 B transferred in 2 seconds. Rate: 0.0 B/s (0.0 b/s)
2019-10-24 23:56:33,685    INFO:         Between sites local-&gt;summit : 1 transfers, 0.0 B transferred in 2 seconds. Rate: 0.0 B/s (0.0 b/s)
2019-10-24 23:56:33,685 CRITICAL:  Some transfers failed! See above, and possibly stderr.



</programlisting>

      <para>In this example, we removed one of the input files. We will cover
      this in more detail in the recovery section. The output of
      pegasus-analyzer indicates that f.a could not be found.</para>
    </section>

    <section id="tutorial_statistics">
      <title>Collecting statistics about a workflow run</title>

      <para>The <literal>pegasus-statistics</literal> command can be used to
      gather statistics about the runtime of the workflow and its jobs. The
      <literal>-s all</literal> argument tells the program to generate all
      statistics it knows how to ca</para>

      <programlisting><emphasis role="bold">$ pegasus-statistics -s all submit/cscXXX_auser/pegasus/diamond/run0001
</emphasis>
#
# Pegasus Workflow Management System - http://pegasus.isi.edu
#
# Workflow summary:
#   Summary of the workflow execution. It shows total
#   tasks/jobs/sub workflows run, how many succeeded/failed etc.
#   In case of hierarchical workflow the calculation shows the
#   statistics across all the sub workflows.It shows the following
#   statistics about tasks, jobs and sub workflows.
#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.
#     * Failed - total count of failed tasks/jobs/sub workflows.
#     * Incomplete - total count of tasks/jobs/sub workflows that are
#       not in succeeded or failed state. This includes all the jobs
#       that are not submitted, submitted but not completed etc. This
#       is calculated as  difference between 'total' count and sum of
#       'succeeded' and 'failed' count.
#     * Total - total count of tasks/jobs/sub workflows.
#     * Retries - total retry count of tasks/jobs/sub workflows.
#     * Total+Retries - total count of tasks/jobs/sub workflows executed
#       during workflow run. This is the cumulative of retries,
#       succeeded and failed count.
# Workflow wall time:
#   The wall time from the start of the workflow execution to the end as
#   reported by the DAGMAN.In case of rescue dag the value is the
#   cumulative of all retries.
# Cumulative job wall time:
#   The sum of the wall time of all jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job wall time as seen from submit side:
#   The sum of the wall time of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
# Cumulative job badput wall time:
#   The sum of the wall time of all failed jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job badput wall time as seen from submit side:
#   The sum of the wall time of all failed jobs as reported by DAGMan.
#   This is similar to the regular cumulative job badput wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
------------------------------------------------------------------------------
Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries
Tasks          4         0       0           4         0         4            
Jobs           8         0       0           8         0         8            
Sub-Workflows  0         0       0           0         0         0            
------------------------------------------------------------------------------

Workflow wall time                                       : 8 mins, 40 secs
Cumulative job wall time                                 : 2 mins, 8 secs
Cumulative job wall time as seen from submit side        : 4 mins, 44 secs
Cumulative job badput wall time                          : 0.0 secs
Cumulative job badput wall time as seen from submit side : 0.0 secs

# Integrity Metrics
# Number of files for which checksums were compared/computed along with total time spent doing it. 
1 files checksums generated with total duration of 0.012 secs

# Integrity Errors
# Total:    
#       Total number of integrity errors encountered across all job executions(including retries) of a workflow. 
# Failures: 
#       Number of failed jobs where the last job instance had integrity errors.
Failures: 0 job failures had integrity errors

Summary                       : submit/cscXXX_auser/pegasus/diamond/run0001/statistics/summary.txt
Workflow execution statistics : submit/cscXXX_auser/pegasus/diamond/run0001/statistics/workflow.txt
Job instance statistics       : submit/cscXXX_auser/pegasus/diamond/run0001/statistics/jobs.txt
Transformation statistics     : submit/cscXXX_auser/pegasus/diamond/run0001/statistics/breakdown.txt
Integrity statistics          : submit/cscXXX_auser/pegasus/diamond/run0001/statistics/integrity.txt
Time statistics               : submit/cscXXX_auser/pegasus/diamond/run0001/statistics/time.txt
[
</programlisting>

      <para>The output of <literal>pegasus-statistics</literal> contains many
      definitions to help users understand what all of the values reported
      mean. Among these are the total wall time of the workflow, which is the
      time from when the workflow was submitted until it finished, and the
      total cumulative job wall time, which is the sum of the runtimes of all
      the jobs.</para>

      <para>The <literal>pegasus-statistics</literal> command also writes out
      several reports in the <filename>statistics</filename> subdirectory of
      the workflow submit directory:</para>

      <programlisting>$ <emphasis role="bold">ls </emphasis>submit/cscXXX_auser/pegasus/diamond/run0001/statistics/
jobs.txt          summary.txt         time.txt          breakdown.txt          workflow.txt</programlisting>

      <para>The file <filename>breakdown.txt</filename>, for example, has min,
      max, and mean runtimes for each transformation:</para>

      <programlisting>$ <emphasis role="bold">more </emphasis> <emphasis
          role="bold">submit/cscXXX_auser/pegasus/diamond/run0001/statistics/breakdown.txt </emphasis>

# Transformation - name of the transformation.
# Count          - the number of times the invocations corresponding to
#                  the transformation was executed.
# Succeeded      - the count of the succeeded invocations corresponding
#                  to the transformation.
# Failed         - the count of the failed invocations corresponding to
#                  the transformation.
# Min(sec)       - the minimum invocation runtime value corresponding
#                  to the transformation.
# Max(sec)       - the maximum invocation runtime value corresponding
#                  to the transformation.
# Mean(sec)      - the mean of the invocation runtime corresponding
#                  to the transformation.
# Total(sec)     - the cumulative of invocation runtime corresponding
#                  to the transformation.

# ac57861f-be59-4241-8e28-a06f217d2332 (diamond)
Transformation           Count     Succeeded Failed  Min       Max       Mean      Total     
analyze                  1         1         0       30.078    30.078    30.078    30.078    
dagman::post             8         8         0       0.0       0.0       0.0       0.0       
findrange                2         2         0       30.048    30.091    30.069    60.139    
pegasus::cleanup         1         1         0       2.151     2.151     2.151     2.151     
pegasus::dirmanager      1         1         0       2.095     2.095     2.095     2.095     
pegasus::transfer        2         2         0       2.114     2.16      2.137     4.274     
preprocess               1         1         0       30.123    30.123    30.123    30.123    


# All (All)
Transformation           Count     Succeeded Failed  Min       Max       Mean      Total     
analyze                  1         1         0       30.078    30.078    30.078    30.078    
dagman::post             8         8         0       0.0       0.0       0.0       0.0       
findrange                2         2         0       30.048    30.091    30.069    60.139    
pegasus::cleanup         1         1         0       2.151     2.151     2.151     2.151     
pegasus::dirmanager      1         1         0       2.095     2.095     2.095     2.095     
pegasus::transfer        2         2         0       2.114     2.16      2.137     4.274     
preprocess               1         1         0       30.123    30.123    30.123    30.123    
</programlisting>
    </section>
  </section>

  <section id="tutorial_wf_generation">
    <title>Generating the Workflow</title>

    <para>The example that you ran earlier already had the workflow
    description (diamond.dax) generated. Pegasus reads workflow descriptions
    from DAX files. The term "DAX" is short for "Directed Acyclic Graph in
    XML". DAX is an XML file format that has syntax for expressing jobs,
    arguments, files, and dependencies. We now will be creating the split
    workflow that we just ran using the Pegasus provided DAX API:</para>

    <figure>
      <title>Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="70%"
                     fileref="images/tutorial-diamond-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In this diagram, the ovals represent computational jobs, the
    dog-eared squares are files, and the arrows are dependencies.</para>

    <para>In order to create a DAX it is necessary to write code for a DAX
    generator. Pegasus comes with Perl, Java, and Python libraries for writing
    DAX generators. In this tutorial we will show how to use the Python
    library.</para>

    <para>The DAX generator for the diamond workflow is in the file
    <filename>daxgen.py</filename>. Look at the file by typing:</para>

    <programlisting>$ <emphasis role="bold">more daxgen.py</emphasis>
...</programlisting>

    <tip>
      <para>We will be using the <literal>more</literal> command to inspect
      several files in this tutorial. <literal>more</literal> is a pager
      application, meaning that it splits text files into pages and displays
      the pages one at a time. You can view the next page of a file by
      pressing the spacebar. Type 'h' to get help on using
      <literal>more</literal>. When you are done, you can type 'q' to close
      the file.</para>
    </tip>

    <para>The code has 3 main sections:</para>

    <orderedlist>
      <listitem>
        <para>A new ADAG object is created. This is the main object to which
        jobs and dependencies are added.</para>

        <programlisting><emphasis role="bold"># Create a abstract dag</emphasis>
print "Creating ADAG..."
diamond = ADAG("diamond")

...
</programlisting>
      </listitem>

      <listitem>
        <para>Jobs and files are added. The 4 jobs in the diagram above are
        added and 6 files are referenced. Arguments are defined using strings
        and File objects. The input and output files are defined for each job.
        This is an important step, as it allows Pegasus to track the files,
        and stage the data if necessary. Workflow outputs are tagged with
        "transfer=true".</para>

        <programlisting><emphasis role="bold"># Add a preprocess job</emphasis>
print "Adding preprocess job..."
preprocess = Job(name="preprocess")
a = File("f.a")
b1 = File("f.b1")
b2 = File("f.b2")
preprocess.addArguments("-i",a,"-o",b1,"-o",b2)
preprocess.uses(a, link=Link.INPUT)
preprocess.uses(b1, link=Link.OUTPUT, transfer=False, register=False)
preprocess.uses(b2, link=Link.OUTPUT, transfer=False, register=False)
preprocess.addProfile(Profile("pegasus", "label", "cluster-1"))
diamond.addJob(preprocess)


...
</programlisting>
      </listitem>

      <listitem>
        <para>Dependencies are added. These are shown as arrows in the diagram
        above. They define the parent/child relationships between the jobs.
        When the workflow is executing, the order in which the jobs will be
        run is determined by the dependencies between them.</para>

        <programlisting><emphasis role="bold"># Add control-flow dependencies</emphasis>
print "Adding control flow dependencies..."
diamond.addDependency(Dependency(parent=preprocess, child=frl))
diamond.addDependency(Dependency(parent=preprocess, child=frr))
diamond.addDependency(Dependency(parent=frl, child=analyze))
diamond.addDependency(Dependency(parent=frr, child=analyze))</programlisting>
      </listitem>
    </orderedlist>

    <para>Generate a DAX file named <filename>diamond.dax</filename> by
    typing:</para>

    <programlisting>$ <emphasis role="bold">./generate_dax.sh </emphasis><emphasis
        role="bold">diamond.dax</emphasis>
Generated dax diamond.dax</programlisting>

    <para>The <filename>diamond.dax</filename> file should contain an XML
    representation of the split workflow. You can inspect it by typing:</para>

    <programlisting>$ <emphasis role="bold">more diamond.dax</emphasis>
...</programlisting>
  </section>

  <section id="tutorial_catalogs">
    <title>Information Catalogs</title>

    <para>The workflow description (DAX) that you specify to Pegasus is
    portable, and usually does not contain any locations to physical input
    files, executables or cluster end points where jobs are executed. Pegasus
    uses three information catalogs during the planning process.</para>

    <figure>
      <title>Information Catalogs used by Pegasus</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/tutorial-pegasus-catalogs.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <section id="tut_site_catalog">
      <title>The Site Catalog</title>

      <para>The site catalog describes the sites where the workflow jobs are
      to be executed. In this tutorial we assume that you have a Personal
      Condor pool running on localhost. If you are using one of the tutorial
      VMs this has already been setup for you. The site catalog for the
      tutorial examples is in <filename>sites.xml</filename>:</para>

      <programlisting>$ <emphasis role="bold">more sites.xml</emphasis>
...


    <emphasis role="bold">&lt;!-- The local site contains information about the submit host --&gt;</emphasis>
    &lt;site handle="local" arch="x86_64" os="LINUX"&gt;
        &lt;!-- This is where intermediate data will be stored --&gt;
        &lt;directory type="shared-scratch" path="/gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/scratch"&gt;
            &lt;file-server operation="all" url="file:///gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/scratch"/&gt;
        &lt;/directory&gt;
        &lt;!-- This is where output data will be stored --&gt;
        &lt;directory type="shared-storage" path="/gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/output"&gt;
            &lt;file-server operation="all" url="file:///gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/output"/&gt;
        &lt;/directory&gt;
    &lt;/site&gt;

    &lt;site handle="summit" arch="x86_64" os="LINUX"&gt;
        <emphasis role="bold">&lt;!-- USE XFER QUEUE ON DTN --&gt;</emphasis>
        &lt;grid type="batch" contact="${USER}@dtn.ccs.ornl.gov" scheduler="lsf" jobtype="auxillary"/&gt;
        <emphasis role="bold">&lt;!-- USE OTHER QUEUES ON DTN --&gt;</emphasis>
        &lt;grid type="batch" contact="${USER}@dtn.ccs.ornl.gov" scheduler="lsf" jobtype="compute"/&gt;

         <emphasis role="bold">&lt;!-- Scratch directory on the cluster --&gt;</emphasis>
        &lt;directory type="shared-scratch" path="/gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/summit/scratch"&gt;
            &lt;file-server operation="all" url="file:///gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/summit/scratch"/&gt;
        &lt;/directory&gt;


        &lt;profile namespace="pegasus" key="style"&gt;ssh&lt;/profile&gt;

        <emphasis role="bold">&lt;!-- This tells glite what batch queue to submit jobs to --&gt;</emphasis>
        &lt;profile namespace="pegasus" key="queue"&gt;batch&lt;/profile&gt;
        
        <emphasis role="bold">&lt;!--- This tells pegasus to have the auxillary jobs run on submit host 
               and not go through the local scheduler queue --&gt;
        </emphasis>&lt;profile namespace="pegasus" key="auxillary.local"&gt;true&lt;/profile&gt;
        
        <emphasis role="bold">&lt;!-- This profile tells Pegasus where the worker package is installed on the site --&gt;
        &lt;!-- Without this, Pegasus will automatically stage a worker package to the site --&gt;
        </emphasis>&lt;profile namespace="env"  key="PEGASUS_HOME"&gt;/ccs/proj/cscXXX/summit/pegasus/stable&lt;/profile&gt;
        &lt;profile namespace="pegasus" key="runtime"&gt;1800&lt;/profile&gt;

        &lt;profile namespace="pegasus" key="change.dir"&gt;true&lt;/profile&gt;

        &lt;profile namespace="pegasus" key="nodes"&gt;1&lt;/profile&gt;
        &lt;profile namespace="pegasus" key="project"&gt;CSCXXX&lt;/profile&gt;
        &lt;profile namespace="pegasus" key="job.aggregator"&gt;mpiexec&lt;/profile&gt;
        &lt;profile namespace="pegasus" key="runtime"&gt;300&lt;/profile&gt;
        <emphasis role="bold">&lt;!-- This profile tells Pegasus to create two clustered jobs
            per level of the workflow, when horizontal clustering is
            enabled --&gt;</emphasis>
	       &lt;profile namespace="pegasus" key="clusters.num" &gt;2&lt;/profile&gt;
         
    &lt;/site&gt;


</programlisting>

      <note>
        <para>By default (unless specified in properties), Pegasus picks ups
        the site catalog from a XML file named sites.xml in the current
        working directory from where pegasus-plan is invoked.</para>
      </note>

      <para>There are two sites defined in the site catalog: "local" and
      "summit". The "local" site is used by Pegasus to learn about the submit
      host where the workflow management system runs. The "summit" site is the
      summit cluster at TACC.</para>

      <orderedlist>
        <listitem>
          <para>The <emphasis role="bold">local</emphasis> site is configured
          with a "storage" file system that is mounted on the submit host
          (indicated by the file:// URL). This file system is where the output
          data from the workflow will be stored. When the workflow is planned
          we will tell Pegasus that the output site is "local".</para>
        </listitem>

        <listitem>
          <para>The <emphasis role="bold">summit</emphasis> site is also
          configured with a "scratch" file system. This file system is where
          the working directory will be created and is set up for a directory
          on GPFS. When we plan the workflow we will tell Pegasus that the
          execution site is "summit".</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title id="tut_xform_catalog">The Transformation Catalog</title>

      <para>The transformation catalog describes all of the executables
      (called "transformations") used by the workflow. This description
      includes the site(s) where they are located, the architecture and
      operating system they are compiled for, and any other information
      required to properly transfer them to the execution site and run
      them.</para>

      <para>For this tutorial, the transformation catalog is in the file
      <filename>tc.txt</filename>:</para>

      <programlisting>$ <emphasis role="bold">more tc.txt</emphasis>
# This is the transformation catalog. It lists information about each of the
# executables that are used by the workflow.

tr preprocess {
    site summit {
        pfn "/gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/bin/preprocess"
        arch "x86_64"
        os "LINUX"
        type "INSTALLED"

    }
}


...</programlisting>

      <note>
        <para>By default (unless specified in properties), Pegasus picks up
        the transformation catalog from a text file named tc.txt in the
        current working directory from where pegasus-plan is invoked.</para>
      </note>

      <para>The <filename>tc.txt</filename> file contains information about
      two transformations: wc, and split. These three transformations are
      referenced in the split DAX. The transformation catalog indicates that
      both transformations are installed on the condorpool site, and are
      compiled for x86_64 Linux.</para>
    </section>

    <section>
      <title id="tut_replica_catalog">The Replica Catalog</title>

      <para><emphasis role="bold">Note:</emphasis> Replica Catalog
      configuration is not required for the tutorial setup. It is only
      required if you want to refer to input files on external servers.</para>

      <para>The example that you ran, was configured with the inputs already
      present on the submit host (where Pegasus is installed) in a directory.
      If you have inputs at external servers, then you can specify the URLs to
      the input files in the Replica Catalog. This catalog tells Pegasus where
      to find each of the input files for the workflow.</para>

      <para>All files in a Pegasus workflow are referred to in the DAX using
      their Logical File Name (LFN). These LFNs are mapped to Physical File
      Names (PFNs) when Pegasus plans the workflow. This level of indirection
      enables Pegasus to map abstract DAXes to different execution sites and
      plan out the required file transfers automatically.</para>

      <para>The Replica Catalog for the diamond workflow is in the
      <filename>rc.txt</filename> file:</para>

      <programlisting>$ <emphasis role="bold">more rc.txt</emphasis>
# This is the replica catalog. It lists information about each of the
# input files used by the workflow.

# The format is:
# LFN     PFN    pool="SITE"
#
# For example:
#data.txt  file:///tmp/data.txt         site="local"
#data.txt  http://example.org/data.txt  site="example"
f.a file:///gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/input/f.a   site="local"</programlisting>

      <note>
        <para>By default (unless specified in properties), Pegasus picks ups
        the transformation catalog from a text file named tc.txt in the
        current working directory from where pegasus-plan is invoked. In our
        tutorial, input files are on the submit host and we used the --input
        dir option to pegasus-plan to specify where they are located.</para>
      </note>

      <para>This replica catalog contains only one entry for the diamond
      workflow’s only input file.</para>
    </section>
  </section>

  <section id="tutorial_configuration">
    <title>Configuring Pegasus</title>

    <para>In addition to the information catalogs, Pegasus takes a
    configuration file that specifies settings that control how it plans the
    workflow.</para>

    <para>For the diamond workflow, the Pegasus configuration file is
    relatively simple. It only contains settings to help Pegasus find the
    information catalogs. These settings are in the
    <filename>pegasus.properties</filename> file:</para>

    <programlisting>$ <emphasis role="bold">more pegasus.properties</emphasis>
# This tells Pegasus where to find the Site Catalog
pegasus.catalog.site.file=sites.xml

# This tells Pegasus where to find the Replica Catalog
pegasus.catalog.replica=File
pegasus.catalog.replica.file=rc.txt

# This tells Pegasus where to find the Transformation Catalog
pegasus.catalog.transformation=Text
pegasus.catalog.transformation.file=tc.txt

# Use shared filesystem on cluster for data staging
pegasus.data.configuration=sharedfs

# for tutorial purposes we don't want pegasus-transfer
# to make multiple attempts to transfer data
pegasus.transfer.arguments=-m 1

# This is the name of the application for analytics
pegasus.metrics.app=pegasus-tutorial
</programlisting>
  </section>

  <section id="tutorial_failure_recovery">
    <title>Recovery from Failures</title>

    <para>Executing workflows in a distributed environment can lead to
    failures. Often, they are a result of the underlying infrastructure being
    temporarily unavailable, or errors in workflow setup such as incorrect
    executables specified, or input files being unavailable.</para>

    <para>In case of transient infrastructure failures such as a node being
    temporarily down in a cluster, Pegasus will automatically retry jobs in
    case of failure. After a set number of retries (usually once), a hard
    failure occurs, because of which workflow will eventually fail.</para>

    <para>In most of the cases, these errors are correctable (either the
    resource comes back online or application errors are fixed). Once the
    errors are fixed, you may not want to start a new workflow but instead
    start from the point of failure. In order to do this, you can submit the
    rescue workflows automatically created in case of failures. A rescue
    workflow contains only a description of only the work that remains to be
    done.</para>

    <section>
      <title>Submitting Rescue Workflows</title>

      <para>In this example, we will take our previously run workflow and
      introduce errors such that workflow we just executed fails at
      runtime.</para>

      <para>First we will "hide" the input file to cause a failure by renaming
      it:</para>

      <programlisting>$ <emphasis role="bold">mv input/f.a input/f.a.bak</emphasis>
      </programlisting>

      <para>Now submit the workflow again:</para>

      <programlisting>$ <emphasis role="bold">./plan_dax.sh diamond.dax </emphasis>
2018.12.13 18:55:18.985 CST:    
2018.12.13 18:55:18.991 CST:   ----------------------------------------------------------------------- 
2018.12.13 18:55:18.996 CST:   File for submitting this DAG to HTCondor           : diamond-0.dag.condor.sub 
2018.12.13 18:55:19.001 CST:   Log of DAGMan debugging messages                 : diamond-0.dag.dagman.out 
2018.12.13 18:55:19.007 CST:   Log of HTCondor library output                     : diamond-0.dag.lib.out 
2018.12.13 18:55:19.012 CST:   Log of HTCondor library error messages             : diamond-0.dag.lib.err 
2018.12.13 18:55:19.017 CST:   Log of the life of condor_dagman itself          : diamond-0.dag.dagman.log 
2018.12.13 18:55:19.023 CST:    
2018.12.13 18:55:19.028 CST:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: 
2018.12.13 18:55:19.039 CST:   ----------------------------------------------------------------------- 
2018.12.13 18:55:19.740 CST:   Your database is compatible with Pegasus version: 4.9.1dev 
2018.12.13 18:55:19.844 CST:   Submitting to condor diamond-0.dag.condor.sub 
2018.12.13 18:55:19.952 CST:   Submitting job(s). 
2018.12.13 18:55:19.958 CST:   1 job(s) submitted to cluster 1184. 
2018.12.13 18:55:19.963 CST:    
2018.12.13 18:55:19.968 CST:   Your workflow has been started and is running in the base directory: 
2018.12.13 18:55:19.974 CST:    
2018.12.13 18:55:19.979 CST:     $WORK/tutorial/diamond/submit/trainXX/pegasus/diamond/run0002 
2018.12.13 18:55:19.984 CST:    
2018.12.13 18:55:19.990 CST:   *** To monitor the workflow you can run *** 
2018.12.13 18:55:19.995 CST:    
2018.12.13 18:55:20.001 CST:     pegasus-status -l $WORK/tutorial/diamond/submit/trainXX/pegasus/diamond/run0002 
2018.12.13 18:55:20.006 CST:    
2018.12.13 18:55:20.011 CST:   *** To remove your workflow run *** 
2018.12.13 18:55:20.017 CST:    
2018.12.13 18:55:20.022 CST:     pegasus-remove $WORK/tutorial/diamond/submit/trainXX/pegasus/diamond/run0002 
2018.12.13 18:55:20.027 CST:    
2018.12.13 18:55:25.047 CST:   Time taken to execute is 1.714 seconds 


</programlisting>

      <para>We will now monitor the workflow using the pegasus-status command
      till it fails. We will add -w option to pegasus-status to watch
      automatically till the workflow finishes:</para>

      <programlisting><emphasis role="bold">$ pegasus-status submit/cscXXX_auser/pegasus/diamond/run0002</emphasis>
(no matching jobs found in Condor Q)
UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      6       0       0       0       0       1       1  12.5
Summary: 1 DAG total (Failure:1)
</programlisting>

      <para>Now we can use the pegasus-analyzer command to determine what went
      wrong:</para>

      <programlisting>$ <emphasis role="bold"> </emphasis><emphasis
          role="bold">pegasus-analyzer submit/cscXXX_auser/pegasus/diamond/run0002
</emphasis>
************************************Summary*************************************

 Submit Directory   : submit/cscXXX_auser/pegasus/diamond/run0002
 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      1 (12.50%)
 # jobs failed      :      1 (12.50%)
 # jobs held        :      0 (0.00%)
 # jobs unsubmitted :      6 (75.00%)

******************************Failed jobs' details******************************

===========================stage_in_local_summit_0_0============================

 last state: POST_SCRIPT_FAILED
       site: local
submit file: 00/00/stage_in_local_summit_0_0.sub
output file: 00/00/stage_in_local_summit_0_0.out.001
 error file: 00/00/stage_in_local_summit_0_0.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : pegasus-submit
executable  : /opt/pegasus-4.9.3dev/bin/pegasus-transfer
arguments   :   --threads   2   -m   1  
exitcode    : 1
working dir : /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0002

------------------Task #1 - pegasus::transfer - None - stdout-------------------

2019-10-24 23:56:31,654    INFO:  Reading URL pairs from stdin
2019-10-24 23:56:31,655    INFO:  1 transfers loaded
2019-10-24 23:56:31,655    INFO:  PATH=/usr/bin:/bin
2019-10-24 23:56:31,655    INFO:  LD_LIBRARY_PATH=
2019-10-24 23:56:31,681    INFO:  --------------------------------------------------------------------------------
2019-10-24 23:56:31,681    INFO:  Starting transfers - attempt 1
2019-10-24 23:56:33,684   ERROR:  Expected local file does not exist: /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/input/f.a
2019-10-24 23:56:33,685    INFO:  --------------------------------------------------------------------------------
2019-10-24 23:56:33,685    INFO:  Stats: Total 1 transfers, 0.0 B transferred in 2 seconds. Rate: 0.0 B/s (0.0 b/s)
2019-10-24 23:56:33,685    INFO:         Between sites local-&gt;summit : 1 transfers, 0.0 B transferred in 2 seconds. Rate: 0.0 B/s (0.0 b/s)
2019-10-24 23:56:33,685 CRITICAL:  Some transfers failed! See above, and possibly stderr.


</programlisting>

      <para>The above listing indicates that it could not transfer
      pegasus.html. Let's correct that error by restoring the pegasus.html
      file:</para>

      <programlisting><emphasis role="bold">$  mv input/f.a.bak input/f.a</emphasis>
      </programlisting>

      <para>Now in order to start the workflow from where we left off, instead
      of executing pegasus-plan we will use the command pegasus-run on the
      directory from our previous failed workflow run:</para>

      <programlisting>$  <emphasis role="bold">pegasus-run submit/cscXXX_auser/pegasus/diamond/run0002</emphasis>
Rescued /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0002/diamond-0.log as /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0002/diamond-0.log.000
Submitting to condor diamond-0.dag.condor.sub
Submitting job(s).
1 job(s) submitted to cluster 14.

Your workflow has been started and is running in the base directory:

  submit/cscXXX_auser/pegasus/diamond/run0002

*** To monitor the workflow you can run ***

  pegasus-status -l submit/cscXXX_auser/pegasus/diamond/run0002

*** To remove your workflow run ***

  pegasus-remove submit/cscXXX_auser/pegasus/diamond/run0002


</programlisting>

      <para>The workflow will now run to completion and succeed.</para>

      <programlisting><emphasis role="bold">$ pegasus-status -l </emphasis><emphasis
          role="bold"/><emphasis role="bold">submit/trainXX/pegasus/diamond/run0002</emphasis>

(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    0     0     0     0     0     8     0 100.0 Success *diamond-0.dag         
                     
</programlisting>
    </section>
  </section>

  <section id="tutorial_pmc">
    <title>High Throughput Tasks as MPI Master/Worker Cluster Jobs</title>

    <para>Often, users have lots of short running single processor jobs in
    their workflow, that if submitted individually to the underlying SLURM
    cluster take a long time to execute, as each job sits in the SLURM queue.
    For example in our previous example, each job in the blackdiamond workflow
    actually runs for a minute each, and using only one core on a whole node.
    Additionally, since each job is submitted as a separate job to SLURM, each
    job sits in the cluster SLURM queue before it is executed. In order to
    alleviate this, it makes sense to cluster the short running jobs together.
    Pegasus allows users to cluster tasks in their workflow into larger
    chunks, and then execute them using a MPI based master worker tool called
    <emphasis><emphasis role="bold">pegasus-mpi-cluster</emphasis></emphasis>
    .</para>

    <para>In this example, we take the same blackdiamond workflow that we ran
    previously and now run it using PMC where the whole workflow is clustered
    into a single MPI job. In order to tell Pegasus to cluster the jobs we
    have to do the following</para>

    <orderedlist>
      <listitem>
        <para>Tell Pegasus what jobs are clustered. In this example, we do it
        by annotating the DAX with a special pegasus profile called label. In
        the DAX generator daxgen.py you will see the following</para>

        <programlisting>preprocess = Job(name="preprocess")
a = File("f.a")
b1 = File("f.b1")
b2 = File("f.b2")
preprocess.addArguments("-i",a,"-o",b1,"-o",b2)
preprocess.uses(a, link=Link.INPUT)
preprocess.uses(b1, link=Link.OUTPUT, transfer=False, register=False)
preprocess.uses(b2, link=Link.OUTPUT, transfer=False, register=False)

<emphasis role="bold">preprocess.addProfile(Profile("pegasus", "label", "cluster-1"))
</emphasis>
diamond.addJob(preprocess)

        </programlisting>
      </listitem>

      <listitem>
        <para>Tell pegasus that it has to do job clustering and what
        executable to use for job clustering.</para>

        <para>To do this, you do the following</para>

        <itemizedlist>
          <listitem>
            <para>In pegasus.properties file specify the property <emphasis
            role="bold">pegasus.job.aggregator=mpiexec</emphasis></para>
          </listitem>

          <listitem>
            <para>In the transformation catalog, specify the path to the
            clustering executable. In this case, it is a wrapper around PMC
            that does mpiexec on pegasus-mpi-cluster. In tc.txt you can see
            the last entry as</para>

            <programlisting>$ <emphasis role="bold">cat tc.txt</emphasis>

tr pegasus::mpiexec {
  site summit {
    # we cannot specify cores as pegasus profile on OLCF
    # as in simple LSF requests only requires nodes to be allocated
    profile env "PEGASUS_CORES" "36"
    profile pegasus "nodes" "1"
    profile pegasus "runtime" "43200"
    profile pegasus "change.dir" "true"

    pfn "/gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/bin/pmc-wrapper"
    type "INSTALLED"
  }
}</programlisting>

            <para>The profiles tell Pegasus that the PMC executable needs to
            be run on 48 processors on a single node.</para>

            <para>The pmc-wrapper is a simple bash script that sets up the
            TACC MPI environment.</para>

            <programlisting>$<emphasis role="bold"> cat bin/pmc-wrapper </emphasis>
#!/bin/bash

###################################### Fix for aprun change dir error on TITAN ################################################
#### https://www.olcf.ornl.gov/for-users/system-user-guides/titan/titan-user-guide/#filesystems-available-to-compute-nodes ####
###############################################################################################################################
cp *.in $PEGASUS_SCRATCH_DIR
cd $PEGASUS_SCRATCH_DIR
###############################################################################################################################

jsrun -n $PEGASUS_CORES ${PEGASUS_HOME}/bin/pegasus-mpi-cluster "$@"


</programlisting>
          </listitem>
        </itemizedlist>
      </listitem>

      <listitem>
        <para>Lastly, while planning the workflow we add <emphasis
        role="bold">--cluster </emphasis>option to pegasus-plan. That is what
        we have in plan_cluster_dax.sh file.</para>

        <programlisting>$ <emphasis role="bold">cat plan_cluster_dax.sh</emphasis>

#!/bin/bash

DIR=$(cd $(dirname $0) &amp;&amp; pwd)

if [ $# -ne 1 ]; then
    echo "Usage: $0 DAXFILE"
    exit 1
fi

DAXFILE=$1

# This command tells Pegasus to plan the workflow contained in 
# dax file passed as an argument. The planned workflow will be stored
# in the "submit" directory. The execution # site is "".
# --input-dir tells Pegasus where to find workflow input files.
# --output-dir tells Pegasus where to place workflow output files.
pegasus-plan --conf pegasus.properties \
    --dax $DAXFILE \
    --dir $DIR/submit \
    --input-dir $DIR/input \
    --output-dir $DIR/output \
    --cleanup leaf \
    --cluster label \
    --force \
    --sites summit \
    --submit
</programlisting>
      </listitem>
    </orderedlist>

    <para><emphasis role="bold">Let us now plan and run the
    workflow.</emphasis></para>

    <programlisting>$ <emphasis role="bold">/plan_cluster_dax.sh diamond.dax 
</emphasis>2019.10.24 23:59:57.586 UTC:    
2019.10.24 23:59:57.592 UTC:   ----------------------------------------------------------------------- 
2019.10.24 23:59:57.597 UTC:   File for submitting this DAG to HTCondor           : diamond-0.dag.condor.sub 
2019.10.24 23:59:57.602 UTC:   Log of DAGMan debugging messages                 : diamond-0.dag.dagman.out 
2019.10.24 23:59:57.607 UTC:   Log of HTCondor library output                     : diamond-0.dag.lib.out 
2019.10.24 23:59:57.613 UTC:   Log of HTCondor library error messages             : diamond-0.dag.lib.err 
2019.10.24 23:59:57.618 UTC:   Log of the life of condor_dagman itself          : diamond-0.dag.dagman.log 
2019.10.24 23:59:57.623 UTC:    
2019.10.24 23:59:57.628 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: 
2019.10.24 23:59:57.638 UTC:   ----------------------------------------------------------------------- 
2019.10.24 23:59:58.042 UTC:   Your database is compatible with Pegasus version: 4.9.3dev 
2019.10.24 23:59:58.094 UTC:   Submitting to condor diamond-0.dag.condor.sub 
2019.10.24 23:59:58.106 UTC:   Submitting job(s). 
2019.10.24 23:59:58.111 UTC:   1 job(s) submitted to cluster 17. 
2019.10.24 23:59:58.117 UTC:    
2019.10.24 23:59:58.122 UTC:   Your workflow has been started and is running in the base directory: 
2019.10.24 23:59:58.127 UTC:    
2019.10.24 23:59:58.132 UTC:     /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0003 
2019.10.24 23:59:58.137 UTC:    
2019.10.24 23:59:58.142 UTC:   *** To monitor the workflow you can run *** 
2019.10.24 23:59:58.148 UTC:    
2019.10.24 23:59:58.153 UTC:     pegasus-status -l /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0003 
2019.10.24 23:59:58.158 UTC:    
2019.10.24 23:59:58.163 UTC:   *** To remove your workflow run *** 
2019.10.24 23:59:58.169 UTC:    
2019.10.24 23:59:58.174 UTC:     pegasus-remove /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/diamond/submit/cscXXX_auser/pegasus/diamond/run0003 
2019.10.24 23:59:58.179 UTC:    
2019.10.24 23:59:58.882 UTC:   Time taken to execute is 1.126 seconds 
</programlisting>

    <para>This is what the diamond workflow looks like after Pegasus has
    finished planning the DAX</para>

    <figure>
      <title>Clustered Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="50%"
                     fileref="images/diamond-pmc-dag.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>You can see that instead of 4 jobs making up the diamond have been
    replaced by a single merge_cluster-1 job, that is executed as a MPI
    job.</para>

    <para>stdout/stderr of the jobs now appears as a set of outputs in the
    merged job output. This can be found in the 00/00/merge_cluster-1.out.000
    file in the submit directory.</para>
  </section>

  <section id="tutorial_mpi_workflows">
    <title>MPI Jobs</title>

    <para>This exercise walks through running a simple workflow with an MPI
    job. This example executes a workflow consisting of a single MPI job that
    executes on Summit. The MPI executable is a simple hello world MPI
    executable that is shipped with the example and needs to be compiled by
    each user. This example is a canonical example, that highlights the data
    management capabilities of Pegasus, whereby as part of the workflow
    execution you can retrieve input from a local/remote location , execute
    the jobs defined in the DAX ( in this case a single MPI job), ship the
    data out to local directory/remote location, and cleanup the scratch space
    automatically as the workflow progresses.</para>

    <programlisting><emphasis role="bold">$ cd $WORK/tutorial</emphasis>
</programlisting>

    <para>We now use pegasus-init to setup up this workflow. This time around
    we select choice 8 as the workflow to setup</para>

    <programlisting><emphasis role="bold">$</emphasis> <emphasis role="bold">pegasus-init mpi</emphasis>
Do you want to generate a tutorial workflow? (y/n) [n]: y
1: Local Machine Condor Pool
2: USC HPCC Cluster
3: OSG from ISI submit node
4: XSEDE, with Bosco
5: Bluewaters, with Glite
6: TACC Wrangler with Glite
7: OLCF TITAN with Glite
<emphasis role="bold">8: OLCF Summit from Kubernetes using BOSCO</emphasis>
What environment is tutorial to be setup for? (1-8) [1]: 8
What project your jobs should run under. For example on TACC there are like : TG-DDM160003 ?: CSCXXX
1: Process
2: Pipeline
3: Split
4: Merge
5: EPA (requires R)
6: Population Modeling using Containers
7: Diamond
<emphasis role="bold">8: MPI Hello World</emphasis>
What tutorial workflow do you want? (1-8) [1]: 8
<emphasis role="bold">Pegasus Tutorial setup for example workflow - mpi-hw for execution on summit-kub-bosco in directory /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/mpi</emphasis></programlisting>

    <para>You will find the c executable in it and a simple wrapper that
    launches the executable using jsrun.</para>

    <programlisting><emphasis role="bold">$ cat bin/mpi-hello-world-wrapper 
</emphasis>#!/bin/bash
set -e

# before launching the job switch to the directory that
# pegasus created for the workflow
cd $PEGASUS_SCRATCH_DIR

jsrun -n $PEGASUS_CORES  /ccs/proj/cscXXX/summit/pegasus/stable/bin/pegasus-mpi-hw  "$@"</programlisting>

    <para>We now generate the DAX</para>

    <programlisting>$ ./generate_dax.sh mpi.dax
</programlisting>

    <figure>
      <title>MPI Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/tutorial-mpi-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <programlisting><emphasis role="bold">$  more mpi.dax </emphasis>
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated: 2019-10-25 00:15:11.493068 --&gt;
&lt;!-- generated by: cscXXX_auser --&gt;
&lt;!-- generator: python --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.6.xsd" version="3.6" name="mpi-hello-world"&gt;
  &lt;metadata key="created"&gt;Fri Oct 25 00:15:11 2019&lt;/metadata&gt;
  &lt;metadata key="dax.api"&gt;python&lt;/metadata&gt;
  &lt;metadata key="creator"&gt;cscXXX_auser@pegasus-submit&lt;/metadata&gt;
 &lt;job id="ID0000001" namespace="pegasus" name="mpihw"&gt;
   &lt;argument&gt;-i  &lt;file name="f.in"/&gt; -o  &lt;file name="f.out"/&gt;&lt;/argument&gt;
   &lt;profile namespace="globus" key="jobtype"&gt;mpi&lt;/profile&gt;
   &lt;profile namespace="pegasus" key="nodes"&gt;2&lt;/profile&gt;
   &lt;profile namespace="pegasus" key="cores"&gt;32&lt;/profile&gt;
   &lt;profile namespace="pegasus" key="runtime"&gt;300&lt;/profile&gt;
   &lt;uses name="f.in" link="input"/&gt;
   &lt;uses name="f.out" link="output"/&gt;
 &lt;/job&gt;
&lt;/adag&gt;
</programlisting>

    <para>We will now plan this workflow and submit it for execution</para>

    <programlisting><emphasis role="bold">$ ./plan_dax.sh mpi.dax</emphasis> 

2019.10.25 00:16:21.344 UTC:    
2019.10.25 00:16:21.350 UTC:   ----------------------------------------------------------------------- 
2019.10.25 00:16:21.355 UTC:   File for submitting this DAG to HTCondor           : mpi-hello-world-0.dag.condor.sub 
2019.10.25 00:16:21.360 UTC:   Log of DAGMan debugging messages                 : mpi-hello-world-0.dag.dagman.out 
2019.10.25 00:16:21.366 UTC:   Log of HTCondor library output                     : mpi-hello-world-0.dag.lib.out 
2019.10.25 00:16:21.371 UTC:   Log of HTCondor library error messages             : mpi-hello-world-0.dag.lib.err 
2019.10.25 00:16:21.376 UTC:   Log of the life of condor_dagman itself          : mpi-hello-world-0.dag.dagman.log 
2019.10.25 00:16:21.382 UTC:    
2019.10.25 00:16:21.387 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: 
2019.10.25 00:16:21.397 UTC:   ----------------------------------------------------------------------- 
2019.10.25 00:16:21.806 UTC:   Your database is compatible with Pegasus version: 4.9.3dev 
2019.10.25 00:16:21.858 UTC:   Submitting to condor mpi-hello-world-0.dag.condor.sub 
2019.10.25 00:16:21.871 UTC:   Submitting job(s). 
2019.10.25 00:16:21.876 UTC:   1 job(s) submitted to cluster 26. 
2019.10.25 00:16:21.882 UTC:    
2019.10.25 00:16:21.887 UTC:   Your workflow has been started and is running in the base directory: 
2019.10.25 00:16:21.892 UTC:    
2019.10.25 00:16:21.897 UTC:     /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/mpi/submit/cscXXX_auser/pegasus/mpi-hello-world/run0001 
2019.10.25 00:16:21.902 UTC:    
2019.10.25 00:16:21.907 UTC:   *** To monitor the workflow you can run *** 
2019.10.25 00:16:21.913 UTC:    
2019.10.25 00:16:21.918 UTC:     pegasus-status -l /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/mpi/submit/cscXXX_auser/pegasus/mpi-hello-world/run0001 
2019.10.25 00:16:21.923 UTC:    
2019.10.25 00:16:21.928 UTC:   *** To remove your workflow run *** 
2019.10.25 00:16:21.933 UTC:    
2019.10.25 00:16:21.939 UTC:     pegasus-remove /gpfs/alpine/cscXXX/scratch/cscXXX_auser/tutorial/mpi/submit/cscXXX_auser/pegasus/mpi-hello-world/run0001 
2019.10.25 00:16:21.944 UTC:    
2019.10.25 00:16:26.955 UTC:   Time taken to execute is 1.124 seconds 
</programlisting>

    <para>We monitor the workflow and let it complete<programlisting><emphasis
          role="bold">$ pegasus-status -l -w ./submit/cscXXX_auser/pegasus/mpi-hello-world/run0001/
</emphasis>

(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    0     0     0     0     0     6     0 100.0 Success *mpi-hello-world-0.dag        </programlisting><parameter>The
    output file will be in the output folder</parameter></para>

    <programlisting><emphasis role="bold">$ cat output/f.out 
</emphasis></programlisting>
  </section>

  <section id="tutorial_conclusion">
    <title>Conclusion</title>

    <para>This brings you to the end of the Pegasus tutorial on OLCF Summit
    Cluster The tutorial should have given you an overview of how to compose a
    simple workflow using Pegasus and running it on the OLCF Summit cluster.
    The tutorial examples, should provide a good starting point for you to
    port your application to a Pegasus workflow. If you need help in porting
    your application to Pegasus contact us on the following support
    channels</para>

    <para>Public mailman list: pegasus-users@isi.edu (subscribe <ulink
    url="https://mailman.isi.edu/mailman/listinfo/pegasus-users">here</ulink>)</para>

    <para>Private support list: pegasus-support@isi.edu</para>

    <para>Detailed Pegasus Documentation can be found <ulink
    url="https://pegasus.isi.edu/documentation/">here</ulink>.</para>
  </section>
</chapter>
