<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial">
  <title>Tutorial</title>

  <section>
    <title>Introduction</title>

    <para>This tutorial will take you through the steps of creating and
    running a simple scientific workflow using Pegasus WMS. This tutorial is
    intended for new users who want to get a quick overview of Pegasus
    concepts and usage. The tutorial covers the creating, planning,
    submitting, monitoring, debugging, and generating statistics for a simple
    diamond-shaped workflow. More information about the topics covered in this
    tutorial can be found in the Pegasus user guide.</para>

    <section>
      <title>What are Scientific Workflows</title>

      <para>Scientific workflows allow users to easily express multi-step
      computational tasks, for example retrieve data from an instrument or a
      database, reformat the data, and run an analysis. A scientific workflow
      describes the dependencies between the tasks and in most cases the
      workflow is described as a directed acyclic graph (DAG), where the nodes
      are tasks and the edges denote the task dependencies. A defining
      property for a scientific workflow is that it manages data flow. The
      tasks in a scientific workflow can be everything from short serial tasks
      to very large parallel tasks (MPI for example) surrounded by a large
      number of small, serial tasks used for pre- and post-processing.</para>

      <para>Workflows can vary from simple to complex. Below are some
      examples</para>

      <para><emphasis role="bold">Single Task Workflow</emphasis></para>

      <para>A single task to execute</para>

      <figure>
        <title>Single Task Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="50%"
                       fileref="images/concepts-single-job-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para><emphasis role="bold">Bag of Tasks Workflow</emphasis></para>

      <para>No dependencies between the tasks.</para>

      <figure>
        <title>Bag of Tasks Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/concepts-bag-tasks-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para><emphasis role="bold">Pipeline of Tasks</emphasis><figure>
          <title>Pipeline of Tasks</title>

          <mediaobject>
            <imageobject>
              <imagedata align="center" contentdepth="70%"
                         fileref="images/concepts-pipeline-tasks-wf.jpg"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para><emphasis role="bold">Split Workflow</emphasis><figure>
          <title>Split Workflow</title>

          <mediaobject>
            <imageobject>
              <imagedata align="center" contentwidth="70%"
                         fileref="images/concepts-split-wf.jpg"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para><emphasis role="bold">Merge Workflow</emphasis><figure>
          <title>Merge Workflow</title>

          <mediaobject>
            <imageobject>
              <imagedata align="center" contentwidth="70%"
                         fileref="images/concepts-merge-wf.jpg"/>
            </imageobject>
          </mediaobject>
        </figure></para>
    </section>
  </section>

  <section>
    <title>Getting Started</title>

    <para>All of the steps in this tutorial are performed on the command-line.
    The convention we will use for command-line input and output is to put
    things that you should type in bold, monospace font, and to put the output
    you should get in a normal weight, monospace font, like this:</para>

    <programlisting>[user@host dir]$ <emphasis role="bold">you type this</emphasis>
you get this</programlisting>

    <para>Where <literal>[user@host dir]$</literal> is the terminal prompt,
    the text you should type is “<literal>you type this</literal>”, and the
    output you should get is "<literal>you get this</literal>". The terminal
    prompt will be abbreviated as <literal>$</literal>. Because some of the
    outputs are long, we don’t always include everything. Where the output is
    truncated we will add an ellipsis '...' to indicate the omitted
    output.</para>

    <para><emphasis role="bold">For the purpose of this tutorial replace any
    instance of userXX with your hpc-pegasus.usc.edu
    username.</emphasis></para>

    <para><emphasis role="bold">If you are having trouble with this tutorial,
    or anything else related to Pegasus, you can contact the Pegasus Users
    mailing list at <email>pegasus-users@isi.edu</email> to get
    help.</emphasis></para>

    <para>In order to make it easier for new users to start with Pegasus on
    USC HPCC, we have a dedicated submit node <emphasis
    role="bold">hpc-pegasus.usc.edu</emphasis> that users can use to do the
    tutorial and submit their workflows to the HPCC cluster using Pegasus. To
    request access to the submit node, please send me to the HPCC staff
    <emphasis role="bold"><email>hpcc@usc.edu</email> </emphasis>. Usually,
    getting an account on the submit machines take one business day or
    less.</para>

    <para>To start the tutorial copy the tutorial examples to a directory
    called tutorial in your home directory and untar the examples.</para>

    <programlisting>[user@hpc-pegasus]$ cd ~
[user@hpc-pegasus]$ <emphasis role="bold">mkdir tutorial</emphasis>
[user@hpc-pegasus ~]$ <emphasis role="bold">cd tutorial/</emphasis>
[user@hpc-pegasus]$ <emphasis role="bold">cp /home/rcf-proj/gmj/pegasus/tutorial/usc-pegasus-tutorial.tgz .</emphasis>
[user@hpc-pegasus]$ <emphasis role="bold">tar zxf usc-pegasus-tutorial.tgz</emphasis>
[user@hpc-pegasus]$ <emphasis role="bold">ls</emphasis> 
blackdiamond-hpcc-sharedfs-example  mpi-hello-world  usc-pegasus-tutorial.tgz

</programlisting>

    <para>The remainder of this tutorial will assume that you have a terminal
    open to the directory where the tutorial files are installed i.e.
    ~/tutorial.</para>
  </section>

  <section>
    <title>Generating the Workflow</title>

    <para>We will be creating and running a simple diamond-shaped workflow
    that looks like this:</para>

    <figure>
      <title>Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/concepts-diamond.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In this diagram, the ovals represent computational jobs, the
    dog-eared squares are files, and the arrows are dependencies.</para>

    <para>Pegasus reads workflow descriptions from DAX files. The term “DAX”
    is short for “Directed Acyclic Graph in XML”. DAX is an XML file format
    that has syntax for expressing jobs, arguments, files, and
    dependencies.</para>

    <para>In order to create a DAX it is necessary to write code for a DAX
    generator. Pegasus comes with Perl, Java, and Python libraries for writing
    DAX generators. In this tutorial we will show how to use both the Java and
    Python library.</para>

    <para>The DAX generator for the diamond workflow is in the file
    <filename>generate_dax.py</filename>. Look at the file by typing:</para>

    <programlisting>$ <emphasis role="bold">cd </emphasis><emphasis
        role="bold">blackdiamond-hpcc-sharedfs-example</emphasis>
$ <emphasis role="bold">more BlackDiamondDAX.java</emphasis>
...</programlisting>

    <tip>
      <para>We will be using the <literal>more</literal> command to inspect
      several files in this tutorial. <literal>more</literal> is a pager
      application, meaning that it splits text files into pages and displays
      the pages one at a time. You can view the next page of a file by
      pressing the spacebar. Type 'h' to get help on using
      <literal>more</literal>. When you are done, you can type 'q' to close
      the file.</para>
    </tip>

    <para>The code has 4 sections:</para>

    <orderedlist>
      <listitem>
        <para>The name for the DAX output file is retrieved from the
        arguments.</para>
      </listitem>

      <listitem>
        <para>A new ADAG object is created. This is the main object to which
        jobs and dependencies are added.</para>
      </listitem>

      <listitem>
        <para>Jobs and files are added. The 4 jobs in the diagram above are
        added and the 6 files are referenced. Arguments are defined using
        strings and File objects. The input and output files are defined for
        each job. This is an important step, as it allows Pegasus to track the
        files, and stage the data if necessary. Workflow outputs are tagged
        with “transfer=true”.</para>
      </listitem>

      <listitem>
        <para>Dependencies are added. These are shown as arrows in the diagram
        above. They define the parent/child relationships between the jobs.
        When the workflow is executing, the order in which the jobs will be
        run is determined by the dependencies between them.</para>
      </listitem>
    </orderedlist>

    <para>Generate a DAX file named <filename>diamond.dax</filename> by
    typing:</para>

    <programlisting>$ <emphasis role="bold">CLASSPATH=`pegasus-config --classpath`</emphasis>
$ <emphasis role="bold">export CLASSPATH=".:$CLASSPATH"</emphasis>
$ <emphasis role="bold">javac BlackDiamondDAX.java</emphasis>
$ <emphasis role="bold">java BlackDiamondDAX /usr/bin diamond.dax</emphasis>
Creating ADAG...
Adding preprocess job...
Adding left Findrange job...
Adding right Findrange job...
Adding Analyze job...
Adding control flow dependencies...
Writing diamond.dax</programlisting>

    <para>The <filename>diamond.dax</filename> file should contain an XML
    representation of the diamond workflow. You can inspect it by
    typing:</para>

    <programlisting>$ <emphasis role="bold">more diamond.dax</emphasis>
...</programlisting>
  </section>

  <section>
    <title>Information Catalogs</title>

    <para>There are three information catalogs that Pegasus uses when planning
    the workflow. These are the <link linkend="tut_site_catalog">Site
    Catalog</link>, <link linkend="tut_xform_catalog">Transformation
    Catalog</link>, and <link linkend="tut_replica_catalog">Replica
    Catalog</link>.</para>

    <para>To generate the various catalogs for this example, run the shell
    script generate_catalogs.sh</para>

    <para><programlisting>$ <emphasis role="bold">./generate_catalogs.sh</emphasis>
...</programlisting></para>

    <section id="tut_site_catalog">
      <title>The Site Catalog</title>

      <para>The site catalog describes the sites where the workflow jobs are
      to be executed. Typically the sites in the site catalog describe remote
      clusters, such as PBS clusters or Condor pools. In this tutorial we
      assume that you have a Personal Condor pool running on localhost. If you
      are using one of the tutorial VMs this has already been setup for
      you.</para>

      <para>The site catalog is in <filename>sites.xml</filename>:</para>

      <programlisting>$ <emphasis role="bold">more sites.xml</emphasis>
...
﻿    &lt;!-- The local site contains information about the submit host --&gt;
    &lt;!-- The arch and os keywords are used to match binaries in the transformation catalog --&gt;
    &lt;site  handle="local" arch="x86" os="LINUX"&gt;
        &lt;directory type="shared-scratch" path="/home/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/work"&gt;
            &lt;file-server operation="all" url="file:///home/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/work"/&gt;
        &lt;/directory&gt;
        &lt;directory type="local-storage" path="/home/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/outputs"&gt;
            &lt;file-server operation="all" url="file:///home/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/outputs"/&gt;
        &lt;/directory&gt;
    &lt;/site&gt;

    &lt;!-- the hpcc cluster designates the USC HPCC cluster --&gt;
    &lt;site  handle="hpcc" arch="x86" os="LINUX"&gt;        
        &lt;!--shared scratch directory indicates a directory that is visible
            on all the nodes of the HPCC cluster. This is where the jobs
            execute --&gt;
        &lt;directory type="shared-scratch" path="/home/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/HPCC/shared-scratch"&gt;
            &lt;file-server operation="all" url="file:///home/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/HPCC/shared-scratch"/&gt;
        &lt;/directory&gt;

        &lt;!-- tell pegasus it is a PBS cluster and submission to be via glite --&gt;
        &lt;profile namespace="pegasus" key="style" &gt;glite&lt;/profil&gt;e
        &lt;profile namespace="condor" key="grid_resource"&gt;pbs&lt;/profile&gt;

        &lt;profile namespace="env" key="PEGASUS_HOME"&gt;/usr&lt;/profile&gt;
        &lt;profile namespace="pegasus" key="change.dir"&gt;true&lt;/profile&gt;

        &lt;!-- maxwalltime in minutes for the jobs run on this cluster --&gt;
        &lt;profile namespace="globus" key="maxwalltime"&gt;600&lt;/profile&gt;
    &lt;/site&gt;



...</programlisting>

      <para>There are two sites defined in the site catalog: “local” and
      “hpcc”. The “local” site is used by Pegasus to learn about the submit
      host where the workflow management system runs. The “hpcc” site is the
      USC HPCC cluster.</para>

      <para>The local site is configured with a “storage” file system that is
      mounted on the submit host (indicated by the file:// URL). This file
      system is where the output data from the workflow will be stored. When
      the workflow is planned we will tell Pegasus that the output site is
      “local”.</para>

      <para>The hpcc site is configured with a “scratch” file system
      accessible via file (indicated by the file:// URL). This file system is
      where the working directory will be created. When we plan the workflow
      we will tell Pegasus that the execution site is “hpcc”. Note that in
      this case, since the local site and the HPCC cluster share a filesystem
      ( your HOME directory ), we can copy the inputs using a simple file
      copy.</para>

      <para>Pegasus supports many different file transfer protocols. For
      example, you can set up transfers from your submit host to the cluster
      using SCP. In that case, the scratch file system with have SCP://URL. To
      specify the passwordless ssh key to use, you will need to add a pegasus
      profile key named SSH_PRIVATE_KEY that tells Pegasus where to find the
      private key to use for SCP transfers. Remember to add the passwordless
      key to your ssh authorized keys.</para>

      <para>Finally, the hpcc site is configured with two profiles that tell
      Pegasus that it is a plain PBS Cluster and jobs need to be submitted via
      the Condor glite mechanism. Pegasus supports many ways of submitting
      tasks to a remote cluster. In this configuration it will submit jobs
      directly to the PBS cluster using the HTCondor glite
      funcitonality.</para>
    </section>

    <section>
      <title id="tut_xform_catalog">The Transformation Catalog</title>

      <para>The transformation catalog describes all of the executables
      (called “transformations”) used by the workflow. This description
      includes the site(s) where they are located, the architecture and
      operating system they are compiled for, and any other information
      required to properly transfer them to the execution site and run
      them.</para>

      <para>For this tutorial, the transformation catalog is in the file
      <filename>tc.dat</filename>:</para>

      <programlisting>$ <emphasis role="bold">more tc.dat</emphasis>
...
﻿# This is the transformation catalog. It lists information about each of the
# executables that are used by the workflow.

tr blackdiamond::preprocess:4.0 {
    site hpcc {
        pfn "/usr/bin/pegasus-keg"
        arch "x86_64"
        os "linux"
        type "INSTALLED"
        profile pegasus "clusters.size" "2" 
    }
}
...</programlisting>

      <para>The <filename>tc.dat</filename> file contains information about
      three transformations: preprocess, findrange, and analyze. These three
      transformations are referenced in the diamond DAX. The transformation
      catalog indicates that all three transformations are installed on the
      hpcc site, and are compiled for x86_64 Linux.</para>

      <para>The actual executable files are located in the
      <filename>bin</filename> directory. All three executables are actually
      symlinked to the same Python script. This script is just an example
      transformation that sleeps for 30 seconds, and then writes its own name
      and the contents of all its input files to all of its output
      files.</para>
    </section>

    <section>
      <title id="tut_replica_catalog">The Replica Catalog</title>

      <para>The final catalog is the Replica Catalog. This catalog tells
      Pegasus where to find each of the input files for the workflow.</para>

      <para>All files in a Pegasus workflow are referred to in the DAX using
      their Logical File Name (LFN). These LFNs are mapped to Physical File
      Names (PFNs) when Pegasus plans the workflow. This level of indirection
      enables Pegasus to map abstract DAXes to different execution sites and
      plan out the required file transfers automatically.</para>

      <para>The Replica Catalog for the diamond workflow is in the
      <filename>rc.dat</filename> file:</para>

      <programlisting>$ <emphasis role="bold">more rc.dat</emphasis>
# This is the replica catalog. It lists information about each of the
# input files used by the workflow.

# The format is:
# LFN     PFN    pool="SITE"

f.a    file:///home/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/inpu/f.a    pool="local"</programlisting>

      <para>This replica catalog contains only one entry for the diamond
      workflow’s only input file. This entry has an LFN of “f.a” with a PFN of
      “file:///home/tutorial/input/f.a” and the file is stored on the local
      site, which implies that it will need to be transferred to the hpcc site
      when the workflow runs. The Replica Catalog uses the keyword "pool" to
      refer to the site. Don't be confused by this: the value of the pool
      variable should be the name of the site where the file is located from
      the Site Catalog.</para>
    </section>
  </section>

  <section>
    <title>Configuring Pegasus</title>

    <para>In addition to the information catalogs, Pegasus takes a
    configuration file that specifies settings that control how it plans the
    workflow.</para>

    <para>For the diamond workflow, the Pegasus configuration file is
    relatively simple. It only contains settings to help Pegasus find the
    information catalogs. These settings are in the
    <filename>pegasus.conf</filename> file:</para>

    <programlisting>$ <emphasis role="bold">more pegasus.conf</emphasis>
#This tells Pegasus where to find the Site Catalog                                                                                                                      
pegasus.catalog.site=XML3
pegasus.catalog.site.file=./conf/sites.xml

# This tells Pegasus where to find the Replica Catalog                                                                                                                  
pegasus.catalog.replica=File
pegasus.catalog.replica.file=./conf/rc.dat

# This tells Pegasus where to find the Transformation Catalog                                                                                                           
pegasus.catalog.transformation=Text
pegasus.catalog.transformation.file=./conf/tc.dat

# the hpcc cluster nodes share a filesystem                                                                                                                             
pegasus.data.configuration = sharedfs

</programlisting>
  </section>

  <section>
    <title>Planning the Workflow</title>

    <para>The planning stage is where Pegasus maps the abstract DAX to one or
    more execution sites. The planning step includes:</para>

    <orderedlist>
      <listitem>
        <para>Adding a job to create the remote working directory</para>
      </listitem>

      <listitem>
        <para>Adding stage-in jobs to transfer input data to the remote
        working directory</para>
      </listitem>

      <listitem>
        <para>Adding cleanup jobs to remove data from the remote working
        directory when it is no longer needed</para>
      </listitem>

      <listitem>
        <para>Adding stage-out jobs to transfer data to the final output
        location as it is generated</para>
      </listitem>

      <listitem>
        <para>Adding registration jobs to register the data in a replica
        catalog</para>
      </listitem>

      <listitem>
        <para>Task clustering to combine several short-running jobs into a
        single, longer-running job. This is done to make short-running jobs
        more efficient.</para>
      </listitem>

      <listitem>
        <para>Adding wrappers to the jobs to collect provenance information so
        that statistics and plots can be created when the workflow is
        finished</para>
      </listitem>
    </orderedlist>

    <para>The <literal>pegasus-plan</literal> command is used to plan a
    workflow. This command takes quite a few arguments, so we created a
    <filename>plan_dax.sh</filename> wrapper script that has all of the
    arguments required for the diamond workflow:</para>

    <programlisting>$ <emphasis role="bold">more plan_dax.sh</emphasis>
...</programlisting>

    <para>The script invokes the <literal>pegasus-plan</literal> command with
    arguments for the configuration file (<literal>--conf</literal>), the DAX
    file (<literal>-d</literal>), the submit directory
    (<literal>--dir</literal>), the execution site
    (<literal>--sites</literal>), the output site (<literal>-o</literal>) and
    two extra arguments that prevent Pegasus from removing any jobs from the
    workflow (<literal>--force</literal>) and that prevent Pegasus from adding
    cleanup jobs to the workflow (<literal>--nocleanup</literal>).</para>

    <para>Top plan the diamond workflow invoke the
    <filename>plan_dax.sh</filename> script with the path to the DAX
    file:</para>

    <programlisting>$ <emphasis role="bold">./plan_dax.sh diamond.dax</emphasis>
2012.07.24 21:11:03.256 EDT:   

I have concretized your abstract workflow. The workflow has been entered 
into the workflow database with a state of "planned". The next step is to 
start or execute your workflow. The invocation required is:

pegasus-run  /auto/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/work/dags/userXX/pegasus/blackdiamond/run0001


2012.07.24 21:11:03.257 EDT:   Time taken to execute is 1.103 seconds
</programlisting>

    <para>Note the line in the output that starts with
    <literal>pegasus-run</literal>. That is the command that we will use to
    submit the workflow. The path it contains is the path to the submit
    directory where all of the files required to submit and monitor the
    workflow are stored.</para>

    <para>This is what the diamond workflow looks like after Pegasus has
    finished planning the DAX:</para>

    <figure>
      <title>Diamond DAG</title>

      <mediaobject>
        <imageobject>
          <imagedata contentwidth="70%"
                     fileref="images/concepts-diamond-dag.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>For this workflow the only jobs Pegasus needs to add are a directory
    creation job, a stage-in job (for f.a), and a stage-out job (for f.d). No
    registration jobs are added because all the files in the DAX are marked
    register="false", and no cleanup jobs are added because we passed the
    <literal>--nocleanup</literal> argument to
    <literal>pegasus-plan</literal>.</para>
  </section>

  <section>
    <title>Submitting the Workflow</title>

    <para>Once the workflow has been planned, the next step is to submit it to
    DAGMan/Condor for execution. This is done using the
    <literal>pegasus-run</literal> command. This command takes the path to the
    submit directory as an argument. Run the command that was printed by the
    <filename>plan_dax.sh</filename> script:</para>

    <programlisting>$ <emphasis role="bold">pegasus-run </emphasis><emphasis
        role="bold">work/dags/userXX/pegasus/blackdiamond/run0001</emphasis>
-----------------------------------------------------------------------
File for submitting this DAG to Condor           : blackdiamond-0.dag.condor.sub
Log of DAGMan debugging messages                 : blackdiamond-0.dag.dagman.out
Log of Condor library output                     : blackdiamond-0.dag.lib.out
Log of Condor library error messages             : blackdiamond-0.dag.lib.err
Log of the life of condor_dagman itself          : blackdiamond-0.dag.dagman.log

Submitting job(s).
1 job(s) submitted to cluster 1968.
-----------------------------------------------------------------------

Your workflow has been started and is running in the base directory:

  work/dags/userXX/pegasus/blackdiamond/run0001/

*** To monitor the workflow you can run ***

  pegasus-status -l work/dags/userXX/pegasus/blackdiamond/run0001/

*** To remove your workflow run ***

  pegasus-remove work/dags/userXX/pegasus/blackdiamond/run0001/
</programlisting>
  </section>

  <section>
    <title>Monitoring the Workflow</title>

    <para>After the workflow has been submitted you can monitor it using the
    <literal>pegasus-status</literal> command:</para>

    <programlisting>$ <emphasis role="bold">pegasus-status </emphasis><emphasis
        role="bold">work/dags/userXX/pegasus/blackdiamond/run0001/</emphasis>

STAT  IN_STATE  JOB                                               
Run      01:48  diamond-0                                         
Run      00:05   |-findrange_ID0000002                            
Run      00:05   \_findrange_ID0000003                            
Summary: 3 Condor jobs total (R:3)

UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      2       0       0       3       0       3       0  37.5
Summary: 1 DAG total (Running:1)
</programlisting>

    <para>This command shows the workflow (diamond-0) and the running jobs (in
    the above output it shows the two findrange jobs). It also gives
    statistics on the number of jobs in each state and the percentage of the
    jobs in the workflow that have finished successfully.</para>

    <para>Use the <literal>watch</literal> command to continuously monitor the
    workflow:</para>

    <programlisting>$ <emphasis role="bold">pegasus-status -l -w work/dags/userXX/pegasus/blackdiamond/run0001/
</emphasis>...</programlisting>

    <para>You should see all of the jobs in the workflow run one after the
    other. After a few minutes you will see:</para>

    <programlisting>(no matching jobs found in Condor Q)
UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      0       0       0       0       0       8       0 100.0
Summary: 1 DAG total (Success:1)
</programlisting>

    <para>That means the workflow is finished successfully. You can type
    <literal>ctrl-c</literal> to terminate the <literal>watch</literal>
    command.</para>

    <para>If the workflow finished successfully you should see the output file
    <filename>f.d</filename> in the <filename>output</filename> directory.
    This file was created by the various transformations in the workflow and
    shows all of the executables that were invoked by the workflow:</para>

    <programlisting>$ <emphasis role="bold">more outputs/f.d
 </emphasis>--- start f.c1 ----
  --- start f.b1 ----
    --- start f.a ----
      This is sample input to KEG
    --- final f.a ----
    Timestamp Today: 20141208T144644.443-08:00 (1418078804.443;60.008)
    Applicationname: preprocess [] @ 10.125.4.219 (VPN)
    Current Workdir: /auto/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/HPCC/shared-scratch/userXX/pegasus/blackdiamond/run0001
    Systemenvironm.: x86_64-Linux 2.6.32-431.29.2.el6.x86_64
    Processor Info.: 12 x Intel(R) Xeon(R) CPU           X5650  @ 2.67GHz @ 2666.957
    Load Averages  : 0.637 0.263 0.199
    Memory Usage MB: 24020 total, 23397 free, 0 shared, 101 buffered
    Swap Usage   MB: 1023 total, 984 free
    Filesystem Info: /                        ext4  1007MB total,   664MB avail
    Filesystem Info: /boot                    ext4   503MB total,   434MB avail
    Filesystem Info: /tmp                     ext4   138GB total,   131GB avail
    Filesystem Info: /usr                     ext4  4031MB total,   786MB avail
    Filesystem Info: /var                     ext4  2015MB total,  1222MB avail
    Filesystem Info: /dev/shm                 tmpfs    11GB total,    11GB avail
    Output Filename: f.b1
    Input Filenames: f.a
  --- final f.b1 ----
  Timestamp Today: 20141208T145013.199-08:00 (1418079013.199;60.008)
  Applicationname: findrange [] @ 10.125.4.220 (VPN)
  Current Workdir: /auto/rcf-40/userXX/tutorial/blackdiamond-hpcc-sharedfs-example/HPCC/shared-scratch/userXX/pegasus/blackdiamond/run0001
  Systemenvironm.: x86_64-Linux 2.6.32-431.29.2.el6.x86_64
  Processor Info.: 12 x Intel(R) Xeon(R) CPU           X5650  @ 2.67GHz @ 2666.656
  Load Average

....</programlisting>

    <para>Remember that the example transformations in this workflow just
    print their name to all of their output files and then copy all of their
    input files to their output files.</para>
  </section>

  <section>
    <title>Debugging the Workflow</title>

    <para>In the case that one or more jobs fails, then the output of the
    <literal>pegasus-status</literal> command above will have a non-zero value
    in the <literal>FAILURE</literal> column.</para>

    <para>You can debug the failure using the
    <literal>pegasus-analyzer</literal> command. This command will identify
    the jobs that failed and show their output. Because the workflow
    succeeded, <literal>pegasus-analyzer</literal> will only show some basic
    statistics about the number of successful jobs:</para>

    <programlisting>$ <emphasis role="bold">pegasus-analyzer </emphasis><emphasis
        role="bold">work/dags/userXX/pegasus/blackdiamond/run0001</emphasis>
pegasus-analyzer: initializing...

****************************Summary***************************

 Total jobs         :      7 (100.00%)
 # jobs succeeded   :      7 (100.00%)
 # jobs failed      :      0 (0.00%)
 # jobs unsubmitted :      0 (0.00%)
</programlisting>

    <para>If the workflow had failed you would see something like this:</para>

    <programlisting>$ <emphasis role="bold">pegasus-analyzer submit/tutorial/pegasus/diamond/run0002</emphasis>
pegasus-analyzer: initializing...

**************************Summary*************************************

 Total jobs         :      7 (100.00%)
 # jobs succeeded   :      2 (28.57%)
 # jobs failed      :      1 (14.29%)
 # jobs unsubmitted :      4 (57.14%)

**********************Failed jobs' details****************************

====================preprocess_ID0000001==============================

 last state: POST_SCRIPT_FAILED
       site: hpcc
submit file: preprocess_ID0000001.sub
output file: preprocess_ID0000001.out.003
 error file: preprocess_ID0000001.err.003

-----------------------Task #1 - Summary-----------------------------

site        : hpcc
hostname    : hpc-200.usc.edu
executable  : /home/tutorial/bin/preprocess
arguments   : -i f.a -o f.b1 -o f.b2
exitcode    : -128
working dir : -

-------------Task #1 - preprocess - ID0000001 - stderr---------------

FATAL: The main job specification is invalid or missing.
</programlisting>

    <para>In this example I removed the <filename>bin/preprocess</filename>
    executable and re-planned/re-submitted the workflow (that is why the
    command has run0002). The output of <literal>pegasus-analyzer</literal>
    indicates that the preprocess task failed with an error message that
    indicates that the executable could not be found.</para>
  </section>

  <section>
    <title>Collecting Statistics</title>

    <para>The <literal>pegasus-statistics</literal> command can be used to
    gather statistics about the runtime of the workflow and its jobs. The
    <literal>-s all</literal> argument tells the program to generate all
    statistics it knows how to calculate:</para>

    <programlisting>$ <emphasis role="bold">pegasus-statistics –s all </emphasis><emphasis
        role="bold">work/dags/userXX/pegasus/blackdiamond/run0001</emphasis>

#
# Pegasus Workflow Management System - http://pegasus.isi.edu
#
# Workflow summary:
#   Summary of the workflow execution. It shows total
#   tasks/jobs/sub workflows run, how many succeeded/failed etc.
#   In case of hierarchical workflow the calculation shows the
#   statistics across all the sub workflows.It shows the following
#   statistics about tasks, jobs and sub workflows.
#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.
#     * Failed - total count of failed tasks/jobs/sub workflows.
#     * Incomplete - total count of tasks/jobs/sub workflows that are
#       not in succeeded or failed state. This includes all the jobs
#       that are not submitted, submitted but not completed etc. This
#       is calculated as  difference between 'total' count and sum of
#       'succeeded' and 'failed' count.
#     * Total - total count of tasks/jobs/sub workflows.
#     * Retries - total retry count of tasks/jobs/sub workflows.
#     * Total+Retries - total count of tasks/jobs/sub workflows executed
#       during workflow run. This is the cumulative of retries,
#       succeeded and failed count.
# Workflow wall time:
#   The walltime from the start of the workflow execution to the end as
#   reported by the DAGMAN.In case of rescue dag the value is the
#   cumulative of all retries.
# Workflow cumulative job wall time:
#   The sum of the walltime of all jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the walltime value includes jobs from the sub workflows as well.
# Cumulative job walltime as seen from submit side:
#   The sum of the walltime of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job walltime, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the walltime value includes jobs
#   from the sub workflows as well.
------------------------------------------------------------------------------
Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries
Tasks          4         0       0           4         0         4            
Jobs           22        0       0           22        0         22           
Sub-Workflows  0         0       0           0         0         0            
------------------------------------------------------------------------------

Workflow wall time                               : 15 mins, 49 secs
Workflow cumulative job wall time                : 4 mins, 23 secs
Cumulative job walltime as seen from submit side : 5 mins, 12 secs

Summary                       : work/dags/userXX/pegasus/blackdiamond/run0001/statistics/summary.txt
Workflow execution statistics : work/dags/userXX/pegasus/blackdiamond/run0001/statistics/workflow.txt
Job instance statistics       : work/dags/userXX/pegasus/blackdiamond/run0001/statistics/jobs.txt
Transformation statistics     : work/dags/userXX/pegasus/blackdiamond/run0001/statistics/breakdown.txt
Time statistics               : work/dags/userXX/pegasus/blackdiamond/run0001/statistics/time.txt

************************************************************************
</programlisting>

    <para>The output of <literal>pegasus-statistics</literal> contains many
    definitions to help users understand what all of the values reported mean.
    Among these are the total wall time of the workflow, which is the time
    from when the workflow was submitted until it finished, and the total
    cumulative job wall time, which is the sum of the runtimes of all the
    jobs.</para>

    <para>The <literal>pegasus-statistics</literal> command also writes out
    several reports in the <filename>statistics</filename> subdirectory of the
    workflow submit directory:</para>

    <programlisting>$ <emphasis role="bold">ls </emphasis><emphasis
        role="bold">work/dags/userXX/pegasus/blackdiamond/run0001</emphasis>
breakdown.csv  jobs.txt          summary.txt         time.txt
breakdown.txt  summary-time.csv  time-per-host.csv   workflow.csv
jobs.csv       summary.csv       time.csv            workflow.txt</programlisting>

    <para>The file <filename>breakdown.txt</filename>, for example, has min,
    max, and mean runtimes for each transformation:</para>

    <programlisting>$ <emphasis role="bold">more </emphasis><emphasis
        role="bold">work/dags/userXX/pegasus/blackdiamond/run0001/breakdown.txt</emphasis>
# 022658c8-c588-4784-abfa-d31b55259ebe (blackdiamond)
Transformation           Count     Succeeded Failed  Min       Max       Mean      Total     
dagman::post             22        22        0       5.0       6.0       5.136     113.0     
pegasus::analyze:4.0     1         1         0       60.019    60.019    60.019    60.019    
pegasus::dirmanager      1         1         0       0.085     0.085     0.085     0.085     
pegasus::findrange:4.0   2         2         0       60.014    60.016    60.015    120.029   
pegasus::preprocess:4.0  1         1         0       60.022    60.022    60.022    60.022    
pegasus::rc-client       5         5         0       0.596     0.708     0.649     3.246     
pegasus::transfer        9         9         0       2.063     2.465     2.244     20.196    
system::chmod            3         3         0       0.001     0.002     0.002     0.006     


# All (All)
Transformation           Count     Succeeded  Failed  Min        Max        Mean      Total     
dagman::post             22        22         0       5.0        6.0        5.136     113.0     
pegasus::analyze:4.0     1         1          0       60.019     60.019     60.019    60.019    
pegasus::dirmanager      1         1          0       0.085      0.085      0.085     0.085     
pegasus::findrange:4.0   2         2          0       60.014     60.016     60.015    120.029   
pegasus::preprocess:4.0  1         1          0       60.022     60.022     60.022    60.022    
pegasus::rc-client       5         5          0       0.596      0.708      0.649     3.246     
pegasus::transfer        9         9          0       2.063      2.465      2.244     20.196    
system::chmod            3         3          0       0.001      0.002      0.002     0.006  </programlisting>

    <para>In this case, because the example transformation sleeps for 30
    seconds, the min, mean, and max runtimes for each of the analyze,
    findrange, and preprocess transformations are all close to 30.</para>
  </section>

  <section>
    <title>Running the whole workflow as an MPI job</title>

    <para>Often, users have lots of short running single processor jobs in
    their workflow, that if submitted individually to the underlying PBS
    cluster take a long time to execute, as each job sits in the PBS queue.
    For example in our previous example, each job in the blackdiamond workflow
    actually runs for a minute each. However, since each job is submitted as a
    separate job to PBS, each job sits in the cluster PBS queue before it is
    executed. In order to alleviate this, it makes sense to cluster the short
    running jobs together. Pegasus allows users to cluster tasks in their
    workflow into larger chunks, and then execute them using a MPI based
    master worker tool called <emphasis><emphasis
    role="bold">pegasus-mpi-cluster</emphasis></emphasis> .</para>

    <para>In this example, we take the same blackdiamond workflow that we ran
    previously and now run it using PMC where the whole workflow is clustered
    into a single MPI job. In order to tell Pegasus to cluster the jobs we
    have to do the following</para>

    <orderedlist>
      <listitem>
        <para>Tell Pegasus what jobs are clustered. In this example, we do it
        by annotating the DAX with a special pegasus profile called label. In
        the DAX generator BlackDiamondDAX.java you will see the
        following</para>

        <programlisting>        // Add a preprocess job
        System.out.println( "Adding preprocess job..." );
        Job j1 = new Job("j1", "pegasus", "preprocess", "4.0");
        j1.addArgument("-a preprocess -T 60 -i ").addArgument(fa);
        ...
        <emphasis role="bold">//associate the label with the job. all jobs with same label
        //are run with PMC when doing job clustering
        j1.addProfile( "pegasus", "label", "p1");</emphasis>
        
        dax.addJob(j1);</programlisting>
      </listitem>

      <listitem>
        <para>Tell pegasus that it has to do job clustering and what
        executable to use for job clustering.</para>

        <para>To do this, you do the following</para>

        <itemizedlist>
          <listitem>
            <para>In pegasus.conf file specify the property <emphasis
            role="bold">pegasus.job.aggregator mpiexec</emphasis></para>
          </listitem>

          <listitem>
            <para>In the transformation catalog, specify the path to the
            clustering executable. In this case, it is a wrapper around PMC
            that does mpiexec on pegasus-mpi-cluster. In conf/tc.dat you can
            see the last entry as</para>

            <programlisting>[userXX@hpc-pegasus mpi-hello-world]$ <emphasis
                role="bold">tail conf/tc.dat</emphasis>


# pegasus mpi clustering executable
tr pegasus::mpiexec{
    site hpcc {
        pfn "/home/rcf-proj/gmj/pegasus/SOFTWARE/pegasus/pegasus-mpi-cluster-wrapper"
        arch "x86_64"
        os "linux"
        type "INSTALLED"
        profile pegasus "clusters.size" "2" 

        #the various parameters to specify the size of the MPI job
        #in which the workflow runs
        profile globus "jobtype" "mpi"
        profile globus "maxwalltime" "2880"
        # specfiy the ppn parameter.
        profile globus "xcount" "4:IB"
        # specify the nodes parameter
        profile globus "hostcount" "1"
        #specify the pmem parameter
        profile globus "maxmemory" "1gb"
        #specify the mem parameter
        profile globus "totalmemory" "16gb"

    }
}
</programlisting>

            <para>The profiles tell Pegasus that the PMC executable needs to
            be run on 4 processors on a single node, process per process as
            1GB and total memory on the node as 16GB.</para>
          </listitem>
        </itemizedlist>
      </listitem>

      <listitem>
        <para>Lastly, while planning the workflow we add <emphasis
        role="bold">--cluster </emphasis>option to pegasus-plan. That is what
        we have in plan_cluster_dax.sh file.</para>

        <programlisting>$ <emphasis role="bold">cat plan_cluster_dax.sh</emphasis>

#!/bin/sh

set -e

# plan and submit the  workflow
pegasus-plan \
    --conf pegasus.conf \
    --sites hpcc \
    --output-site local \
    --dir work/dags \
    --dax diamond.dax \
    -v \
    --force \
    --nocleanup \
    --cluster label
</programlisting>
      </listitem>
    </orderedlist>

    <para><emphasis role="bold">Let us now plan and run the
    workflow.</emphasis></para>

    <programlisting>[userXX@hpc-pegasus mpi-hello-world]$ <emphasis
        role="bold">./<emphasis role="bold">plan_cluster_dax.sh</emphasis></emphasis>

[vahi@hpc-pegasus blackdiamond-hpcc-sharedfs-example]$ ./plan_cluster_dax.sh
2014.12.18 15:29:25.877 PST: [WARNING]  --nocleanup option is deprecated. Use --cleanup none   
2014.12.18 15:29:25.885 PST: [INFO]  Planner invoked with following arguments --conf pegasus.conf --sites hpcc --output-site local --dir work/dags --dax diamond.dax -v --force --nocleanup --cluster label --submit  
2014.12.18 15:29:26.304 PST: [INFO] event.pegasus.parse.dax dax.id /auto/rcf-40/vahi/tutorial/blackdiamond-hpcc-sharedfs-example/diamond.dax  - STARTED 
2014.12.18 15:29:26.306 PST: [INFO] event.pegasus.parse.dax dax.id /auto/rcf-40/vahi/tutorial/blackdiamond-hpcc-sharedfs-example/diamond.dax  (0.002 seconds) - FINISHED 
2014.12.18 15:29:26.309 PST: [INFO] event.pegasus.parse.dax dax.id /auto/rcf-40/vahi/tutorial/blackdiamond-hpcc-sharedfs-example/diamond.dax  - STARTED 
2014.12.18 15:29:26.349 PST: [INFO] event.pegasus.add.data-dependencies dax.id blackdiamond_0  - STARTED 
2014.12.18 15:29:26.350 PST: [INFO] event.pegasus.add.data-dependencies dax.id blackdiamond_0  (0.001 seconds) - FINISHED 
2014.12.18 15:29:26.350 PST: [INFO] event.pegasus.parse.dax dax.id /auto/rcf-40/vahi/tutorial/blackdiamond-hpcc-sharedfs-example/diamond.dax  (0.041 seconds) - FINISHED 
2014.12.18 15:29:26.386 PST: [INFO] event.pegasus.stampede.events dax.id blackdiamond_0  - STARTED 
2014.12.18 15:29:26.440 PST: [INFO] event.pegasus.stampede.events dax.id blackdiamond_0  (0.054 seconds) - FINISHED 
2014.12.18 15:29:26.442 PST: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - STARTED 
2014.12.18 15:29:26.479 PST: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - STARTED 
2014.12.18 15:29:26.492 PST: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  (0.013 seconds) - FINISHED 
2014.12.18 15:29:26.509 PST: [INFO] event.pegasus.cluster dax.id blackdiamond_0  - STARTED 
2014.12.18 15:29:26.542 PST: [INFO]  Starting Graph Traversal 
2014.12.18 15:29:26.544 PST: [INFO]  Starting Graph Traversal - DONE 
2014.12.18 15:29:26.549 PST: [INFO]  Determining relations between partitions 
2014.12.18 15:29:26.549 PST: [INFO]  Determining relations between partitions - DONE 
2014.12.18 15:29:26.549 PST: [INFO] event.pegasus.cluster dax.id blackdiamond_0  (0.04 seconds) - FINISHED 
2014.12.18 15:29:26.554 PST: [INFO]  Grafting transfer nodes in the workflow 
2014.12.18 15:29:26.554 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - STARTED 
2014.12.18 15:29:26.651 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  (0.097 seconds) - FINISHED 
2014.12.18 15:29:26.651 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - STARTED 
2014.12.18 15:29:26.660 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  (0.009 seconds) - FINISHED 
2014.12.18 15:29:26.660 PST: [INFO] event.pegasus.refinement dax.id blackdiamond_0  (0.218 seconds) - FINISHED 
2014.12.18 15:29:26.706 PST: [INFO]  Generating codes for the executable workflow 
2014.12.18 15:29:26.707 PST: [INFO] event.pegasus.code.generation dax.id blackdiamond_0  - STARTED 
2014.12.18 15:29:26.920 PST: [INFO] event.pegasus.code.generation dax.id blackdiamond_0  (0.213 seconds) - FINISHED 
2014.12.18 15:29:27.403 PST:   Submitting job(s). 
2014.12.18 15:29:27.409 PST:   1 job(s) submitted to cluster 2290. 
2014.12.18 15:29:27.414 PST:    
2014.12.18 15:29:27.420 PST:   ----------------------------------------------------------------------- 
2014.12.18 15:29:27.425 PST:   File for submitting this DAG to Condor           : blackdiamond-0.dag.condor.sub 
2014.12.18 15:29:27.430 PST:   Log of DAGMan debugging messages                 : blackdiamond-0.dag.dagman.out 
2014.12.18 15:29:27.436 PST:   Log of Condor library output                     : blackdiamond-0.dag.lib.out 
2014.12.18 15:29:27.441 PST:   Log of Condor library error messages             : blackdiamond-0.dag.lib.err 
2014.12.18 15:29:27.446 PST:   Log of the life of condor_dagman itself          : blackdiamond-0.dag.dagman.log 
2014.12.18 15:29:27.452 PST:    
2014.12.18 15:29:27.457 PST:   ----------------------------------------------------------------------- 
2014.12.18 15:29:27.462 PST:    
2014.12.18 15:29:27.468 PST:   Your workflow has been started and is running in the base directory: 
2014.12.18 15:29:27.473 PST:    
2014.12.18 15:29:27.478 PST:     /auto/rcf-40/vahi/tutorial/blackdiamond-hpcc-sharedfs-example/work/dags/vahi/pegasus/blackdiamond/run0002 
2014.12.18 15:29:27.483 PST:    
2014.12.18 15:29:27.489 PST:   *** To monitor the workflow you can run *** 
2014.12.18 15:29:27.494 PST:    
2014.12.18 15:29:27.499 PST:     pegasus-status -l /auto/rcf-40/vahi/tutorial/blackdiamond-hpcc-sharedfs-example/work/dags/vahi/pegasus/blackdiamond/run0002 
2014.12.18 15:29:27.505 PST:    
2014.12.18 15:29:27.510 PST:   *** To remove your workflow run *** 
2014.12.18 15:29:27.515 PST:    
2014.12.18 15:29:27.521 PST:     pegasus-remove /auto/rcf-40/vahi/tutorial/blackdiamond-hpcc-sharedfs-example/work/dags/vahi/pegasus/blackdiamond/run0002 
2014.12.18 15:29:27.526 PST:    
2014.12.18 15:29:28.334 PST:   Time taken to execute is 1.708 seconds 
2014.12.18 15:29:28.334 PST: [INFO] event.pegasus.planner planner.version 4.4.1cvs  (2.463 seconds) - FINISHED </programlisting>

    <para>This is what the diamond workflow looks like after Pegasus has
    finished planning the DAX:</para>

    <figure>
      <title>Clustered Diamond DAG</title>

      <mediaobject>
        <imageobject>
          <imagedata contentwidth="70%"
                     fileref="images/concepts-clustered-diamond-dag.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>You can see that instead of 4 jobs making up the diamond have been
    replaced by a single merge_p1 job, that is executed as a MPI job.</para>
  </section>

  <section>
    <title>MPI Workflows</title>

    <para>You can also submit workflows that have MPI jobs to the HPCC cluster
    using Pegasus. The MPI hello world example in the tutorial directory. This
    is a more canned example, where there is a submit script that creates the
    workflow, the catalogs and plans and submits the workflow.</para>

    <para>Before running this example, we have to compile the sample MPI hello
    world program.</para>

    <programlisting>$ <emphasis role="bold">cd ~/tutorial/mpi-hello-world
</emphasis>$ <emphasis role="bold">source /usr/usc/openmpi/default/setup.sh</emphasis>
$ <emphasis role="bold">make</emphasis>
mpicc  -O pegasus-mpi-hw.c -c -o pegasus-mpi-hw.o
mpicc  pegasus-mpi-hw.o -o pegasus-mpi-hw </programlisting>

    <para><emphasis role="bold">Generate the DAX and plan and submit the
    workflow</emphasis></para>

    <programlisting>[userXX@hpc-pegasus mpi-hello-world]$ <emphasis
        role="bold">./submit</emphasis>

Work directory: /home/rcf-40/userXX/tutorial/mpi-hello-world/work/2014-12-08_152027


Creating the wrapper for mpi job

Generating the DAX

Generating the Site Catalog

Planning and submitting the workflow
2014.12.08 15:20:28.070 PST: [WARNING]  --nocleanup option is deprecated. Use --cleanup none   
2014.12.08 15:20:28.085 PST: [INFO]  Planner invoked with following arguments --conf pegasus.conf --sites hpcc --output-site local --dir dags --dax mpi-hw.dax -v --nocleanup --submit  
2014.12.08 15:20:28.451 PST: [INFO] event.pegasus.parse.dax dax.id /auto/rcf-40/userXX/tutorial/mpi-hello-world/work/2014-12-08_152027/mpi-hw.dax  - STARTED 
2014.12.08 15:20:28.453 PST: [INFO] event.pegasus.parse.dax dax.id /auto/rcf-40/userXX/tutorial/mpi-hello-world/work/2014-12-08_152027/mpi-hw.dax  (0.002 seconds) - FINISHED 
2014.12.08 15:20:28.456 PST: [INFO] event.pegasus.parse.dax dax.id /auto/rcf-40/userXX/tutorial/mpi-hello-world/work/2014-12-08_152027/mpi-hw.dax  - STARTED 
2014.12.08 15:20:28.497 PST: [INFO] event.pegasus.add.data-dependencies dax.id mpi-hello-world_0  - STARTED 
2014.12.08 15:20:28.497 PST: [INFO] event.pegasus.add.data-dependencies dax.id mpi-hello-world_0  (0.0 seconds) - FINISHED 
2014.12.08 15:20:28.497 PST: [INFO] event.pegasus.parse.dax dax.id /auto/rcf-40/userXX/tutorial/mpi-hello-world/work/2014-12-08_152027/mpi-hw.dax  (0.041 seconds) - FINISHED 
2014.12.08 15:20:28.556 PST: [INFO] event.pegasus.stampede.events dax.id mpi-hello-world_0  - STARTED 
2014.12.08 15:20:28.618 PST: [INFO] event.pegasus.stampede.events dax.id mpi-hello-world_0  (0.062 seconds) - FINISHED 
2014.12.08 15:20:28.620 PST: [INFO] event.pegasus.refinement dax.id mpi-hello-world_0  - STARTED 
2014.12.08 15:20:28.664 PST: [INFO] event.pegasus.reduce dax.id mpi-hello-world_0  - STARTED 
2014.12.08 15:20:28.666 PST: [INFO]  Nodes/Jobs Deleted from the Workflow during reduction  
2014.12.08 15:20:28.666 PST: [INFO]  Nodes/Jobs Deleted from the Workflow during reduction  - DONE 
2014.12.08 15:20:28.666 PST: [INFO] event.pegasus.reduce dax.id mpi-hello-world_0  (0.002 seconds) - FINISHED 
2014.12.08 15:20:28.666 PST: [INFO] event.pegasus.siteselection dax.id mpi-hello-world_0  - STARTED 
2014.12.08 15:20:28.678 PST: [INFO] event.pegasus.siteselection dax.id mpi-hello-world_0  (0.012 seconds) - FINISHED 
2014.12.08 15:20:28.704 PST: [INFO]  Grafting transfer nodes in the workflow 
2014.12.08 15:20:28.704 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id mpi-hello-world_0  - STARTED 
2014.12.08 15:20:28.792 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id mpi-hello-world_0  (0.088 seconds) - FINISHED 
2014.12.08 15:20:28.792 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id mpi-hello-world_0  - STARTED 
2014.12.08 15:20:28.799 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id mpi-hello-world_0  (0.007 seconds) - FINISHED 
2014.12.08 15:20:28.799 PST: [INFO] event.pegasus.refinement dax.id mpi-hello-world_0  (0.179 seconds) - FINISHED 
2014.12.08 15:20:28.856 PST: [INFO]  Generating codes for the executable workflow 
2014.12.08 15:20:28.857 PST: [INFO] event.pegasus.code.generation dax.id mpi-hello-world_0  - STARTED 
2014.12.08 15:20:29.063 PST: [INFO] event.pegasus.code.generation dax.id mpi-hello-world_0  (0.206 seconds) - FINISHED 
2014.12.08 15:20:29.579 PST:   Submitting job(s). 
2014.12.08 15:20:29.584 PST:   1 job(s) submitted to cluster 1995. 
2014.12.08 15:20:29.590 PST:    
2014.12.08 15:20:29.595 PST:   ----------------------------------------------------------------------- 
2014.12.08 15:20:29.600 PST:   File for submitting this DAG to Condor           : mpi-hello-world-0.dag.condor.sub 
2014.12.08 15:20:29.606 PST:   Log of DAGMan debugging messages                 : mpi-hello-world-0.dag.dagman.out 
2014.12.08 15:20:29.611 PST:   Log of Condor library output                     : mpi-hello-world-0.dag.lib.out 
2014.12.08 15:20:29.616 PST:   Log of Condor library error messages             : mpi-hello-world-0.dag.lib.err 
2014.12.08 15:20:29.622 PST:   Log of the life of condor_dagman itself          : mpi-hello-world-0.dag.dagman.log 
2014.12.08 15:20:29.627 PST:    
2014.12.08 15:20:29.632 PST:   ----------------------------------------------------------------------- 
2014.12.08 15:20:29.638 PST:    
2014.12.08 15:20:29.643 PST:   Your workflow has been started and is running in the base directory: 
2014.12.08 15:20:29.648 PST:    
2014.12.08 15:20:29.654 PST:      work/2014-12-08_152027/dags/userXX/pegasus/mpi-hello-world/run0001 
2014.12.08 15:20:29.659 PST:    
2014.12.08 15:20:29.664 PST:   *** To monitor the workflow you can run *** 
2014.12.08 15:20:29.670 PST:    
2014.12.08 15:20:29.675 PST:     pegasus-status -l work/2014-12-08_152027/dags/userXX/pegasus/mpi-hello-world/run0001 
2014.12.08 15:20:29.681 PST:    
2014.12.08 15:20:29.686 PST:   *** To remove your workflow run *** 
2014.12.08 15:20:29.691 PST:    
2014.12.08 15:20:29.697 PST:     pegasus-remove  work/2014-12-08_152027/dags/userXX/pegasus/mpi-hello-world/run0001 
2014.12.08 15:20:29.702 PST:    
2014.12.08 15:20:29.978 PST:   Time taken to execute is 1.741 seconds 
2014.12.08 15:20:29.979 PST: [INFO] event.pegasus.planner planner.version 4.4.0  (1.922 seconds) - FINISHED 
</programlisting>

    <para>This example, uses the Python DAX API to generate the workflow DAX.
    In addition, the locations of the input file and the executables i.e the
    replica catalog and the transformation catalog are contained within the
    generated DAX itself.</para>

    <para>Lets look at the python DAX generator</para>

    <para>The code has 5 sections:</para>

    <orderedlist>
      <listitem>
        <para>A few system libraries and the Pegasus.DAX3 library are
        imported. The search path is modified to include the directory with
        the Pegasus Python library.</para>
      </listitem>

      <listitem>
        <para>The name for the DAX output file is retrieved from the
        arguments.</para>
      </listitem>

      <listitem>
        <para>A new ADAG object is created. This is the main object to which
        jobs and dependencies are added.</para>
      </listitem>

      <listitem>
        <para>Jobs and files are added. The 4 jobs in the diagram above are
        added and the 6 files are referenced. Arguments are defined using
        strings and File objects. The input and output files are defined for
        each job. This is an important step, as it allows Pegasus to track the
        files, and stage the data if necessary. Workflow outputs are tagged
        with “transfer=true”.</para>
      </listitem>

      <listitem>
        <para>Dependencies are added. These are shown as arrows in the diagram
        above. They define the parent/child relationships between the jobs.
        When the workflow is executing, the order in which the jobs will be
        run is determined by the dependencies between them.</para>
      </listitem>
    </orderedlist>

    <programlisting>$ <emphasis role="bold">more</emphasis><emphasis
        role="bold"> mpi-hw.py</emphasis> 
...

</programlisting>

    <para>The generated DAX mpi-hw.dax has a single job that takes in an input
    file and generates an output file.</para>

    <programlisting>[userXX@hpc-pegasus mpi-hello-world]$<emphasis role="bold"> more work/*/mpi-hw.dax </emphasis>


&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated: 2014-12-08 15:20:27.465657 --&gt;
&lt;!-- generated by: userXX --&gt;
&lt;!-- generator: python --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus
.isi.edu/schema/dax-3.4.xsd" version="3.4" name="mpi-hello-world"&gt;

   &lt;!-- this section is a substitute for a separate replica catalog file --&gt;
   &lt;file name="fin"&gt;
      &lt;pfn url="file:///auto/rcf-40/userXX/tutorial/mpi-hello-world/work/2014-12-08_152027/f.in" site="hpcc"/&gt;
   &lt;/file&gt;

   &lt;!-- this section is a substitute for a separate transformation catalog file --&gt;
   &lt;executable name="mpihw" namespace="pegasus" arch="x86_64" os="linux" installed="true"&gt;
        &lt;pfn url="file:///auto/rcf-40/userXX/tutorial/mpi-hello-world/work/2014-12-08_152027/mpi-hello-world-wrapper" site="hpcc"/&gt;
   &lt;/executable&gt;

   &lt;job id="ID0000001" namespace="pegasus" name="mpihw"&gt;
      &lt;argument&gt;-o  &lt;file name="f.out"/&gt;&lt;/argument&gt;
      
      &lt;!-- tell pegasus it is an MPI job --&gt;
      &lt;profile namespace="globus" key="jobtype"&gt;mpi&lt;/profile&gt;
     
      &lt;!-- the globus key hostCount is NODES --&gt;
      &lt;profile namespace="globus" key="xcount"&gt;16&lt;/profile&gt;
     
      &lt;!-- the globus key xcount is PROCS or PPN --&gt;
      &lt;profile namespace="globus" key="hostcount"&gt;1&lt;/profile&gt;
      
      &lt;!-- the globus key maxwalltime is WALLTIME in minutes --&gt;
      &lt;profile namespace="globus" key="maxwalltime"&gt;120&lt;/profile&gt;

      &lt;uses name="f.out" link="output"/&gt;
      &lt;uses name="fin" link="input"/&gt;
  &lt;/job&gt;
&lt;/adag&gt;
</programlisting>
  </section>
</chapter>
