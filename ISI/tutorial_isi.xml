<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial">
  <title>Tutorial</title>

  <section id="tutorial_introduction">
    <title>Introduction</title>

    <para>This tutorial will take you through the steps of running simple
    workflows using Pegasus Workflow Management System. Pegasus allows
    scientists to</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Automate </emphasis>their scientific
        computational work, as portable workflows. Pegasus enables scientists
        to construct workflows in abstract terms without worrying about the
        details of the underlying execution environment or the particulars of
        the low-level specifications required by the middleware (Condor,
        Globus, or Amazon EC2). It automatically locates the necessary input
        data and computational resources necessary for workflow execution. It
        cleans up storage as the workflow is executed so that data-intensive
        workflows have enough space to execute on storage-constrained
        resources.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Recover </emphasis>from failures at
        runtime. When errors occur, Pegasus tries to recover when possible by
        retrying tasks, and when all else fails, provides a rescue workflow
        containing a description of only the work that remains to be done. It
        also enables users to move computations from one resource to another.
        Pegasus keeps track of what has been done (provenance) including the
        locations of data used and produced, and which software was used with
        which parameters.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Debug </emphasis>- debug failures in their
        computations using a set of system provided debugging tools and an
        online workflow monitoring dashboard.</para>
      </listitem>
    </orderedlist>

    <para>This tutorial is intended for new users who want to get a quick
    overview of Pegasus concepts and usage. The accompanying tutorial VM comes
    configured with a variety of examples. The instructions listed here refer
    mainly to the simple diamond workflow example. The tutorial covers</para>

    <itemizedlist>
      <listitem>
        <para>submission of an already generated example workflow with
        Pegasus.</para>
      </listitem>

      <listitem>
        <para>how to use the Pegasus Workflow Dashboard for monitoring
        workflows.</para>
      </listitem>

      <listitem>
        <para>the command line tools for monitoring , debugging and generating
        statistics.</para>
      </listitem>

      <listitem>
        <para>recovery from failures</para>
      </listitem>

      <listitem>
        <para>creation of workflow using system provided API</para>
      </listitem>

      <listitem>
        <para>information catalogs configuration.</para>
      </listitem>
    </itemizedlist>

    <para>More information about the topics covered in this tutorial can be
    found in later chapters of this user's guide.</para>

    <para>All of the steps in this tutorial are performed on the command-line.
    The convention we will use for command-line input and output is to put
    things that you should type in bold, monospace font, and to put the output
    you should get in a normal weight, monospace font, like this:</para>

    <programlisting>[user@host dir]$ <emphasis role="bold">you type this</emphasis>
you get this</programlisting>

    <para>Where <literal>[user@host dir]$</literal> is the terminal prompt,
    the text you should type is “<literal>you type this</literal>”, and the
    output you should get is "<literal>you get this</literal>". The terminal
    prompt will be abbreviated as <literal>$</literal>. Because some of the
    outputs are long, we don’t always include everything. Where the output is
    truncated we will add an ellipsis '...' to indicate the omitted
    output.</para>

    <para><emphasis role="bold">If you are having trouble with this tutorial,
    or anything else related to Pegasus, you can contact the Pegasus Users
    mailing list at <email>pegasus-users@isi.edu</email> to get help. You can
    also contact us on our <ulink
    url="https://pegasus.isi.edu/support">support chatroom</ulink> on HipChat.
    </emphasis></para>
  </section>

  <section id="tutorial_started">
    <title>Getting Started</title>

    <para>Easiest way to start the tutorial is to login to workflow.isi.edu
    with the training account and password assigned to you.</para>

    <programlisting>$ <emphasis role="bold">ssh pegtrainXX@workflow.isi.edu</emphasis>
Enter pegtrainXX<emphasis role="bold">@</emphasis>workflow.isi.edu password</programlisting>

    <note>
      <para>For the purpose of this tutorial replace any instance of
      pegtrainXX with the user name assigned to you. To request access to a
      training account send email to
      <email>pegasus-support@isi.edu</email></para>
    </note>

    <para>The remainder of this tutorial will assume that you have a terminal
    open to the directory where the tutorial files are installed on
    workflow.isi.edu. These files are located in the training accounts home
    directory <emphasis
    role="bold"><filename>/nfs/ccg3/ccg/home/pegtrainXX/examples</filename></emphasis><filename>
    </filename>.</para>
  </section>

  <section id="tutorial_scientific_workflows">
    <title>What are Scientific Workflows</title>

    <para>Scientific workflows allow users to easily express multi-step
    computational tasks, for example retrieve data from an instrument or a
    database, reformat the data, and run an analysis. A scientific workflow
    describes the dependencies between the tasks and in most cases the
    workflow is described as a directed acyclic graph (DAG), where the nodes
    are tasks and the edges denote the task dependencies. A defining property
    for a scientific workflow is that it manages data flow. The tasks in a
    scientific workflow can be everything from short serial tasks to very
    large parallel tasks (MPI for example) surrounded by a large number of
    small, serial tasks used for pre- and post-processing.</para>

    <para>Workflows can vary from simple to complex. Below are some examples.
    In the figures below, the task are designated by circles/ellipses while
    the files created by the tasks are indicated by rectangles. Arrows
    indicate task dependencies.</para>

    <para><emphasis role="bold">Process Workflow</emphasis></para>

    <para>It consists of a single task that runs the `ls` command and
    generates a listing of the files in the `/` directory.</para>

    <figure>
      <title>Process ( Single Task ) Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentdepth="50%"
                     fileref="images/tutorial-single-job-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para><emphasis role="bold">Pipeline of Tasks</emphasis></para>

    <para>The pipeline workflow consists of two tasks linked together in a
    pipeline. The first job runs the `curl` command to fetch the Pegasus home
    page and store it as an HTML file. The result is passed to the `wc`
    command, which counts the number of lines in the HTML file. <figure>
        <title>Pipeline of Tasks</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="70%"
                       fileref="images/tutorial-pipeline-tasks-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Split Workflow</emphasis></para>

    <para>The split workflow downloads the Pegasus home page using the `curl`
    command, then uses the `split` command to divide it into 4 pieces. The
    result is passed to the `wc` command to count the number of lines in each
    piece.<figure>
        <title>Split Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/tutorial-split-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Merge Workflow</emphasis></para>

    <para>The merge workflow runs the `ls` command on several */bin
    directories and passes the results to the `cat` command, which merges the
    files into a single listing. The merge workflow is an example of a
    parameter sweep over arguments.<figure>
        <title>Merge Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/tutorial-merge-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Diamond Workflow</emphasis></para>

    <para>The diamond workflow runs combines the split and merge workflow
    patterns to create a more complex workflow.</para>

    <figure>
      <title>Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="70%"
                     fileref="images/tutorial-diamond-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para><emphasis role="bold">Complex Workflows</emphasis></para>

    <para>The above examples can be used as building blocks for much complex
    workflows. Some of these are showcased on the <ulink
    url="https://pegasus.isi.edu/applications">Pegasus Applications
    page</ulink>.</para>
  </section>

  <section id="tutorial_submitting_wf">
    <title>Submitting an Example Workflow</title>

    <para>The tutorial comes configured with a variety of examples. For the
    rest of the tutorial we will be using the split example. Other examples
    are also structured similarly.</para>

    <programlisting>$ <emphasis role="bold">cd ~/examples</emphasis>
$ ls 
﻿diamond  merge  pipeline  process  split
</programlisting>

    <para>To start the tutorial, we will run an already generated split
    workflow through Pegasus. The split workflow looks like this</para>

    <figure>
      <title>Split Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="70%"
                     fileref="images/tutorial-split-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The input workflow description for Pegasus is called the DAX. In
    each of the example directories, there is a file with a .dax extension. We
    will cover on how to create the DAX programmatically later in the <link
    linkend="tutorial_wf_generation">tutorial</link>. Pegasus takes in the DAX
    and generates an executable HTCondor workflow that is run on an execution
    site.</para>

    <para>The <literal>pegasus-plan</literal> command is used to submit the
    workflow through Pegasus. If you have trouble copy pasting into the VM,
    you can also invoke the <filename>plan_dax.sh</filename> wrapper script
    that has all of the arguments required for the split workflow:</para>

    <para>The pegasus-plan command takes in the input workflow ( DAX file
    specified by --dax option) and maps the abstract DAX to one or more
    execution sites, and submits the generated executable workflow for
    execution. The options to pegasus-plan in this example tells
    Pegasus</para>

    <itemizedlist>
      <listitem>
        <para>the workflow to run</para>
      </listitem>

      <listitem>
        <para>the input directory where the inputs are placed</para>
      </listitem>

      <listitem>
        <para>the output directory where the outputs are placed</para>
      </listitem>

      <listitem>
        <para>the site on which the workflow has to execute</para>
      </listitem>
    </itemizedlist>

    <para>To plan the split workflow invoke the pegasus-plan command as
    follows</para>

    <programlisting><emphasis role="bold">$ pegasus-plan  --dax split.dax --input-dir ./input  --output-dir ./outputs --sites condorpool --submit
</emphasis>
﻿
2015.10.22 19:12:10.402 PDT:    
2015.10.22 19:12:10.409 PDT:   ----------------------------------------------------------------------- 
2015.10.22 19:12:10.414 PDT:   File for submitting this DAG to Condor           : split-0.dag.condor.sub 
2015.10.22 19:12:10.420 PDT:   Log of DAGMan debugging messages                 : split-0.dag.dagman.out 
2015.10.22 19:12:10.426 PDT:   Log of Condor library output                     : split-0.dag.lib.out 
2015.10.22 19:12:10.431 PDT:   Log of Condor library error messages             : split-0.dag.lib.err 
2015.10.22 19:12:10.436 PDT:   Log of the life of condor_dagman itself          : split-0.dag.dagman.log 
2015.10.22 19:12:10.442 PDT:    
2015.10.22 19:12:10.458 PDT:   ----------------------------------------------------------------------- 
2015.10.22 19:12:14.292 PDT:   Your database is compatible with Pegasus version: 4.5.3 
2015.10.22 19:12:15.198 PDT:   Submitting to condor split-0.dag.condor.sub 
2015.10.22 19:12:15.997 PDT:   Submitting job(s). 
2015.10.22 19:12:16.003 PDT:   1 job(s) submitted to cluster 111. 
2015.10.22 19:12:16.035 PDT:    
2015.10.22 19:12:16.055 PDT:   Your workflow has been started and is running in the base directory: 
2015.10.22 19:12:16.061 PDT:    
2015.10.22 19:12:16.071 PDT:     /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0001 
2015.10.22 19:12:16.078 PDT:    
2015.10.22 19:12:16.084 PDT:   *** To monitor the workflow you can run *** 
2015.10.22 19:12:16.090 PDT:    
2015.10.22 19:12:16.098 PDT:     pegasus-status -l /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0001 
2015.10.22 19:12:16.114 PDT:    
2015.10.22 19:12:16.119 PDT:   *** To remove your workflow run *** 
2015.10.22 19:12:16.125 PDT:    
2015.10.22 19:12:16.131 PDT:     pegasus-remove /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0001 
2015.10.22 19:12:16.137 PDT:    
2015.10.22 19:12:17.630 PDT:   Time taken to execute is 10.109 seconds</programlisting>

    <note>
      <para>The line in the output that starts with
      <literal>pegasus-status</literal> , contains the command you can use on
      the command line to monitor the status of the workflow . The path it
      contains is the path to the submit directory where all of the files
      required to submit and monitor the workflow are stored.</para>
    </note>

    <para>This is what the split workflow looks like after Pegasus has
    finished planning the DAX:</para>

    <figure>
      <title>Split DAG</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="100%"
                     fileref="images/tutorial-split-dag.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>For this workflow the only jobs Pegasus needs to add are a directory
    creation job, a stage-in job (for pegasus.html), and stage-out jobs (for
    wc count outputs). The cleanup jobs remove data that is no longer required
    as workflow executes.</para>
  </section>

  <section id="tutorial_wf_dashboard">
    <title>Workflow Dashboard for Monitoring and Debugging</title>

    <para>The Pegasus Dashboard is a web interface for monitoring and
    debugging workflows and is shipped with each Pegasus release. We will now
    use the web dashboard to monitor the status of the workflow.</para>

    <para>By default, the dashboard server can only monitor workflows run by
    the current user i.e. the user who is running the pegasus-service.</para>

    <para>To access the dashboard open in the webbrowser, the following
    link</para>

    <para><emphasis role="bold"> https://workflow.isi.edu:5000/u/pegtrainXX/
    </emphasis></para>

    <note>
      <para>Replace pegtrainXX with your username</para>
    </note>

    <para>When the webpage loads up, it will ask you for a username and a
    password. Log in with the same user name and password you used to log onto
    workflow.isi.edu</para>

    <para>The Dashboard's home page lists all workflows, which have been run
    by the current-user. The home page shows the status of each of the
    workflow i.e. Running/Successful/Failed/Failing. The home page lists only
    the top level workflows (Pegasus supports hierarchical workflows i.e.
    workflows within a workflow). The rows in the table are color coded</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">Green</emphasis>: indicates workflow
        finished successfully.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Red</emphasis>: indicates workflow
        finished with a failure.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Blue</emphasis>: indicates a workflow is
        currently running.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Gray</emphasis>: indicates a workflow that
        was archived.</para>
      </listitem>
    </itemizedlist>

    <figure>
      <title>Dashboard Home Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_home.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>To view details specific to a workflow, the user can click on
    corresponding workflow label. The workflow details page lists workflow
    specific information like workflow label, workflow status, location of the
    submit directory, etc. The details page also displays pie charts showing
    the distribution of jobs based on status.</para>

    <para>In addition, the details page displays a tab listing all
    sub-workflows and their statuses. Additional tabs exist which list
    information for all running, failed, successful, and failing jobs.</para>

    <para>The information displayed for a job depends on it's status. For
    example, the failed jobs tab displays the job name, exit code, links to
    available standard output, and standard error contents.</para>

    <figure>
      <title>Dashboard Workflow Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_workflow_details.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>To view details specific to a job the user can click on the
    corresponding job's job label. The job details page lists information
    relevant to a specific job. For example, the page lists information like
    job name, exit code, run time, etc.</para>

    <para>The job instance section of the job details page lists all attempts
    made to run the job i.e. if a job failed in its first attempt due to
    transient errors, but ran successfully when retried, the job instance
    section shows two entries; one for each attempt to run the job.</para>

    <para>The job details page also shows tab's for failed, and successful
    task invocations (Pegasus allows users to group multiple smaller task's
    into a single job i.e. a job may consist of one or more tasks)</para>

    <figure>
      <title>Dashboard Job Description Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_job_details.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The task invocation details page provides task specific information
    like task name, exit code, duration etc. Task details differ from job
    details, as they are more granular in nature.</para>

    <figure>
      <title>Dashboard Invocation Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_invocation_details.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The dashboard also has web pages for workflow statistics and
    workflow charts, which graphically renders information provided by the
    pegasus-statistics and pegasus-plots command respectively.</para>

    <para>The Statistics page shows the following statistics.</para>

    <orderedlist>
      <listitem>
        <para>Workflow level statistics</para>
      </listitem>

      <listitem>
        <para>Job breakdown statistics</para>
      </listitem>

      <listitem>
        <para>Job specific statistics</para>
      </listitem>
    </orderedlist>

    <figure>
      <title>Dashboard Statistics Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_statistics.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section id="tutorial_monitoring_cmd_tools">
    <title>Command line tools for Monitoring and Debugging</title>

    <para>Pegasus also comes with a series of command line tools that users
    can use to monitor and debug their workflows.</para>

    <itemizedlist>
      <listitem>
        <para>pegasus-status : monitor the status of the workflow</para>
      </listitem>

      <listitem>
        <para>pegasus-analyzer : debug a failed workflow</para>
      </listitem>

      <listitem>
        <para>pegasus-statistics : generate statistics from a workflow
        run.</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>pegasus-status - monitoring the workflow</title>

      <para>After the workflow has been submitted you can monitor it using the
      <literal>pegasus-status</literal> command:</para>

      <programlisting>$ <emphasis role="bold">pegasus-status ﻿tutorial/pegasus/split/run0001</emphasis>
﻿STAT  IN_STATE  JOB                                                                     
﻿Run      00:39  split-0 ( /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0001 )
Idle     00:03   ┗━split_ID0000001                                                      
Summary: 2 Condor jobs total (I:1 R:1)

UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
   14     0     0     1     0     2     0  11.8 Running *split-0.dag     </programlisting>

      <para>This command shows the workflow (split-0) and the running jobs (in
      the above output it shows the two findrange jobs). It also gives
      statistics on the number of jobs in each state and the percentage of the
      jobs in the workflow that have finished successfully.</para>

      <para>Use the <literal>watch</literal> command to continuously monitor
      the workflow:</para>

      <programlisting>$ <emphasis role="bold">pegasus-status -w﻿tutorial/pegasus/split/run0001</emphasis>
...</programlisting>

      <para>You should see all of the jobs in the workflow run one after the
      other. After a few minutes you will see:</para>

      <programlisting>﻿(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    0     0     0     0     0    17     0 100.0 Success *split-0.dag      </programlisting>

      <para>That means the workflow is finished successfully. You can type
      <literal>ctrl-c</literal> to terminate the <literal>watch</literal>
      command.</para>

      <para>If the workflow finished successfully you should see the output
      count files in the <filename>outputs</filename> directory. This file was
      created by the various transformations in the workflow.</para>

      <programlisting><emphasis role="bold">$﻿ls outputs/
</emphasis>count.txt.a  count.txt.b  count.txt.c  count.txt.d
</programlisting>

      <para>The locations of the output files are also registered in the
      rc.dat file.</para>

      <programlisting><emphasis role="bold">$ more rc.dat 
</emphasis># file-based replica catalog: 2015-10-22T20:04:26.171-07:00
count.txt.b file:///nfs/ccg3/ccg/home/pegtrainXX/examples/split/./outputs/count.txt.b site="local"
count.txt.d file:///nfs/ccg3/ccg/home/pegtrainXX/examples/split/./outputs/count.txt.d site="local"
count.txt.a file:///nfs/ccg3/ccg/home/pegtrainXX/examples/split/./outputs/count.txt.a site="local"
count.txt.c file:///nfs/ccg3/ccg/home/pegtrainXX/examples/split/./outputs/count.txt.c site="local"
</programlisting>

      <para/>
    </section>

    <section>
      <title>pegasus-analyzer - debug a failed workflow</title>

      <para>In the case that one or more jobs fails, then the output of the
      <literal>pegasus-status</literal> command above will have a non-zero
      value in the <literal>FAILURE</literal> column.</para>

      <para>You can debug the failure using the
      <literal>pegasus-analyzer</literal> command. This command will identify
      the jobs that failed and show their output. Because the workflow
      succeeded, <literal>pegasus-analyzer</literal> will only show some basic
      statistics about the number of successful jobs:</para>

      <programlisting>$ <emphasis role="bold">pegasus-analyzer ﻿tutorial/pegasus/split/run0001</emphasis>
pegasus-analyzer: initializing...

****************************Summary***************************

 Total jobs         :      7 (100.00%)
 # jobs succeeded   :      7 (100.00%)
 # jobs failed      :      0 (0.00%)
 # jobs unsubmitted :      0 (0.00%)
</programlisting>

      <para>If the workflow had failed you would see something like
      this:</para>

      <programlisting>$ <emphasis role="bold">pegasus-analyzer tutorial/pegasus/split/run0002</emphasis>

﻿************************************Summary*************************************

 Submit Directory   : tutorial/pegasus/split/run0002
 Total jobs         :     17 (100.00%)
 # jobs succeeded   :      1 (5.88%)
 # jobs failed      :      1 (5.88%)
 # jobs unsubmitted :     15 (88.24%)

******************************Failed jobs' details******************************

==========================stage_in_local_condorpool_0_0==========================

 last state: POST_SCRIPT_FAILED
       site: local
submit file: stage_in_local_condorpool_0_0.sub
output file: stage_in_local_condorpool_0_0.out.001
 error file: stage_in_local_condorpool_0_0.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : unknown
executable  : /usr/bin/pegasus-transfer
arguments   :   --threads   2  
exitcode    : 1
working dir : /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0002

------------------Task #1 - pegasus::transfer - None - stdout-------------------

2015-10-22 21:13:50,970    INFO:  Reading URL pairs from stdin
2015-10-22 21:13:50,970    INFO:  PATH=/usr/bin:/bin
2015-10-22 21:13:50,970    INFO:  LD_LIBRARY_PATH=
2015-10-22 21:13:50,972    INFO:  1 transfers loaded
2015-10-22 21:13:50,972    INFO:  Sorting the tranfers based on transfer type and source/destination
2015-10-22 21:13:50,972    INFO:  --------------------------------------------------------------------------------
2015-10-22 21:13:50,972    INFO:  Starting transfers - attempt 1
2015-10-22 21:13:50,972    INFO:  Using 1 threads for this round of transfers
2015-10-22 21:13:53,845   ERROR:  Command exited with non-zero exit code (1): /usr/bin/scp -r -B -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/tutorial/.ssh/id_rsa -P 22 '/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html' 'tutorial@127.0.0.1:/home/tutorial/work/tutorial/pegasus/split/run0002/pegasus.html'
2015-10-22 21:15:55,911    INFO:  --------------------------------------------------------------------------------
2015-10-22 21:15:55,912    INFO:  Starting transfers - attempt 2
2015-10-22 21:15:55,912    INFO:  Using 1 threads for this round of transfers
2015-10-22 21:15:58,446   ERROR:  Command exited with non-zero exit code (1): /usr/bin/scp -r -B -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/tutorial/.ssh/id_rsa -P 22 '/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html' 'tutorial@127.0.0.1:/home/tutorial/work/tutorial/pegasus/split/run0002/pegasus.html'
2015-10-22 21:16:40,468    INFO:  --------------------------------------------------------------------------------
2015-10-22 21:16:40,469    INFO:  Starting transfers - attempt 3
2015-10-22 21:16:40,469    INFO:  Using 1 threads for this round of transfers
2015-10-22 21:16:43,168   ERROR:  Command exited with non-zero exit code (1): /usr/bin/scp -r -B -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/tutorial/.ssh/id_rsa -P 22 '/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html' 'tutorial@127.0.0.1:/home/tutorial/work/tutorial/pegasus/split/run0002/pegasus.html'
2015-10-22 21:16:43,173    INFO:  --------------------------------------------------------------------------------
2015-10-22 21:16:43,173    INFO:  Stats: no local files in the transfer set
2015-10-22 21:16:43,173 CRITICAL:  Some transfers failed! See above, and possibly stderr.


-------------Task #1 - pegasus::transfer - None - Kickstart stderr--------------

Warning: Permanently added '127.0.0.1' (RSA) to the list of known hosts.<emphasis
          role="bold">
/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html: No such file or directory</emphasis>
..
<emphasis role="bold">/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html: No such file or directory
</emphasis></programlisting>

      <para>In this example, we removed one of the input files. We will cover
      this in the recovery section. The output of
      <literal>pegasus-analyzer</literal> indicates that pegasus.html file
      could not be found.</para>
    </section>

    <section>
      <title>pegasus-statistics - collect statistics about a workflow
      run</title>

      <para>The <literal>pegasus-statistics</literal> command can be used to
      gather statistics about the runtime of the workflow and its jobs. The
      <literal>-s all</literal> argument tells the program to generate all
      statistics it knows how to calculate:</para>

      <programlisting>$ <emphasis role="bold">pegasus-statistics –s all ﻿tutorial/pegasus/split/run0001</emphasis>

#
# Pegasus Workflow Management System - http://pegasus.isi.edu
#
# Workflow summary:
#   Summary of the workflow execution. It shows total
#   tasks/jobs/sub workflows run, how many succeeded/failed etc.
#   In case of hierarchical workflow the calculation shows the
#   statistics across all the sub workflows.It shows the following
#   statistics about tasks, jobs and sub workflows.
#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.
#     * Failed - total count of failed tasks/jobs/sub workflows.
#     * Incomplete - total count of tasks/jobs/sub workflows that are
#       not in succeeded or failed state. This includes all the jobs
#       that are not submitted, submitted but not completed etc. This
#       is calculated as  difference between 'total' count and sum of
#       'succeeded' and 'failed' count.
#     * Total - total count of tasks/jobs/sub workflows.
#     * Retries - total retry count of tasks/jobs/sub workflows.
#     * Total+Retries - total count of tasks/jobs/sub workflows executed
#       during workflow run. This is the cumulative of retries,
#       succeeded and failed count.
# Workflow wall time:
#   The wall time from the start of the workflow execution to the end as
#   reported by the DAGMAN.In case of rescue dag the value is the
#   cumulative of all retries.
# Workflow cumulative job wall time:
#   The sum of the wall time of all jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job wall time as seen from submit side:
#   The sum of the wall time of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
# Cumulative job wall time as seen from submit side:
#   The sum of the wall time of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
# Workflow cumulative job badput wall time:
#   The sum of the wall time of all failed jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job badput wall time as seen from submit side:
#   The sum of the wall time of all failed jobs as reported by DAGMan.
#   This is similar to the regular cumulative job badput wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.

﻿------------------------------------------------------------------------------
Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries
Tasks          5         0       0           5         0         5            
Jobs           17        0       0           17        0         17           
Sub-Workflows  0         0       0           0         0         0            
------------------------------------------------------------------------------

Workflow wall time                                       : 2 mins, 6 secs
Workflow cumulative job wall time                        : 38 secs
Cumulative job wall time as seen from submit side        : 42 secs
Workflow cumulative job badput wall time                 : 
Cumulative job badput wall time as seen from submit side : 

Summary                       : tutorial/pegasus/split/run0001/statistics/summary.txt
Workflow execution statistics : tutorial/pegasus/split/run0001/statistics/workflow.txt
Job instance statistics       : tutorial/pegasus/split/run0001/statistics/jobs.txt
Transformation statistics     : tutorial/pegasus/split/run0001/statistics/breakdown.txt
Time statistics               : tutorial/pegasus/split/run0001/statistics/time.txt


</programlisting>

      <para>The output of <literal>pegasus-statistics</literal> contains many
      definitions to help users understand what all of the values reported
      mean. Among these are the total wall time of the workflow, which is the
      time from when the workflow was submitted until it finished, and the
      total cumulative job wall time, which is the sum of the runtimes of all
      the jobs.</para>

      <para>The <literal>pegasus-statistics</literal> command also writes out
      several reports in the <filename>statistics</filename> subdirectory of
      the workflow submit directory:</para>

      <programlisting>$ <emphasis role="bold">ls ﻿tutorial/pegasus/split/run0001/statistics/</emphasis>
breakdown.csv  jobs.txt          summary.txt         time.txt
breakdown.txt  summary-time.csv  time-per-host.csv   workflow.csv
jobs.csv       summary.csv       time.csv            workflow.txt</programlisting>

      <para>The file <filename>breakdown.txt</filename>, for example, has min,
      max, and mean runtimes for each transformation:</para>

      <programlisting>$ <emphasis role="bold">more ﻿tutorial/pegasus/split/run0001/statistics/breakdown.txt</emphasis>
# legends
# Transformation - name of the transformation.
# Count          - the number of times the invocations corresponding to
#                  the transformation was executed.
# Succeeded      - the count of the succeeded invocations corresponding
#                  to the transformation.
# Failed         - the count of the failed invocations corresponding to
#                  the transformation.
# Min(sec)       - the minimum invocation runtime value corresponding to
#                  the transformation.
# Max(sec)       - the maximum invocation runtime value corresponding to
#                  the transformation.
# Mean(sec)      - the mean of the invocation runtime corresponding to
#                  the transformation.
# Total(sec)     - the cumulative of invocation runtime corresponding to
#                  the transformation.

﻿# 773d8fa3-8bff-4f75-8e2b-38e2c904f803 (split)
Transformation           Count     Succeeded Failed  Min       Max       Mean      Total     
dagman::post             17        17        0       5.0       6.0       5.412     92.0      
pegasus::cleanup         6         6         0       1.474     3.178     2.001     12.008    
pegasus::dirmanager      1         1         0       2.405     2.405     2.405     2.405     
pegasus::rc-client       2         2         0       2.382     7.406     4.894     9.788     
pegasus::transfer        3         3         0       3.951     5.21      4.786     14.358    
split                    1         1         0       0.009     0.009     0.009     0.009     
wc                       4         4         0       0.005     0.029     0.012     0.047     


# All (All)
Transformation           Count     Succeeded  Failed  Min       Max       Mean      Total     
dagman::post             17        17         0       5.0       6.0       5.412     92.0      
pegasus::cleanup         6         6          0       1.474     3.178     2.001     12.008    
pegasus::dirmanager      1         1          0       2.405     2.405     2.405     2.405     
pegasus::rc-client       2         2          0       2.382     7.406     4.894     9.788     
pegasus::transfer        3         3          0       3.951     5.21      4.786     14.358    
split                    1         1          0       0.009     0.009     0.009     0.009     
wc                       4         4          0       0.005     0.029     0.012     0.047  
</programlisting>

      <para>In this case, because the example transformation sleeps for 30
      seconds, the min, mean, and max runtimes for each of the analyze,
      findrange, and preprocess transformations are all close to 30.</para>
    </section>
  </section>

  <section id="tutorial_failure_recovery">
    <title>Recovery from Failures</title>

    <para>Executing workflows in a distributed environment can lead to
    failures. Often, they are a result of the underlying infrastructure being
    temporarily unavailable, or errors in workflow setup such as incorrect
    executables specified, or input files being unavailable.</para>

    <para>In case of transient infrastructure failures such as a node being
    temporarily down in a cluster, Pegasus will automatically retry jobs in
    case of failure. After a set number of retries ( usually once) , a hard
    failure occurs, because of which workflow will eventually fail.</para>

    <para>In most of the cases, these errors are correctable ( either the
    resource comes back online or application errors are fixed) . Once the
    errors are fixed, you may not want to start a new workflow but instead
    start from the point of failure. In order to do this, you can submit the
    rescue workflows automatically created in case of failures. A rescue
    workflow contains only a description of only the work that remains to be
    done.</para>

    <section>
      <title>Submitting Rescue Workflows</title>

      <para>In this example, we will take our previously run workflow and
      introduce errors such that workflow we just executed fails at
      runtime.</para>

      <para>First we will move the pegasus.html file to
      pegasus.html.bad</para>

      <programlisting>$ ﻿mv input/pegasus.html input/pegasus.html.bak
</programlisting>

      <para>It now has an incorrect entry to pegasus.html</para>

      <programlisting><emphasis role="bold">$ more rc.dat
</emphasis>#bad location of pegasus.html

pegasus.html file:///nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html.bad site="local"
</programlisting>

      <para>We will now submit the workflow again.</para>

      <programlisting><emphasis role="bold">$ pegasus-plan  --dax split.dax  --output-dir ./outputs --sites condorpool --submit</emphasis>
2015.10.22 20:20:08.299 PDT:    
2015.10.22 20:20:08.307 PDT:   ----------------------------------------------------------------------- 
2015.10.22 20:20:08.312 PDT:   File for submitting this DAG to Condor           : split-0.dag.condor.sub 
2015.10.22 20:20:08.323 PDT:   Log of DAGMan debugging messages                 : split-0.dag.dagman.out 
2015.10.22 20:20:08.330 PDT:   Log of Condor library output                     : split-0.dag.lib.out 
2015.10.22 20:20:08.339 PDT:   Log of Condor library error messages             : split-0.dag.lib.err 
2015.10.22 20:20:08.346 PDT:   Log of the life of condor_dagman itself          : split-0.dag.dagman.log 
2015.10.22 20:20:08.352 PDT:    
2015.10.22 20:20:08.368 PDT:   ----------------------------------------------------------------------- 
2015.10.22 20:20:12.331 PDT:   Your database is compatible with Pegasus version: 4.5.3 
2015.10.22 20:20:13.326 PDT:   Submitting to condor split-0.dag.condor.sub 
2015.10.22 20:20:14.224 PDT:   Submitting job(s). 
2015.10.22 20:20:14.254 PDT:   1 job(s) submitted to cluster 168. 
2015.10.22 20:20:14.288 PDT:    
2015.10.22 20:20:14.297 PDT:   Your workflow has been started and is running in the base directory: 
2015.10.22 20:20:14.303 PDT:    
2015.10.22 20:20:14.309 PDT:     /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0002 
2015.10.22 20:20:14.315 PDT:    
2015.10.22 20:20:14.321 PDT:   *** To monitor the workflow you can run *** 
2015.10.22 20:20:14.326 PDT:    
2015.10.22 20:20:14.332 PDT:     pegasus-status -l /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0002 
2015.10.22 20:20:14.351 PDT:    
2015.10.22 20:20:14.369 PDT:   *** To remove your workflow run *** 
2015.10.22 20:20:14.376 PDT:    
2015.10.22 20:20:14.388 PDT:     pegasus-remove /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0002 
2015.10.22 20:20:14.397 PDT:    
2015.10.22 20:20:16.146 PDT:   Time taken to execute is 10.292 seconds 
</programlisting>

      <para>We will now monitor the workflow using the pegasus-status command
      till it fails. We will add -w option to pegasus-status to watch
      automatically till the workflow finishes</para>

      <programlisting><emphasis role="bold">$ ﻿</emphasis><emphasis
          role="bold">pegasus-status -w tutorial/pegasus/split/run0002</emphasis>
﻿(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
   15     0     0     0     0     1     1   5.9 Failure *split-0.dag                            
</programlisting>

      <para>We will now use the pegasus-analyzer command to determine what
      went wrong</para>

      <programlisting><emphasis role="bold">$ </emphasis>﻿<emphasis
          role="bold">pegasus-analyzer tutorial/pegasus/split/run0002</emphasis>

﻿************************************Summary*************************************

 Submit Directory   : tutorial/pegasus/split/run0002
 Total jobs         :     17 (100.00%)
 # jobs succeeded   :      1 (5.88%)
 # jobs failed      :      1 (5.88%)
 # jobs unsubmitted :     15 (88.24%)

******************************Failed jobs' details******************************

==========================stage_in_local_condorpool_0_0==========================

 last state: POST_SCRIPT_FAILED
       site: local
submit file: stage_in_local_condorpool_0_0.sub
output file: stage_in_local_condorpool_0_0.out.001
 error file: stage_in_local_condorpool_0_0.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : unknown
executable  : /usr/bin/pegasus-transfer
arguments   :   --threads   2  
exitcode    : 1
working dir : /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0002

------------------Task #1 - pegasus::transfer - None - stdout-------------------

2015-10-22 21:13:50,970    INFO:  Reading URL pairs from stdin
2015-10-22 21:13:50,970    INFO:  PATH=/usr/bin:/bin
2015-10-22 21:13:50,970    INFO:  LD_LIBRARY_PATH=
2015-10-22 21:13:50,972    INFO:  1 transfers loaded
2015-10-22 21:13:50,972    INFO:  Sorting the tranfers based on transfer type and source/destination
2015-10-22 21:13:50,972    INFO:  --------------------------------------------------------------------------------
2015-10-22 21:13:50,972    INFO:  Starting transfers - attempt 1
2015-10-22 21:13:50,972    INFO:  Using 1 threads for this round of transfers
2015-10-22 21:13:53,845   ERROR:  Command exited with non-zero exit code (1): /usr/bin/scp -r -B -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/tutorial/.ssh/id_rsa -P 22 '/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html' 'tutorial@127.0.0.1:/home/tutorial/work/tutorial/pegasus/split/run0002/pegasus.html'
2015-10-22 21:15:55,911    INFO:  --------------------------------------------------------------------------------
2015-10-22 21:15:55,912    INFO:  Starting transfers - attempt 2
2015-10-22 21:15:55,912    INFO:  Using 1 threads for this round of transfers
2015-10-22 21:15:58,446   ERROR:  Command exited with non-zero exit code (1): /usr/bin/scp -r -B -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/tutorial/.ssh/id_rsa -P 22 '/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html' 'tutorial@127.0.0.1:/home/tutorial/work/tutorial/pegasus/split/run0002/pegasus.html'
2015-10-22 21:16:40,468    INFO:  --------------------------------------------------------------------------------
2015-10-22 21:16:40,469    INFO:  Starting transfers - attempt 3
2015-10-22 21:16:40,469    INFO:  Using 1 threads for this round of transfers
2015-10-22 21:16:43,168   ERROR:  Command exited with non-zero exit code (1): /usr/bin/scp -r -B -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/tutorial/.ssh/id_rsa -P 22 '/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html' 'tutorial@127.0.0.1:/home/tutorial/work/tutorial/pegasus/split/run0002/pegasus.html'
2015-10-22 21:16:43,173    INFO:  --------------------------------------------------------------------------------
2015-10-22 21:16:43,173    INFO:  Stats: no local files in the transfer set
2015-10-22 21:16:43,173 CRITICAL:  Some transfers failed! See above, and possibly stderr.


-------------Task #1 - pegasus::transfer - None - Kickstart stderr--------------

Warning: Permanently added '127.0.0.1' (RSA) to the list of known hosts.<emphasis
          role="bold">
/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html: No such file or directory</emphasis>
..
<emphasis role="bold">/nfs/ccg3/ccg/home/pegtrainXX/examples/split/input/pegasus.html: No such file or directory
</emphasis>
</programlisting>

      <para>The above listing indicates that it could not transfer
      pegasus.html Let's correct that error</para>

      <programlisting><emphasis role="bold">$ mv input/pegasus.html.bak input/pegasus.html</emphasis></programlisting>

      <para>Now in order to start the workflow from where we left off, instead
      of executing pegasus-plan we will use the command pegasus-run on the
      directory from our previous failed workflow run.</para>

      <programlisting><emphasis role="bold">$</emphasis> <emphasis role="bold">﻿ pegasus-run tutorial/pegasus/split/run0002/</emphasis>
Rescued /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0002/split-0.log as /nfs/ccg3/ccg/home/pegtrainXX/examples/split/tutorial/pegasus/split/run0002/split-0.log.000
Submitting to condor split-0.dag.condor.sub
Submitting job(s).
1 job(s) submitted to cluster 181.

Your workflow has been started and is running in the base directory:

  tutorial/pegasus/split/run0002/

*** To monitor the workflow you can run ***

  pegasus-status -l tutorial/pegasus/split/run0002/

*** To remove your workflow run ***

  pegasus-remove tutorial/pegasus/split/run0002/
</programlisting>

      <para>The workflow will now run to completion and succeed.</para>

      <programlisting><emphasis role="bold">$ pegasus-status -l <emphasis
            role="bold">tutorial/pegasus/split/run0002/</emphasis></emphasis>
﻿no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    0     0     0     0     0    17     0 100.0 Success *split-0.dag                            
Summary: 1 DAG total (Success:1)
                     
</programlisting>
    </section>
  </section>

  <section>
    <title id="tutorial_wf_generation">Generating the Workflow</title>

    <para>The example that you ran earlier already had the workflow
    description (split.dax) generated. Pegasus reads workflow descriptions
    from DAX files. The term “DAX” is short for “Directed Acyclic Graph in
    XML”. DAX is an XML file format that has syntax for expressing jobs,
    arguments, files, and dependencies. We now will be creating the split
    workflow that we just ran using the Pegasus provided DAX API:</para>

    <figure>
      <title>Split Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="70%"
                     fileref="images/tutorial-split-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In this diagram, the ovals represent computational jobs, the
    dog-eared squares are files, and the arrows are dependencies.</para>

    <para>In order to create a DAX it is necessary to write code for a DAX
    generator. Pegasus comes with Perl, Java, and Python libraries for writing
    DAX generators. In this tutorial we will show how to use the Python
    library.</para>

    <para>The DAX generator for the split workflow is in the file
    <filename>generate_dax.py</filename>. Look at the file by typing:</para>

    <programlisting>$ <emphasis role="bold">more generate_dax.py</emphasis>
...</programlisting>

    <tip>
      <para>We will be using the <literal>more</literal> command to inspect
      several files in this tutorial. <literal>more</literal> is a pager
      application, meaning that it splits text files into pages and displays
      the pages one at a time. You can view the next page of a file by
      pressing the spacebar. Type 'h' to get help on using
      <literal>more</literal>. When you are done, you can type 'q' to close
      the file.</para>
    </tip>

    <para>The code has 3 main sections:</para>

    <orderedlist>
      <listitem>
        <para>A new ADAG object is created. This is the main object to which
        jobs and dependencies are added.</para>

        <programlisting># Create a abstract dag
dax = ADAG("split")
...
</programlisting>
      </listitem>

      <listitem>
        <para>Jobs and files are added. The 5 jobs in the diagram above are
        added and 9 files are referenced. Arguments are defined using strings
        and File objects. The input and output files are defined for each job.
        This is an important step, as it allows Pegasus to track the files,
        and stage the data if necessary. Workflow outputs are tagged with
        “transfer=true”.</para>

        <programlisting># the split job that splits the webpage into smaller chunks
webpage = File("pegasus.html")

split = Job("split")
split.addArguments("-l","100","-a","1",webpage,"part.")
split.uses(webpage, link=Link.INPUT)
dax.addJob(split)

...
</programlisting>
      </listitem>

      <listitem>
        <para>Dependencies are added. These are shown as arrows in the diagram
        above. They define the parent/child relationships between the jobs.
        When the workflow is executing, the order in which the jobs will be
        run is determined by the dependencies between them.</para>

        <programlisting># Add control-flow dependencies
dax.depends(wc, split)
</programlisting>
      </listitem>
    </orderedlist>

    <para>Generate a DAX file named <filename>split.dax</filename> by
    typing:</para>

    <programlisting>$ <emphasis role="bold">./generate_dax.py </emphasis>split.dax
Generated dax split.dax</programlisting>

    <para>The <filename>split.dax</filename> file should contain an XML
    representation of the split workflow. You can inspect it by typing:</para>

    <programlisting>$ <emphasis role="bold">more split.dax</emphasis>
...</programlisting>
  </section>

  <section id="tutorial_catalogs">
    <title>Information Catalogs</title>

    <para>The workflow description (DAX) that you specify to Pegasus is
    portable, and usually does not contain any locations to physical input
    files, executables or cluster end points where jobs are executed. Pegasus
    uses three information catalogs during the planning process.</para>

    <figure>
      <title>Information Catalogs used by Pegasus</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/tutorial-pegasus-catalogs.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <section>
      <title id="tut_xform_catalog">The Transformation Catalog</title>

      <para>The transformation catalog describes all of the executables
      (called “transformations”) used by the workflow. This description
      includes the site(s) where they are located, the architecture and
      operating system they are compiled for, and any other information
      required to properly transfer them to the execution site and run
      them.</para>

      <para>For this tutorial, the transformation catalog is in the file
      <filename>tc.txt</filename>:</para>

      <programlisting>$ <emphasis role="bold">more tc.txt</emphasis>
...
<emphasis role="bold"># This is the transformation catalog. It lists information about each of the
# executables that are used by the workflow.
</emphasis>
tr ls { 
  site condorpool {
    pfn "/bin/ls"
    arch "x86_64"
    os "linux"
    type "INSTALLED"
  }
}

...</programlisting>

      <para>The <filename>tc.txt</filename> file contains information about
      three transformations: wc, curl, and ls. These three transformations are
      referenced in the split DAX. The transformation catalog indicates that
      all three transformations are installed on the "condorpool" site, and
      are compiled for x86_64 Linux.</para>
    </section>

    <section id="tut_site_catalog">
      <title>The Site Catalog</title>

      <para>The site catalog describes the sites where the workflow jobs are
      to be executed. In this tutorial, there is a Condor Pool setup on
      workflow.isi.edu . Jobs from this condor pool are setup to be flocked to
      the Open Science Grid (OSG). The site catalog for the tutorial examples
      is in <filename>sites.xml</filename>:</para>

      <programlisting>$ <emphasis role="bold">more sites.xml</emphasis>
...
﻿   <emphasis role="bold">&lt;!-- The local site contains information about the submit host --&gt;
    &lt;!-- The arch and os keywords are used to match binaries in the transformation catalog --&gt;
    </emphasis>&lt;site handle="local" arch="x86_64" os="LINUX"&gt;

        &lt;!-- These are the paths on the submit host were Pegasus stores data --&gt;
        &lt;!-- Scratch is where temporary files go --&gt;
        &lt;directory type="shared-scratch" path="/home/tutorial/run"&gt;
            &lt;file-server operation="all" url="file:///home/tutorial/run"/&gt;
        &lt;/directory&gt;

       <emphasis role="bold"> </emphasis>&lt;!-- Storage is where pegasus stores output files --&gt;<emphasis
          role="bold">
        </emphasis>&lt;directory type="local-storage" path="/home/tutorial/outputs"&gt;
            &lt;file-server operation="all" url="file:///home/tutorial/outputs"/&gt;
        &lt;/directory&gt;

    &lt;/site&gt;

    <emphasis role="bold">&lt;!-- the  condor pool on which compute jobs are run --&gt;</emphasis>
    &lt;site handle="condorpool" arch="x86_64" os="LINUX" osrelease="" osversion="" glibc=""&gt;

         &lt;!-- the project name for the training accounts on OSG --&gt;
         &lt;profile namespace="condor" key="+ProjectName" &gt;"PegasusTraining"&lt;/profile&gt;

         &lt;!-- Requirements to make sure jobs only land RHEL 6 x86_64 nodes --&gt;
         &lt;profile namespace="condor" key="requirements"&gt;OSGVO_OS_STRING == "RHEL 6" &amp;amp;&amp;amp; Arch == "X86_64"&lt;/profile&gt;

         &lt;profile namespace="condor" key="universe" &gt;vanilla&lt;/profile&gt;
         &lt;profile namespace="pegasus" key="style" &gt;condor&lt;/profile&gt;
    &lt;/site&gt;


...</programlisting>

      <para>There are two sites defined in the site catalog: “<emphasis
      role="bold">local</emphasis>” and “<emphasis
      role="bold">condorpool</emphasis>”. The “local” site is used by Pegasus
      to learn about the submit host where the workflow management system
      runs. The “condorpool” site is the personal Condor pool running on your
      (virtual) machine. In this case, the local site and the "condorpool"
      site refer to the same machine, but they are logically separate as far
      as Pegasus is concerned.</para>

      <orderedlist>
        <listitem>
          <para>The <emphasis role="bold">local</emphasis> site is configured
          with a “storage” file system that is mounted on the submit host
          (indicated by the file:// URL). This file system is where the output
          data from the workflow will be stored. When the workflow is planned
          we will tell Pegasus that the output site is “local”.</para>
        </listitem>

        <listitem>
          <para>The <emphasis role="bold">condorpool</emphasis> site is not
          configured with any directories, as all jobs are run in a non shared
          filesystem setup on OSG, and Condor File Transfers are used to
          transferring in data via workflow.isi.edu . When we plan the
          workflow we will tell Pegasus that the execution site is
          “condorpool”.</para>
        </listitem>
      </orderedlist>

      <para>Finally, the "condorpool" site is configured with two profiles
      that tell Pegasus that it is a plain Condor pool. Pegasus supports many
      ways of submitting tasks to a remote cluster. In this configuration it
      will submit vanilla Condor jobs.</para>

      <section>
        <title>HPC Clusters</title>

        <para>Typically the sites in the site catalog describe remote
        clusters, such as PBS clusters or Condor pools.</para>

        <para>Usually, a typical deployment of an HPC cluster is illustrated
        below. The site catalog, captures for each cluster (site)</para>

        <itemizedlist>
          <listitem>
            <para>directories that can be used for executing jobs</para>
          </listitem>

          <listitem>
            <para>whether a shared file system is available</para>
          </listitem>

          <listitem>
            <para>file servers to use for staging input data and staging out
            output data</para>
          </listitem>

          <listitem>
            <para>headnode of the cluster to which jobs can be
            submitted.</para>
          </listitem>
        </itemizedlist>

        <figure>
          <title>Sample HPC Cluster Setup</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/tutorial-hpc-cluster.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Below is a sample site catalog entry for HPC cluster at SDSC
        that is part of XSEDE and supports remote job submission</para>

        <programlisting>&lt;site  handle="sdsc-gordon" arch="x86_64" os="LINUX"&gt;
        &lt;grid  type="gt5" contact="gordon-ln4.sdsc.xsede.org:2119/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
        &lt;grid  type="gt5" contact="gordon-ln4.sdsc.xsede.org:2119/jobmanager-pbs" scheduler="unknown" jobtype="compute"/&gt;

        &lt;!-- the base directory where workflow jobs will execute for the site --&gt;
        &lt;directory type="shared-scratch" path="/oasis/scratch/ux454281/temp_project"&gt;
            &lt;file-server operation="all" url="gsiftp://oasis-dm.sdsc.xsede.org:2811/oasis/scratch/ux454281/temp_project"/&gt;
        &lt;/directory&gt;

        &lt;profile namespace="globus" key="project"&gt;TG-STA110014S&lt;/profile&gt;
        &lt;profile namespace="env" key="PEGASUS_HOME"&gt;/home/ux454281/software/pegasus/pegasus-4.5.0&lt;/profile&gt;
    &lt;/site&gt;</programlisting>
      </section>
    </section>

    <section>
      <title id="tut_replica_catalog">The Replica Catalog</title>

      <para><emphasis role="bold">Note:</emphasis> Replica Catalog
      configuration is not required for the tutorial setup. It is only
      required if you want to refer to input files on external servers.</para>

      <para>The example that you ran , was configured with the inputs already
      present on the submit host ( where pegasus is installed) in a directory.
      If you have inputs at external servers, then you can specify the URL's
      to the input files in the Replica Catalog. This catalog tells Pegasus
      where to find each of the input files for the workflow.</para>

      <para>All files in a Pegasus workflow are referred to in the DAX using
      their Logical File Name (LFN). These LFNs are mapped to Physical File
      Names (PFNs) when Pegasus plans the workflow. This level of indirection
      enables Pegasus to map abstract DAXes to different execution sites and
      plan out the required file transfers automatically.</para>

      <para>The Replica Catalog for the diamond workflow is in the
      <filename>rc.dat</filename> file:</para>

      <programlisting>$ <emphasis role="bold">more rc.dat</emphasis>

<emphasis role="bold"># </emphasis>This is the replica catalog. It lists information about each of the
# input files used by the workflow. You can use this to specify locations to input files present on external servers.<emphasis
          role="bold">

</emphasis>
# The format is:
# LFN     PFN    site="SITE"

f.a    file:///nfs/ccg3/ccg/home/pegtrainXX/examples/diamond/input/f.a    site="local"</programlisting>

      <note>
        <para>In our tutorial, input files are on the submit host and we used
        the --input dir option to # pegasus-plan to specify where they are
        located.</para>
      </note>

      <para>This replica catalog contains only one entry for the diamond
      workflow’s only input file. This entry has an LFN of “f.a” with a PFN of
      “file:///home/tutorial/input/f.a” and the file is stored on the local
      site, which implies that it will need to be transferred to the
      "condorpool" site when the workflow runs.</para>
    </section>
  </section>

  <section id="tutorial_configuration">
    <title>Configuring Pegasus</title>

    <para>In addition to the information catalogs, Pegasus takes a
    configuration file that specifies settings that control how it plans the
    workflow.</para>

    <para>For the diamond workflow, the Pegasus configuration file is
    relatively simple. It only contains settings to help Pegasus find the
    information catalogs. These settings are in the
    <filename>pegasus.conf</filename> file:</para>

    <programlisting>$ <emphasis role="bold">more pegasus.conf</emphasis>
# This tells Pegasus where to find the Site Catalog
pegasus.catalog.site=XML
pegasus.catalog.site.file=sites.xml

# This tells Pegasus where to find the Replica Catalog
pegasus.catalog.replica=File
pegasus.catalog.replica.file=rc.dat

# This tells Pegasus where to find the Transformation Catalog
pegasus.catalog.transformation=Text
pegasus.catalog.transformation.file=tc.txt</programlisting>
  </section>

  <section>
    <title id="tutorial_conclusion">Conclusion</title>

    <para>This brings you to the end of the Pegasus tutorial at ISI submit
    host, with your jobs running on the Open Science Grid. The tutorial should
    have given you an overview of how to compose a simple workflow using
    Pegasus and running it in a native condor pool environment. The tutorial
    examples, should provide a good starting point for you to port your
    application to a Pegasus workflow.</para>

    <para>If you need help in porting your application to Pegasus contact us
    on the following support channels</para>

    <para><emphasis role="bold">public mailman list</emphasis> :
    pegasus-users@isi.edu</para>

    <para><emphasis role="bold">private support list</emphasis>:
    pegasus-support@isi.edu</para>

    <para><emphasis role="bold">Support chatroom</emphasis> on<ulink
    url="https://pegasus.isi.edu/support">HipChat</ulink>.</para>

    <para>Detailed Pegasus Documentation can be found <ulink
    url="http://pegasus.isi.edu/wms/docs/latest/">here</ulink>..</para>

    <para>Please contact the Pegasus Users Mailing list at
    <email>pegasus-users@isi.edu</email> if you need help.</para>
  </section>
</chapter>
