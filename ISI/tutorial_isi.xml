<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial">
  <title>Tutorial</title>

  <section id="tutorial_introduction">
    <title>Introduction</title>

    <para>This tutorial will take you through the steps of running simple
    workflows using Pegasus Workflow Management System. Pegasus allows
    scientists to</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Automate</emphasis> their scientific
        computational work, as portable workflows. Pegasus enables scientists
        to construct workflows in abstract terms without worrying about the
        details of the underlying execution environment or the particulars of
        the low-level specifications required by the middleware (Condor,
        Globus, or Amazon EC2). It automatically locates the necessary input
        data and computational resources necessary for workflow execution. It
        cleans up storage as the workflow is executed so that data-intensive
        workflows have enough space to execute on storage-constrained
        resources.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Recover</emphasis> from failures at
        runtime. When errors occur, Pegasus tries to recover when possible by
        retrying tasks, and when all else fails, provides a rescue workflow
        containing a description of only the work that remains to be done. It
        also enables users to move computations from one resource to another.
        Pegasus keeps track of what has been done (provenance) including the
        locations of data used and produced, and which software was used with
        which parameters.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Debug</emphasis> failures in their
        computations using a set of system provided debugging tools and an
        online workflow monitoring dashboard.</para>
      </listitem>
    </orderedlist>

    <para>This tutorial is intended for new users who want to get a quick
    overview of Pegasus concepts and usage. The accompanying tutorial VM comes
    pre-configured to run the example workflows. The instructions listed here
    refer mainly to the population modelling workflow example. The tutorial
    covers</para>

    <itemizedlist>
      <listitem>
        <para>submission of an already generated example workflow with
        Pegasus.</para>
      </listitem>

      <listitem>
        <para>how to use the Pegasus Workflow Dashboard for monitoring
        workflows.</para>
      </listitem>

      <listitem>
        <para>the command line tools for monitoring, debugging and generating
        statistics.</para>
      </listitem>

      <listitem>
        <para>creation of workflow using system provided API</para>
      </listitem>

      <listitem>
        <para>information catalogs configuration.</para>
      </listitem>

      <listitem>
        <para>using containers to bundle your application dependencies</para>
      </listitem>

      <listitem>
        <para>cluster short running tasks</para>
      </listitem>

      <listitem>
        <para>recovery from failures</para>
      </listitem>
    </itemizedlist>

    <para>More information about the topics covered in this tutorial can be
    found in later chapters of this user's guide.</para>

    <para>All of the steps in this tutorial are performed on the command-line.
    The convention we will use for command-line input and output is to put
    things that you should type in bold, monospace font, and to put the output
    you should get in a normal weight, monospace font, like this:</para>

    <programlisting>[user@host dir]$ <emphasis role="bold">you type this</emphasis>
you get this</programlisting>

    <para>Where <literal>[user@host dir]$</literal> is the terminal prompt,
    the text you should type is “<literal>you type this</literal>”, and the
    output you should get is "<literal>you get this</literal>". The terminal
    prompt will be abbreviated as <literal>$</literal>. Because some of the
    outputs are long, we don’t always include everything. Where the output is
    truncated we will add an ellipsis '...' to indicate the omitted
    output.</para>

    <para><emphasis role="bold">If you are having trouble with this tutorial,
    or anything else related to Pegasus, you can contact the Pegasus Users
    mailing list at <email>pegasus-users@isi.edu</email> to get help. You can
    also contact us on our <ulink
    url="https://pegasus.isi.edu/support">support chatroom</ulink> on HipChat.
    </emphasis></para>
  </section>

  <section id="tutorial_started">
    <title>Getting Started</title>

    <para>All of the steps in this tutorial are performed on the command-line.
    The convention we will use for command-line input and output is to put
    things that you should type in bold, monospace font, and to put the output
    you should get in a normal weight, monospace font, like this:</para>

    <programlisting>[user@workflow]$ <emphasis role="bold">you type this</emphasis>
you get this</programlisting>

    <para>Where <literal>[user@host dir]$</literal> is the terminal prompt,
    the text you should type is “<literal>you type this</literal>”, and the
    output you should get is "<literal>you get this</literal>". The terminal
    prompt will be abbreviated as <literal>$</literal>. Because some of the
    outputs are long, we don’t always include everything. Where the output is
    truncated we will add an ellipsis '...' to indicate the omitted
    output.</para>

    <para>Login to the workflow.isi.edu submit node</para>

    <programlisting><emphasis role="bold">$ ssh pegtrainXX@workflow.isi.edu</emphasis>
Enter pegtrainXX@workflow.isi.edu password
#######################################################################

  This is a Science Automation Technologies group machine.

    * Please do not do any heavy I/O against those your Action home
      directory. Use local filesystems or /nfs/ccg3

    * Ganglia monitoring: http://mimir.isi.edu/ganglia/

    * For support, contact Mats Rynge &lt;rynge@isi.edu&gt;

  *** NO BACKUPS ARE MADE OF ANY DATA ***

#######################################################################

</programlisting>

    <note>
      <para>For the purpose of this tutorial replace any instance of
      pegtrainXX with your workflow.isi.edu username.</para>
    </note>

    <para><emphasis role="bold">If you are having trouble with this tutorial,
    or anything else related to Pegasus, you can contact the Pegasus Users
    mailing list at <email>pegasus-users@isi.edu</email> to get
    help.</emphasis></para>

    <para>The tutorial should be done in the bash shell. Lets make sure that
    you are in the right shell.</para>

    <programlisting>[userXX@workflow]$  bash
[userXX@workflow ~]$ echo $SHELL
/bin/bash
</programlisting>

    <note>
      <para>For the purpose of this tutorial replace any instance of
      pegtrainXX with the user name assigned to you. To request access to a
      training account send email to &lt;pegasus-support@isi.edu&gt;</para>
    </note>
  </section>

  <section id="tutorial_scientific_workflows">
    <title>What are Scientific Workflows</title>

    <para>Scientific workflows allow users to easily express multi-step
    computational tasks, for example retrieve data from an instrument or a
    database, reformat the data, and run an analysis. A scientific workflow
    describes the dependencies between the tasks and in most cases the
    workflow is described as a directed acyclic graph (DAG), where the nodes
    are tasks and the edges denote the task dependencies. A defining property
    for a scientific workflow is that it manages data flow. The tasks in a
    scientific workflow can be everything from short serial tasks to very
    large parallel tasks (MPI for example) surrounded by a large number of
    small, serial tasks used for pre- and post-processing.</para>

    <para>Workflows can vary from simple to complex. Below are some examples.
    In the figures below, the task are designated by circles/ellipses while
    the files created by the tasks are indicated by rectangles. Arrows
    indicate task dependencies.</para>

    <para><emphasis role="bold">Process Workflow</emphasis></para>

    <para>It consists of a single task that runs the <literal>ls</literal>
    command and generates a listing of the files in the `/` directory.</para>

    <figure>
      <title>Process Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentdepth="50%"
                     fileref="images/tutorial-single-job-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para><emphasis role="bold">Pipeline of Tasks</emphasis></para>

    <para>The pipeline workflow consists of two tasks linked together in a
    pipeline. The first job runs the `curl` command to fetch the Pegasus home
    page and store it as an HTML file. The result is passed to the `wc`
    command, which counts the number of lines in the HTML file. <figure>
        <title>Pipeline of Tasks</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="70%"
                       fileref="images/tutorial-pipeline-tasks-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Split Workflow</emphasis></para>

    <para>The split workflow downloads the Pegasus home page using the `curl`
    command, then uses the `split` command to divide it into 4 pieces. The
    result is passed to the `wc` command to count the number of lines in each
    piece.<figure>
        <title>Split Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/tutorial-split-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Merge Workflow</emphasis></para>

    <para>The merge workflow runs the `ls` command on several */bin
    directories and passes the results to the `cat` command, which merges the
    files into a single listing. The merge workflow is an example of a
    parameter sweep over arguments.<figure>
        <title>Merge Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/tutorial-merge-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Diamond Workflow</emphasis></para>

    <para>The diamond workflow runs combines the split and merge workflow
    patterns to create a more complex workflow.</para>

    <figure>
      <title>Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="70%"
                     fileref="images/tutorial-diamond-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para><emphasis role="bold">Complex Workflows</emphasis></para>

    <para>The above examples can be used as building blocks for much complex
    workflows. Some of these are showcased on the <ulink
    url="https://pegasus.isi.edu/applications">Pegasus Applications
    page</ulink>.</para>
  </section>

  <section id="tutorial_submitting_wf">
    <title>Submitting an Example Workflow</title>

    <para>All of the example workflows described in the previous section can
    be generated with the <literal>pegasus-init</literal> command. For this
    tutorial we will be using a complex workflow that does population
    modelling, which can be created like this:</para>

    <programlisting>$ <emphasis role="bold">mkdir ~/tutorial
</emphasis>$ <emphasis role="bold">cd </emphasis>/nfs/ccg3/ccg/home/pegtrainXX/tutorial
$  <emphasis role="bold">pegasus-init population</emphasis>
Do you want to generate a tutorial workflow? (y/n) [n]: y
1: Local Machine
2: USC HPCC Cluster
3: OSG from ISI submit node
4: XSEDE, with Bosco
5: Bluewaters, with Glite
What environment is tutorial to be setup for? (1-5) [1]: 1
1: Process
2: Pipeline
3: Split
4: Merge
5: EPA (requires R)
6: Population Modeling using Containers
7: Diamond
What tutorial workflow do you want? (1-7) [1]: 6
Pegasus Tutorial setup for example workflow - population for execution on submit-host in directory <emphasis
        role="bold">/local-scratch/home/pegtrainXX/population</emphasis>

$ <emphasis role="bold">cd population</emphasis>
$ <emphasis role="bold">ls</emphasis>
daxgen.py  Dockerfile  generate_dax.sh  input  output  pegasus.properties 
 plan_dax.sh  rc.txt  README.md  scripts  Singularity  sites.xml 
 tc.txt  tc.txt.containers
</programlisting>

    <tip>
      <para>The <literal>pegasus-init</literal> tool can be used to generate
      workflow skeletons from templates by asking the user questions. It is
      easier to use pegasus-init than to start a new workflow from
      scratch.</para>
    </tip>

    <para>The population modelling workflow looks like this:</para>

    <figure>
      <title>Population Modelling Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="100%"
                     fileref="images/tutorial-pop-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The input workflow description for Pegasus is called the DAX. It can
    be generated by running the <filename>generate_dax.sh</filename> script
    from the population directory, like this:</para>

    <programlisting>$ <emphasis role="bold">./generate_dax.sh pop.dax</emphasis>
Generated dax pop.dax
    </programlisting>

    <para>This script will run a small Python program
    (<filename>daxgen.py</filename>) that generates a file with a .dax
    extension using the Pegasus Python API. We will cover the details of
    creating a DAX programmatically <link
    linkend="tutorial_wf_generation">later in the tutorial</link>. Pegasus
    reads the DAX and generates an executable HTCondor workflow that is run on
    an execution site.</para>

    <para>The <literal>pegasus-plan</literal> command is used to submit the
    workflow through Pegasus. The pegasus-plan command reads the input
    workflow (DAX file specified by --dax option), maps the abstract DAX to
    one or more execution sites, and submits the generated executable workflow
    to HTCondor. Among other things, the options to pegasus-plan tell
    Pegasus</para>

    <itemizedlist>
      <listitem>
        <para>the workflow to run</para>
      </listitem>

      <listitem>
        <para>where (what site) to run the workflow</para>
      </listitem>

      <listitem>
        <para>the input directory where the inputs are placed</para>
      </listitem>

      <listitem>
        <para>the output directory where the outputs are placed</para>
      </listitem>
    </itemizedlist>

    <para>By default, the workflow is setup to run on the compute sites (i.e
    sites with handle other than "local") defined in the sites.xml file. In
    our example, the workflow will run on a site named "osg" in the sites.xml
    file.</para>

    <note>
      <para>If there are multiple compute sites specified in your sites.xml,
      and you want to choose a specific site, use the --sites option to
      pegasus-plan</para>
    </note>

    <para>To plan the population modelling workflow invoke the pegasus-plan
    command using the <filename>plan_dax.sh</filename> wrapper script as
    follows:</para>

    <programlisting>$ <emphasis role="bold">./plan_dax.sh pop.dax</emphasis>
2018.07.05 18:07:27.563 PDT:    
2018.07.05 18:07:27.568 PDT:   ----------------------------------------------------------------------- 
2018.07.05 18:07:27.574 PDT:   File for submitting this DAG to HTCondor           : population-0.dag.condor.sub 
2018.07.05 18:07:27.579 PDT:   Log of DAGMan debugging messages                 : population-0.dag.dagman.out 
2018.07.05 18:07:27.584 PDT:   Log of HTCondor library output                     : population-0.dag.lib.out 
2018.07.05 18:07:27.589 PDT:   Log of HTCondor library error messages             : population-0.dag.lib.err 
2018.07.05 18:07:27.595 PDT:   Log of the life of condor_dagman itself          : population-0.dag.dagman.log 
2018.07.05 18:07:27.600 PDT:    
2018.07.05 18:07:27.605 PDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: 
2018.07.05 18:07:27.615 PDT:   ----------------------------------------------------------------------- 
2018.07.05 18:07:30.396 PDT:   Created Pegasus database in: sqlite:////scitech/home/pegtrainXX/.pegasus/workflow.db 
2018.07.05 18:07:30.402 PDT:   Your database is compatible with Pegasus version: 4.8.3dev 
2018.07.05 18:07:30.554 PDT:   Submitting to condor population-0.dag.condor.sub 
2018.07.05 18:07:30.582 PDT:   Submitting job(s). 
2018.07.05 18:07:30.587 PDT:   1 job(s) submitted to cluster 992052. 
2018.07.05 18:07:30.593 PDT:    
2018.07.05 18:07:30.598 PDT:   Your workflow has been started and is running in the base directory: 
2018.07.05 18:07:30.603 PDT:    
2018.07.05 18:07:30.608 PDT:     /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0001 
2018.07.05 18:07:30.613 PDT:    
2018.07.05 18:07:30.619 PDT:   *** To monitor the workflow you can run *** 
2018.07.05 18:07:30.624 PDT:    
2018.07.05 18:07:30.629 PDT:     pegasus-status -l /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0001 
2018.07.05 18:07:30.634 PDT:    
2018.07.05 18:07:30.640 PDT:   *** To remove your workflow run *** 
2018.07.05 18:07:30.645 PDT:    
2018.07.05 18:07:30.650 PDT:     pegasus-remove /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0001 
2018.07.05 18:07:30.655 PDT:    
2018.07.05 18:07:30.706 PDT:   Time taken to execute is 4.281 seconds </programlisting>

    <note>
      <para>The line in the output that starts with
      <literal>pegasus-status</literal>, contains the command you can use to
      monitor the status of the workflow. The path it contains is the path to
      the submit directory where all of the files required to submit and
      monitor the workflow are stored.</para>
    </note>

    <para>This is what the population modelling workflow looks like after
    Pegasus has finished planning the DAX:</para>

    <figure>
      <title>Population Modelling DAG</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/tutorial-pop-dag.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>For this workflow the only jobs Pegasus needs to add are a directory
    creation job, a stage-in job (for pegasus.html), and stage-out jobs (for
    wc count outputs). The cleanup jobs remove data that is no longer required
    as workflow executes.</para>
  </section>

  <section id="tutorial_wf_dashboard">
    <title>Workflow Dashboard for Monitoring and Debugging</title>

    <para>The Pegasus Dashboard is a web interface for monitoring and
    debugging workflows. We will use the web dashboard to monitor the status
    of the population modelling workflow.</para>

    <para>By default, the dashboard server can only monitor workflows run by
    the current user i.e. the user who is running the pegasus-service. On
    workflow.isi.edu, it is running in a multiuser mode. Before accessing the
    dashboard, you need to give world read permissions to the workflow
    database in your home directory.</para>

    <programlisting>$ <emphasis role="bold">chmod +r ~/.pegasus/workflow.db</emphasis></programlisting>

    <para>Access the dashboard by navigating your browser to <emphasis
    role="bold">https://workflow.isi.edu:8443/u/pegtrainXX/</emphasis>.</para>

    <para>When the webpage loads up, it will ask you for a username and a
    password. It is your UNIX username and password corresponding to the user
    you used to log in to workflow.isi.edu .</para>

    <para>The Dashboard's home page lists all workflows, which have been run
    by the current-user. The home page shows the status of each workflow i.e.
    Running/Successful/Failed/Failing. The home page lists only the top level
    workflows (Pegasus supports hierarchical workflows i.e. workflows within a
    workflow). The rows in the table are color coded</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">Green</emphasis>: indicates workflow
        finished successfully.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Red</emphasis>: indicates workflow
        finished with a failure.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Blue</emphasis>: indicates a workflow is
        currently running.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Gray</emphasis>: indicates a workflow that
        was archived.</para>
      </listitem>
    </itemizedlist>

    <note>
      <para>On tutorial machine pegasus-service is running through apache.
      Users can also start the dashboard as their own user using
      pegasus-service command on the command line e.g. <programlisting>$ pegasus-service -H workflow.isi.edu -p 5001 -v</programlisting></para>
    </note>

    <figure>
      <title>Dashboard Home Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_home.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>To view details specific to a workflow, the user can click on
    corresponding workflow label. The workflow details page lists workflow
    specific information like workflow label, workflow status, location of the
    submit directory, etc. The details page also displays pie charts showing
    the distribution of jobs based on status.</para>

    <para>In addition, the details page displays a tab listing all
    sub-workflows and their statuses. Additional tabs exist which list
    information for all running, failed, successful, and failing jobs.</para>

    <para>The information displayed for a job depends on it's status. For
    example, the failed jobs tab displays the job name, exit code, links to
    available standard output, and standard error contents.</para>

    <figure>
      <title>Dashboard Workflow Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_workflow_details.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>To view details specific to a job the user can click on the
    corresponding job's job label. The job details page lists information
    relevant to a specific job. For example, the page lists information like
    job name, exit code, run time, etc.</para>

    <para>The job instance section of the job details page lists all attempts
    made to run the job i.e. if a job failed in its first attempt due to
    transient errors, but ran successfully when retried, the job instance
    section shows two entries; one for each attempt to run the job.</para>

    <para>The job details page also shows tab's for failed, and successful
    task invocations (Pegasus allows users to group multiple smaller task's
    into a single job i.e. a job may consist of one or more tasks)</para>

    <figure>
      <title>Dashboard Job Description Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_job_details.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The task invocation details page provides task specific information
    like task name, exit code, duration etc. Task details differ from job
    details, as they are more granular in nature.</para>

    <figure>
      <title>Dashboard Invocation Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_invocation_details.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The dashboard also has web pages for workflow statistics and
    workflow charts, which graphically renders information provided by the
    pegasus-statistics and pegasus-plots command respectively.</para>

    <para>The Statistics page shows the following statistics.</para>

    <orderedlist>
      <listitem>
        <para>Workflow level statistics</para>
      </listitem>

      <listitem>
        <para>Job breakdown statistics</para>
      </listitem>

      <listitem>
        <para>Job specific statistics</para>
      </listitem>
    </orderedlist>

    <figure>
      <title>Dashboard Statistics Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_statistics.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section id="tutorial_monitoring_cmd_tools">
    <title>Command line tools for Monitoring and Debugging</title>

    <para>Pegasus also comes with a series of command line tools that users
    can use to monitor and debug their workflows.</para>

    <itemizedlist>
      <listitem>
        <para>pegasus-status : monitor the status of the workflow</para>
      </listitem>

      <listitem>
        <para>pegasus-analyzer : debug a failed workflow</para>
      </listitem>

      <listitem>
        <para>pegasus-statistics : generate statistics from a workflow
        run.</para>
      </listitem>
    </itemizedlist>

    <para>We will run pegasus-statistics in a later section.</para>

    <section>
      <title>pegasus-status - monitoring the workflow</title>

      <para>After the workflow has been submitted you can monitor it using the
      <literal>pegasus-status</literal> command:</para>

      <programlisting><emphasis role="bold">$  pegasus-status -l /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0001 </emphasis>
(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
   16     0     0     0     0     5     3  20.8 Failure *population-0.dag                       
Summary: 1 DAG total (Failure:1)
</programlisting>

      <para>This command shows the workflow (population-0) and the running
      jobs . It also gives statistics on the number of jobs in each state and
      the percentage of the jobs in the workflow that have finished
      successfully.</para>

      <para>Use the <literal>watch</literal> option to continuously monitor
      the workflow:</para>

      <programlisting>$ <emphasis role="bold">pegasus-status -w submit/pegtrainXX/pegasus/population/run0001</emphasis>
...</programlisting>

      <programlisting>(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    0     0     0     0     0    15     0 100.0 Success *population-0.dag      </programlisting>

      <para>That means the workflow is finished successfully.</para>
    </section>

    <section>
      <title>pegasus-analyzer - debug a failed workflow</title>

      <para>In our case one or more jobs will fail, and the output of the
      <literal>pegasus-status</literal> command above will have a non-zero
      value in the <literal>FAILURE</literal> column.</para>

      <programlisting><emphasis role="bold">$  pegasus-status -l /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0001 </emphasis>
(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
   16     0     0     0     0     5     3  20.8 Failure *population-0.dag                       
Summary: 1 DAG total (Failure:1)
</programlisting>

      <para>You can debug the failure using the
      <literal>pegasus-analyzer</literal> command. This command will identify
      the jobs that failed and show their output.</para>

      <programlisting><emphasis role="bold">$ pegasus-analyzer /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0001</emphasis>

************************************Summary*************************************

 Submit Directory   : /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0001
 Total jobs         :     24 (100.00%)
 # jobs succeeded   :      5 (20.83%)
 # jobs failed      :      3 (12.50%)
 # jobs held        :      3 (12.50%)
 # jobs unsubmitted :     16 (66.67%)

*******************************Held jobs' details*******************************

=======================county_population_raster_ID0000008=======================

submit file            : county_population_raster_ID0000008.sub
last_job_instance_id   : 9
reason                 :  Error from slot1@compute-6.isi.edu: STARTER at 128.9.35.234 failed to send file(s) to &lt;128.9.44.53:9618&gt;: error reading from /var/lib/condor/execute/dir_4209/county_level_pop_2019.tif: (errno 2) No such file or directory; SHADOW failed to receive file(s) from &lt;128.9.35.234:42630&gt;

=======================county_population_raster_ID0000005=======================

submit file            : county_population_raster_ID0000005.sub
last_job_instance_id   : 10
reason                 :  Error from slot1@compute-6.isi.edu: STARTER at 128.9.35.234 failed to send file(s) to &lt;128.9.44.53:9618&gt;: error reading from /var/lib/condor/execute/dir_4383/county_level_pop_2018.tif: (errno 2) No such file or directory; SHADOW failed to receive file(s) from &lt;128.9.35.234:40000&gt;

=======================county_population_raster_ID0000002=======================

submit file            : county_population_raster_ID0000002.sub
last_job_instance_id   : 11
reason                 :  Error from slot1@compute-6.isi.edu: STARTER at 128.9.35.234 failed to send file(s) to &lt;128.9.44.53:9618&gt;: error reading from /var/lib/condor/execute/dir_4598/county_level_pop_2017.tif: (errno 2) No such file or directory; SHADOW failed to receive file(s) from &lt;128.9.35.234:47008&gt;

******************************Failed jobs' details******************************

=======================county_population_raster_ID0000008=======================

 last state: POST_SCRIPT_FAILED
       site: condorpool
submit file: 00/00/county_population_raster_ID0000008.sub
output file: 00/00/county_population_raster_ID0000008.out.001
 error file: 00/00/county_population_raster_ID0000008.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : condorpool
hostname    : compute-6.isi.edu
executable  : /var/lib/condor/execute/dir_4209/county_population_raster
arguments   :   --config   county_cohort_pop_config.ini   --shapefile   SouthSudan_CountyPopulation.shp   --year   2019   --outfile   county_level_pop_2019.tif  
exitcode    : 1
working dir : /var/lib/condor/execute/dir_4209

-------Task #1 - county_population_raster - ID0000008 - Kickstart stderr--------

 Traceback (most recent call last):
  File "./county_population_raster", line 5, in &lt;module&gt;
    import pandas as pd
ImportError: No module named 'pandas'


=======================county_population_raster_ID0000005=======================

 last state: POST_SCRIPT_FAILED
       site: condorpool
submit file: 00/00/county_population_raster_ID0000005.sub
output file: 00/00/county_population_raster_ID0000005.out.001
 error file: 00/00/county_population_raster_ID0000005.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : condorpool
hostname    : compute-6.isi.edu
executable  : /var/lib/condor/execute/dir_4383/county_population_raster
arguments   :   --config   county_cohort_pop_config.ini   --shapefile   SouthSudan_CountyPopulation.shp   --year   2018   --outfile   county_level_pop_2018.tif  
exitcode    : 1
working dir : /var/lib/condor/execute/dir_4383

-------Task #1 - county_population_raster - ID0000005 - Kickstart stderr--------

 Traceback (most recent call last):
  File "./county_population_raster", line 5, in &lt;module&gt;
    import pandas as pd
ImportError: No module named 'pandas'


=======================county_population_raster_ID0000002=======================

 last state: POST_SCRIPT_FAILED
       site: condorpool
submit file: 00/00/county_population_raster_ID0000002.sub
output file: 00/00/county_population_raster_ID0000002.out.001
 error file: 00/00/county_population_raster_ID0000002.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : condorpool
hostname    : compute-6.isi.edu
executable  : /var/lib/condor/execute/dir_4598/county_population_raster
arguments   :   --config   county_cohort_pop_config.ini   --shapefile   SouthSudan_CountyPopulation.shp   --year   2017   --outfile   county_level_pop_2017.tif  
exitcode    : 1
working dir : /var/lib/condor/execute/dir_4598

-------Task #1 - county_population_raster - ID0000002 - Kickstart stderr--------

 Traceback (most recent call last):
  File "./county_population_raster", line 5, in &lt;module&gt;
    import pandas as pd
ImportError: No module named 'pandas'

</programlisting>

      <para>The above output indicates that our county_population_raster jobs
      failed while running on site condorpool and on a node compute-6.isi.edu
      due to a missing python package pandas. This is a pretty common problem
      in scientific workflows where scientific codes depend have software
      dependencies that are often not fulfilled when running on shared or
      remote resources. In fact the software, has a bunch of other
      dependencies that can make it hard to get installed on remote resources
      especially when we don't have administrative control over them.</para>

      <para>Lets consider what options exist for us?</para>

      <orderedlist>
        <listitem>
          <para>We can contact system administrators on the remote sites to
          install these packages for us</para>
        </listitem>

        <listitem>
          <para>We can install these packages in our user accounts ourselves
          without root access</para>
        </listitem>
      </orderedlist>

      <para>To see how you can specify this information to Pegasus we will
      look into the information catalogs that Pegasus referred to while
      planning your workflow a bit later</para>
    </section>
  </section>

  <section id="tutorial_wf_generation">
    <title>Generating the Workflow</title>

    <para>The example that you ran earlier already had the workflow
    description (pop.dax) generated. Pegasus reads workflow descriptions from
    DAX files. The term "DAX" is short for "Directed Acyclic Graph in XML".
    DAX is an XML file format that has syntax for expressing jobs, arguments,
    files, and dependencies. We now will be creating the population modelling
    workflow that we just ran using the Pegasus provided DAX API:</para>

    <figure>
      <title>Population Modelling Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentwidth="100%"
                     fileref="images/tutorial-pop-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In this diagram, the ovals represent computational jobs, the
    dog-eared squares are files, and the arrows are dependencies.</para>

    <para>In order to create a DAX it is necessary to write code for a DAX
    generator. Pegasus comes with Perl, Java, and Python libraries for writing
    DAX generators. In this tutorial we will show how to use the Python
    library.</para>

    <para>The DAX generator for this workflow is in the file
    <filename>daxgen.py</filename>. Look at the file by typing:</para>

    <programlisting>$ <emphasis role="bold">more daxgen.py</emphasis>
...</programlisting>

    <tip>
      <para>We will be using the <literal>more</literal> command to inspect
      several files in this tutorial. <literal>more</literal> is a pager
      application, meaning that it splits text files into pages and displays
      the pages one at a time. You can view the next page of a file by
      pressing the spacebar. Type 'h' to get help on using
      <literal>more</literal>. When you are done, you can type 'q' to close
      the file.</para>
    </tip>

    <para>The code has 3 main sections:</para>

    <orderedlist>
      <listitem>
        <para>A new ADAG object is created. This is the main object to which
        jobs and dependencies are added.</para>

        <programlisting><emphasis role="bold"># Create a abstract dag</emphasis>
dax = ADAG("population")

...
</programlisting>
      </listitem>

      <listitem>
        <para>Jobs and files are added. The 5 jobs in the diagram above are
        added and 9 files are referenced. Arguments are defined using strings
        and File objects. The input and output files are defined for each job.
        This is an important step, as it allows Pegasus to track the files,
        and stage the data if necessary. Workflow outputs are tagged with
        "transfer=true".</para>

        <programlisting><emphasis role="bold"># config file</emphasis>
config = File('county_cohort_pop_config.ini')
dax.addFile(config)
..
<emphasis role="bold">
# add one job per year                                                                                                                                                                                                       
# we have data till 2030    </emphasis>                                                                                                                                                                                                 
for year in range(2017, 2020):
  
   # create a job to process the shape file                                                                                                                                                                                 
    j1 = Job("county_population_raster")

    # we need the geospatial lib                                                                                                                                                                                             
    j1.uses(geospatial, link=Link.INPUT)

    # config file                                                                                                                                                                                                            
    j1.uses(config, link=Link.INPUT)
    j1.addArguments('--config', config)

    # add input shapes file                                                                                                                                                                                                  
    for f in shapefiles:
        j1.uses(f, link=Link.INPUT)
    j1.addArguments('--shapefile', basename + '.shp')

    # year                                                                                                                                                                                                                   
    j1.addArguments('--year', str(year))

    # add outputs                                                                                                                                                                                                            
    tif = File('county_level_pop_' + str(year) + '.tif')
    j1.uses(tif, link=Link.OUTPUT, transfer=True)
    j1.addArguments('--outfile', tif)

    dax.addJob(j1)
...
</programlisting>
      </listitem>

      <listitem>
        <para>Dependencies are added. These are shown as arrows in the diagram
        above. They define the parent/child relationships between the jobs.
        When the workflow is executing, the order in which the jobs will be
        run is determined by the dependencies between them.</para>

        <programlisting># add file dependencies to the  animate job                                                                                                                                                                              
animate.uses(png, link=Link.INPUT)
dax.depends(parent=j3, child=animate)
</programlisting>
      </listitem>
    </orderedlist>

    <para>Generate a DAX file named <filename>pop.dax</filename> by
    typing:</para>

    <programlisting>$ <emphasis role="bold">./generate_dax.sh </emphasis>pop.dax
Generated dax pop.dax</programlisting>

    <para>The <filename>pop.dax</filename> file should contain an XML
    representation of the population workflow. You can inspect it by
    typing:</para>

    <programlisting>$ <emphasis role="bold">more pop.dax</emphasis>
...</programlisting>
  </section>

  <section id="tutorial_catalogs">
    <title>Information Catalogs</title>

    <para>The workflow description (DAX) that you specify to Pegasus is
    portable, and usually does not contain any locations to physical input
    files, executables or cluster end points where jobs are executed. Pegasus
    uses three information catalogs during the planning process.</para>

    <figure>
      <title>Information Catalogs used by Pegasus</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/tutorial-pegasus-catalogs.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <section id="tut_site_catalog">
      <title>The Site Catalog</title>

      <para>The site catalog describes the sites where the workflow jobs are
      to be executed. In this tutorial we assume that you have a Personal
      Condor pool running on localhost. If you are using one of the tutorial
      VMs this has already been setup for you. The site catalog for the
      tutorial examples is in <filename>sites.xml</filename>:</para>

      <programlisting>$ <emphasis role="bold">more sites.xml</emphasis>
...
    <emphasis role="bold">&lt;!-- The local site contains information about the submit host --&gt;</emphasis> 
    &lt;site handle="local" arch="x86_64" os="LINUX"&gt;
        <emphasis role="bold">&lt;!-- This is where intermediate data will be stored --&gt;</emphasis>
        &lt;directory type="shared-scratch" path="/local-scratch/home/pegtrainXX/population/scratch"&gt;
            &lt;file-server operation="all" url="file:///local-scratch/home/pegtrainXX/population/scratch"/&gt;
        &lt;/directory&gt;
        <emphasis role="bold">&lt;!-- This is where output data will be stored --&gt;</emphasis>
        &lt;directory type="shared-storage" path="/local-scratch/home/pegtrainXX/population/output"&gt;
            &lt;file-server operation="all" url="file:///local-scratch/home/pegtrainXX/population/output"/&gt;
        &lt;/directory&gt;
    &lt;/site&gt;


   <emphasis role="bold"> &lt;!-- The condorpool is a local Condor based computational site setup at ISI --&gt;</emphasis>
    &lt;site handle="condorpool" arch="x86_64" os="LINUX"&gt;
        <emphasis role="bold">&lt;!-- These profiles tell Pegasus that the site is a plain Condor pool --&gt;</emphasis>
        &lt;profile namespace="pegasus" key="style"&gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt;

        <emphasis role="bold">&lt;!-- This ensures that when a job runs enough memory and 
              diskspace available to stage and run the container --&gt;</emphasis>
        &lt;profile namespace="condor" key="request_memory" &gt;2  GB&lt;/profile&gt;
        &lt;profile namespace="condor" key="request_disk" &gt;5 GB&lt;/profile&gt;

      <emphasis role="bold"> &lt;!-- This profile tells Pegasus to create two clustered jobs
            per level of the workflow, when horizontal clustering is
            enabled --&gt;</emphasis>
	       &lt;profile namespace="pegasus" key="clusters.num" &gt;2&lt;/profile&gt;

    &lt;/site&gt;
    
    &lt;site  handle="isi_workflow" arch="x86_64" os="LINUX"&gt;
        <emphasis role="bold">&lt;!-- if we want to use non sharedfs mode for running this workflow --&gt; </emphasis> 
        &lt;directory type="shared-scratch" path="/local-scratch/http/${USER}/staging"&gt;
            &lt;file-server operation="get"  url="http://workflow.isi.edu/scratch/${USER}/staging"/&gt;
            &lt;file-server operation="put"   url="scp://${USER}@workflow.isi.edu/local-scratch/http/${USER}/staging"/&gt;
        &lt;/directory&gt;
        &lt;profile namespace="pegasus"  key="SSH_PRIVATE_KEY"&gt;${HOME}/.ssh/workflow&lt;/profile&gt;
    &lt;/site&gt;

...
      </programlisting>

      <note>
        <para>By default (unless specified in properties), Pegasus picks ups
        the site catalog from a XML file named sites.xml in the current
        working directory from where pegasus-plan is invoked.</para>
      </note>

      <para>There are three sites defined in the site catalog: "local",
      "condorpool" and "isi_workflow".</para>

      <orderedlist>
        <listitem>
          <para>The <emphasis role="bold">local</emphasis> site is configured
          with a "storage" file system that is mounted on the submit host
          (indicated by the file:// URL). This file system is where the output
          data from the workflow will be stored. When the workflow is planned
          we will tell Pegasus that the output site is "local".</para>
        </listitem>

        <listitem>
          <para>The <emphasis role="bold">condorpool</emphasis> site is the
          Condor pool configured on your submit machine. On workflow.isi.edu
          it is hooked up to the local Condor pool at ISI.</para>
        </listitem>

        <listitem>
          <para>The <emphasis role="bold">isi_workflow</emphasis> site is also
          configured with a "scratch" file system. This file system is where
          the working directory will be created. When we plan the workflow in
          nonsharedfs mode we tell Pegasus that the staging-site is
          "isi_workflow".</para>
        </listitem>
      </orderedlist>

      <para>Pegasus supports many different file transfer protocols. In this
      case the Pegasus configuration is set up so that input and output files
      are transferred to/from OSG via scp/http. This is done by setting
      <literal>pegasus.data.configuration = condorio</literal> in the
      properties file.</para>

      <para>One can also use in-built Condor file transfer to do the
      transfers. This is done by setting <literal>pegasus.data.configuration =
      condorio</literal> in the properties file.</para>

      <para>Finally, the "condorpool" site is configured with two profiles
      that tell Pegasus that it is a plain Condor pool. Pegasus supports many
      ways of submitting tasks to a remote cluster. In this configuration it
      will submit vanilla Condor jobs.</para>

      <section>
        <title>HPC Clusters</title>

        <para>Typically the sites in the site catalog describe remote
        clusters, such as PBS clusters or Condor pools.</para>

        <para>Usually, a typical deployment of an HPC cluster is illustrated
        below. The site catalog, captures for each cluster (site)</para>

        <itemizedlist>
          <listitem>
            <para>directories that can be used for executing jobs</para>
          </listitem>

          <listitem>
            <para>whether a shared file system is available</para>
          </listitem>

          <listitem>
            <para>file servers to use for staging input data and staging out
            output data</para>
          </listitem>

          <listitem>
            <para>headnode of the cluster to which jobs can be
            submitted.</para>
          </listitem>
        </itemizedlist>

        <figure>
          <title>Sample HPC Cluster Setup</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/tutorial-hpc-cluster.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>Below is a sample site catalog entry for HPC cluster at SDSC
        that is part of XSEDE</para>

        <programlisting>&lt;site  handle="sdsc-gordon" arch="x86_64" os="LINUX"&gt;
        &lt;grid  type="gt5" contact="gordon-ln4.sdsc.xsede.org:2119/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
        &lt;grid  type="gt5" contact="gordon-ln4.sdsc.xsede.org:2119/jobmanager-pbs" scheduler="unknown" jobtype="compute"/&gt;

        &lt;!-- the base directory where workflow jobs will execute for the site --&gt;
        &lt;directory type="shared-scratch" path="/oasis/scratch/ux454281/temp_project"&gt;
            &lt;file-server operation="all" url="gsiftp://oasis-dm.sdsc.xsede.org:2811/oasis/scratch/ux454281/temp_project"/&gt;
        &lt;/directory&gt;

        &lt;profile namespace="globus" key="project"&gt;TG-STA110014S&lt;/profile&gt;
        &lt;profile namespace="env" key="PEGASUS_HOME"&gt;/home/ux454281/software/pegasus/pegasus-4.5.0&lt;/profile&gt;
    &lt;/site&gt;</programlisting>
      </section>
    </section>

    <section>
      <title id="tut_xform_catalog">The Transformation Catalog</title>

      <para>The transformation catalog describes all of the executables
      (called "transformations") used by the workflow. This description
      includes the site(s) where they are located, the architecture and
      operating system they are compiled for, and any other information
      required to properly transfer them to the execution site and run
      them.</para>

      <para>For this tutorial, the transformation catalog is in the file
      <filename>tc.txt</filename>:</para>

      <programlisting>$ <emphasis role="bold">more tc.txt</emphasis>
tr county_population_raster {
    site local {
        type "STAGEABLE"
        arch "x86_64"
        os "LINUX"
        pfn "file:///local-scratch/home/pegtrainXX/population/scripts/county_population_raster.py"
    }
}
...</programlisting>

      <note>
        <para>By default (unless specified in properties), Pegasus picks up
        the transformation catalog from a text file named tc.txt in the
        current working directory from where pegasus-plan is invoked.</para>
      </note>

      <para>The <filename>tc.txt</filename> file contains information about
      the 4 transformations referred to in the population modelling
      DAX.</para>

      <orderedlist>
        <listitem>
          <para>county_population_raster</para>
        </listitem>

        <listitem>
          <para>full_res_pop_raster</para>
        </listitem>

        <listitem>
          <para>raster_to_png</para>
        </listitem>

        <listitem>
          <para>animate</para>
        </listitem>
      </orderedlist>

      <para>The transformation catalog indicates that both transformations are
      of type STAGEABLE (i.e. can be staged along with the jobs) on the local
      site, and are compiled for x86_64 Linux.</para>
    </section>

    <section>
      <title id="tut_replica_catalog">The Replica Catalog</title>

      <para><emphasis role="bold">Note:</emphasis> Replica Catalog
      configuration is not required for the tutorial setup. It is only
      required if you want to refer to input files on external servers.</para>

      <para>The example that you ran, was configured with the inputs already
      present on the submit host (where Pegasus is installed) in a directory.
      If you have inputs at external servers, then you can specify the URLs to
      the input files in the Replica Catalog. This catalog tells Pegasus where
      to find each of the input files for the workflow.</para>

      <para>All files in a Pegasus workflow are referred to in the DAX using
      their Logical File Name (LFN). These LFNs are mapped to Physical File
      Names (PFNs) when Pegasus plans the workflow. This level of indirection
      enables Pegasus to map abstract DAXes to different execution sites and
      plan out the required file transfers automatically.</para>

      <para>The Replica Catalog for the diamond workflow is in the
      <filename>rc.txt</filename> file:</para>

      <programlisting>$ <emphasis role="bold">more rc.txt</emphasis>
# file-based replica catalog: 2018-07-06T14:20:38.300-07:00

county_cohort_pop_config.ini file:///local-scratch/home/pegtrainXX/population/scripts/config/county_cohort_pop_config.ini site="local"
geospatial.py file:///local-scratch/home/pegtrainXX/population/scripts/geospatial.py site="local"
</programlisting>

      <note>
        <para>By default (unless specified in properties), Pegasus picks ups
        the replica catalog from a text file named rc.txt in the current
        working directory from where pegasus-plan is invoked. In our tutorial,
        input files are on the submit host and we also use the --input dir
        option to pegasus-plan to specify where they are located.</para>
      </note>

      <para>This replica catalog contains couple of files for the population
      workflow. One of the entries is with a LFN of
      "county_cohort_pop_config.ini" and PFN as
      "ile:///local-scratch/home/pegtrainXX/population/scripts/config/county_cohort_pop_config.inl"
      and the file is stored on the local site, which implies that it will
      need to be transferred to the condorpool site when the workflow
      runs.</para>
    </section>
  </section>

  <section id="tutorial_configuration">
    <title>Configuring Pegasus</title>

    <para>In addition to the information catalogs, Pegasus takes a
    configuration file that specifies settings that control how it plans the
    workflow.</para>

    <para>For the diamond workflow, the Pegasus configuration file is
    relatively simple. It only contains settings to help Pegasus find the
    information catalogs. These settings are in the
    <filename>pegasus.properties</filename> file:</para>

    <programlisting>$ <emphasis role="bold">more pegasus.properties</emphasis>
# This tells Pegasus where to find the Site Catalog
pegasus.catalog.site.file=sites.xml

# This tells Pegasus where to find the Replica Catalog
pegasus.catalog.replica=File
pegasus.catalog.replica.file=rc.txt

# This tells Pegasus where to find the Transformation Catalog
pegasus.catalog.transformation=Text
pegasus.catalog.transformation.file=tc.txt

# Use staging site to transfer workflow data
pegasus.data.configuration=condorio

# This is the name of the application for analytics
pegasus.metrics.app=pegasus-tutorial
</programlisting>
  </section>

  <section id="tutorial_containers">
    <title>Containers</title>

    <para>So let us now go back to our failed workflow so far. As a recap, our
    workflow failed because there was no pandas python package
    installed.</para>

    <programlisting><emphasis role="bold">$ pegasus-analyzer /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0001</emphasis>

....
=======================county_population_raster_ID0000005=======================

 last state: POST_SCRIPT_FAILED
       site: condorpool
submit file: 00/00/county_population_raster_ID0000005.sub
output file: 00/00/county_population_raster_ID0000005.out.001
 error file: 00/00/county_population_raster_ID0000005.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : condorpool
hostname    : compute-6.isi.edu
executable  : /var/lib/condor/execute/dir_4383/county_population_raster
arguments   :   --config   county_cohort_pop_config.ini   --shapefile   SouthSudan_CountyPopulation.shp   --year   2018   --outfile   county_level_pop_2018.tif  
exitcode    : 1
working dir : /var/lib/condor/execute/dir_4383

-------Task #1 - county_population_raster - ID0000005 - Kickstart stderr--------

 Traceback (most recent call last):
  File "./county_population_raster", line 5, in &lt;module&gt;
    import pandas as pd
ImportError: No module named 'pandas'


=======================county_population_raster_ID0000002=======================

 last state: POST_SCRIPT_FAILED
       site: condorpool
submit file: 00/00/county_population_raster_ID0000002.sub
output file: 00/00/county_population_raster_ID0000002.out.001
 error file: 00/00/county_population_raster_ID0000002.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : condorpool
hostname    : compute-6.isi.edu
executable  : /var/lib/condor/execute/dir_4598/county_population_raster
arguments   :   --config   county_cohort_pop_config.ini   --shapefile   SouthSudan_CountyPopulation.shp   --year   2017   --outfile   county_level_pop_2017.tif  
exitcode    : 1
working dir : /var/lib/condor/execute/dir_4598

-------Task #1 - county_population_raster - ID0000002 - Kickstart stderr--------

 Traceback (most recent call last):
  File "./county_population_raster", line 5, in &lt;module&gt;
    import pandas as pd
ImportError: No module named 'pandas'

</programlisting>

    <para>Lets consider what options exist for us?</para>

    <orderedlist>
      <listitem>
        <para>We can contact system administrators on the remote sites to
        install these packages for us</para>
      </listitem>

      <listitem>
        <para>We can install these packages in our user accounts ourselves
        without root access</para>
      </listitem>
    </orderedlist>

    <para>If we follow either of these two options, you may have to update the
    PYTHONPATH variable that your jobs see to pick up a custom pandas
    install</para>

    <para>You can do by specify "env" profiles either in the Site Catalog for
    site <emphasis role="bold">"condorpool"</emphasis> or for the individual
    executables in the Transformation Catalog. If all your jobs require same
    packages then the Site Catalog is a good option to specify, as Pegasus
    will ensure all jobs running on site <emphasis role="bold">"condorpool"
    </emphasis>. However, there are can be other dependencies that may
    exist.</para>

    <para>Application containers provides a solution to package software with
    complex dependencies to be used during workflow execution. Starting with
    Pegasus 4.8.0, Pegasus has support for application containers in the
    non-shared filesystem or condorio data configurations using PegasusLite.
    Users can specify with their transformations in the Transformation Catalog
    the container in which the the transformation should be executed. Pegasus
    currently has support for the following container technologies:</para>

    <orderedlist>
      <listitem>
        <para>Docker</para>
      </listitem>

      <listitem>
        <para>Singularity</para>
      </listitem>
    </orderedlist>

    <para>In this tutorial, we are going to use Singularity to package our
    application dependences and environment. For the tutorial purposes, there
    is a Singulartiy container (<emphasis
    role="bold">pegasus-isi/darpa_population_modeling</emphasis> ) already
    published to the Singularity Hub which we will use. The container
    specification used to build this container can be found in the file
    Singularity in your tutorial directory.</para>

    <programlisting>$ pwd
$ cat Singularity

# we are boostrapping from a docker base image in docker hub
bootstrap:docker
From:ubuntu:xenial

# Note: python3 is used for this project
%post

apt-get update &amp;&amp; apt-get upgrade -y

apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        build-essential \
        curl \
        gdal-bin \
        gsfonts \
        imagemagick \
        libfreetype6-dev \
        libgdal-dev \
        libpng12-dev \
        libzmq3-dev \
        lsb-release \
        module-init-tools \
        openjdk-8-jdk \
        pkg-config \
        python3 \
        python3-dev \
        python3-pip \
        rsync \
        unzip \
        vim \
        wget

apt-get clean 
rm -rf /var/lib/apt/lists/*

pip3 install --upgrade pip
pip3 install --upgrade setuptools

pip3 install typing
pip3 install numpy
export CFLAGS=$(gdal-config --cflags) &amp;&amp; pip3 install GDAL==$(gdal-config --version | awk -F'[.]' '{print $1"."$2}')
pip3 install pandas
pip3 install geopandas
pip3 install rasterio

</programlisting>

    <section id="tutorial-containers-conf">
      <title>Configuring Workflows To Use Containers</title>

      <para>Containers currently can only be specified in the Transformation
      Catalog. Users have the option of either using a different container for
      each executable or same container for all executables. In the case,
      where you wants to use a container that does not have your executable
      pre-installed, you can mark the executable as STAGEABLE and Pegasus will
      stage the executable into the container, as part of executable
      staging.</para>

      <programlisting><emphasis role="bold">$ more tc.txt.containers</emphasis>
# image comes from Singularity Hub
cont gis {
    # can be either docker or singularity
    type "singularity"

    # URL to image in a docker|singularity hub OR
    # URL to an existing docker image exported as a tar file or singularity image
    image "shub://pegasus-isi/darpa_population_modeling"
}

tr county_population_raster {
    site local {
        type "STAGEABLE"
        container "gis"
        arch "x86_64"
        os "LINUX"
        pfn "file:///local-scratch/home/pegtrainXX/population/scripts/county_population_raster.py"
    }
}
</programlisting>

      <para>The container itself is defined using the cont entry. Multiple
      transformations can refer to the same container.</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">cont</emphasis> cont - A container
          identifier.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">image</emphasis> - URL to image in a
          docker|singularity hub or URL to an existing docker image exported
          as a tar file or singularity image. Example of a docker hub URL is
          docker:///rynge/montage:latest, while for singularity
          shub://pegasus-isi/fedora-montage</para>
        </listitem>
      </orderedlist>

      <para>The first entry describes the container, where the image can be
      found (Singularity Hub in this example). The second entry, of which
      there are many more similar ones in the file, describes the application.
      Note how it refers back to the "gis" container, specifying that we want
      the job to be wrapped in the container.</para>
    </section>
  </section>

  <section id="tutorial_containers_wf">
    <title>Executing a workflow with Containers</title>

    <para>Now we will submit the same workflow using containers . To do that,
    lets copy the tc.txt.containers file to tc.txt and plan the workflow
    again.</para>

    <programlisting><emphasis role="bold">$ cp tc.txt.containers tc.txt</emphasis>


<emphasis role="bold">$ ./plan_dax.sh pop.dax </emphasis>
2018.07.06 11:10:33.224 PDT:    
2018.07.06 11:10:33.229 PDT:   ----------------------------------------------------------------------- 
2018.07.06 11:10:33.235 PDT:   File for submitting this DAG to HTCondor           : population-0.dag.condor.sub 
2018.07.06 11:10:33.240 PDT:   Log of DAGMan debugging messages                 : population-0.dag.dagman.out 
2018.07.06 11:10:33.245 PDT:   Log of HTCondor library output                     : population-0.dag.lib.out 
2018.07.06 11:10:33.250 PDT:   Log of HTCondor library error messages             : population-0.dag.lib.err 
2018.07.06 11:10:33.255 PDT:   Log of the life of condor_dagman itself          : population-0.dag.dagman.log 
2018.07.06 11:10:33.261 PDT:    
2018.07.06 11:10:33.266 PDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: 
2018.07.06 11:10:33.276 PDT:   ----------------------------------------------------------------------- 
2018.07.06 11:10:35.892 PDT:   Your database is compatible with Pegasus version: 4.8.3dev 
2018.07.06 11:10:36.078 PDT:   Submitting to condor population-0.dag.condor.sub 
2018.07.06 11:10:36.171 PDT:   Submitting job(s). 
2018.07.06 11:10:36.177 PDT:   1 job(s) submitted to cluster 992416. 
2018.07.06 11:10:36.182 PDT:    
2018.07.06 11:10:36.187 PDT:   Your workflow has been started and is running in the base directory: 
2018.07.06 11:10:36.192 PDT:    
2018.07.06 11:10:36.198 PDT:     /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0002 
2018.07.06 11:10:36.203 PDT:    
2018.07.06 11:10:36.208 PDT:   *** To monitor the workflow you can run *** 
2018.07.06 11:10:36.213 PDT:    
2018.07.06 11:10:36.218 PDT:     pegasus-status -l /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0002 
2018.07.06 11:10:36.224 PDT:    
2018.07.06 11:10:36.229 PDT:   *** To remove your workflow run *** 
2018.07.06 11:10:36.234 PDT:    
2018.07.06 11:10:36.239 PDT:     pegasus-remove /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0002 
2018.07.06 11:10:36.245 PDT:    
2018.07.06 11:10:36.345 PDT:   Time taken to execute is 5.797 seconds </programlisting>

    <para>We will now monitor the workflow using the pegasus-status command
    till it fails. We will add -w option to pegasus-status to watch
    automatically till the workflow finishes:</para>

    <programlisting><emphasis role="bold">$ </emphasis><emphasis role="bold">pegasus-status -w submit/pegtrainXX/pegasus/population/run0002</emphasis>
STAT  IN_STATE  JOB                                                                                                    
Run      03:00  population-0 ( /local-scratch/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0002 )
Summary: 1 Condor job total (R:1)

UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    0     0     0     0     0    25     0 100.0 Success *population-0.dag       
</programlisting>

    <para>You can check the outputs created in the output directory</para>

    <programlisting><emphasis role="bold">$ ls -lht output/</emphasis>
total 50M
-rw-r--r-- 1 pegtrainXX ccg-external 1.3M Jul  6 13:29 animation.gif
-rw-r--r-- 1 pegtrainXX ccg-external 795K Jul  6 13:29 pop_dist_2019.png
-rw-r--r-- 1 pegtrainXX ccg-external 801K Jul  6 13:29 pop_dist_2020.png
-rw-r--r-- 1 pegtrainXX ccg-external 785K Jul  6 13:29 pop_dist_2018.png
-rw-r--r-- 1 pegtrainXX ccg-external 778K Jul  6 13:29 pop_dist_2017.png
-rw-r--r-- 1 pegtrainXX ccg-external 5.8M Jul  6 13:28 pop_dist_2019.tif
-rw-r--r-- 1 pegtrainXX ccg-external 5.8M Jul  6 13:28 pop_dist_2020.tif
-rw-r--r-- 1 pegtrainXX ccg-external 5.8M Jul  6 13:28 pop_dist_2018.tif
-rw-r--r-- 1 pegtrainXX ccg-external 5.8M Jul  6 13:28 pop_dist_2017.tif
-rw-r--r-- 1 pegtrainXX ccg-external 5.8M Jul  6 13:28 county_level_pop_2019.tif
-rw-r--r-- 1 pegtrainXX ccg-external 5.8M Jul  6 13:28 county_level_pop_2020.tif
-rw-r--r-- 1 pegtrainXX ccg-external 5.8M Jul  6 13:28 county_level_pop_2018.tif
-rw-r--r-- 1 pegtrainXX ccg-external 5.8M Jul  6 13:28 county_level_pop_2017.tif</programlisting>

    <para>To see how Pegasus handled the container in this case, let’s look at
    some plumbing for one of the mProject job. The HTCondor submit file can be
    seen with:</para>

    <programlisting><emphasis role="bold">$ cd submit/pegtrainXX/pegasus/population/run0002
$  cat `find . -name county_population_raster_ID0000002.sub`</emphasis></programlisting>

    <para>Look at the transfer_input_files attribute line, and specifically
    for the gis.simg file. It is transferred together with all the other
    inputs for the job:</para>

    <programlisting>transfer_input_files = SouthSudan_CountyPopulation.shx,county_cohort_pop_config.ini,county_population_raster,geospatial.py,SouthSudan_CountyPopulation.dbf,
                       SouthSudan_CountyPopulation.sbx,SouthSudan_CountyPopulation.prj,SouthSudan_CountyPopulation.cpg,SouthSudan_CountyPopulation.shp,
                       SouthSudan_CountyPopulation.sbn,<emphasis role="bold">gis.simg</emphasis>,/opt/training/pegasus-4.8.3dev/share/pegasus/sh/pegasus-lite-common.sh,
                       /scitech/home/pegtrain01/population/submit/pegtrain01/pegasus/population/run0002/pegasus-worker-4.8.3dev-x86_64_rhel_7.tar.gz
</programlisting>

    <para>Looking at the corresponding .sh file we can see how Pegasus
    executed the container:</para>

    <programlisting>$ cat `find . -name county_population_raster_ID0000002.sh`

...

singularity exec --pwd /srv --home $PWD:/srv gis.simg ./county_population_raster_ID0000002-cont.sh 
...</programlisting>

    <para>The county_population_raster_ID0000002-cont.sh is a script generated
    at runtime, containing the execution of the user codes.</para>
  </section>

  <section id="tutorial_statistics">
    <title>Collecting statistics about a workflow run</title>

    <para>The <literal>pegasus-statistics</literal> command can be used to
    gather statistics about the runtime of the workflow and its jobs. The
    <literal>-s all</literal> argument tells the program to generate all
    statistics it knows how to calculate:</para>

    <programlisting><emphasis role="bold">$  cd ~/population
$  pegasus-statistics -s all submit/pegtrainXX/pegasus/population/run0002</emphasis>

#
# Pegasus Workflow Management System - http://pegasus.isi.edu
#
# Workflow summary:
#   Summary of the workflow execution. It shows total
#   tasks/jobs/sub workflows run, how many succeeded/failed etc.
#   In case of hierarchical workflow the calculation shows the
#   statistics across all the sub workflows.It shows the following
#   statistics about tasks, jobs and sub workflows.
#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.
#     * Failed - total count of failed tasks/jobs/sub workflows.
#     * Incomplete - total count of tasks/jobs/sub workflows that are
#       not in succeeded or failed state. This includes all the jobs
#       that are not submitted, submitted but not completed etc. This
#       is calculated as  difference between 'total' count and sum of
#       'succeeded' and 'failed' count.
#     * Total - total count of tasks/jobs/sub workflows.
#     * Retries - total retry count of tasks/jobs/sub workflows.
#     * Total+Retries - total count of tasks/jobs/sub workflows executed
#       during workflow run. This is the cumulative of retries,
#       succeeded and failed count.
# Workflow wall time:
#   The wall time from the start of the workflow execution to the end as
#   reported by the DAGMAN.In case of rescue dag the value is the
#   cumulative of all retries.
# Cumulative job wall time:
#   The sum of the wall time of all jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job wall time as seen from submit side:
#   The sum of the wall time of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
# Cumulative job badput wall time:
#   The sum of the wall time of all failed jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job badput wall time as seen from submit side:
#   The sum of the wall time of all failed jobs as reported by DAGMan.
#   This is similar to the regular cumulative job badput wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
------------------------------------------------------------------------------
Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries
Tasks          13        0       0           13        0         13           
Jobs           28        0       0           28        0         28           
Sub-Workflows  0         0       0           0         0         0            
------------------------------------------------------------------------------

Workflow wall time                                       : 3 mins, 20 secs
Cumulative job wall time                                 : 1 min, 26 secs
Cumulative job wall time as seen from submit side        : 3 mins, 0 secs
Cumulative job badput wall time                          : 
Cumulative job badput wall time as seen from submit side : 



Summary                       : submit/pegtrainXX/pegasus/population/run0002/statistics/summary.txt
Workflow execution statistics : submit/pegtrainXX/pegasus/population/run0002/statistics/workflow.txt
Job instance statistics       : submit/pegtrainXX/pegasus/population/run0002/statistics/jobs.txt
Transformation statistics     : submit/pegtrainXX/pegasus/population/run0002/statistics/breakdown.txt
Time statistics               : submit/pegtrainXX/pegasus/population/run0002/statistics/time.txt</programlisting>

    <para>The output of <literal>pegasus-statistics</literal> contains many
    definitions to help users understand what all of the values reported mean.
    Among these are the total wall time of the workflow, which is the time
    from when the workflow was submitted until it finished, and the total
    cumulative job wall time, which is the sum of the runtimes of all the
    jobs.</para>

    <para>The <literal>pegasus-statistics</literal> command also writes out
    several reports in the <filename>statistics</filename> subdirectory of the
    workflow submit directory:</para>

    <programlisting>$ <emphasis role="bold">ls submit/pegtrainXX/pegasus/population/run0002/statistics/</emphasis>
jobs.txt          summary.txt         time.txt          breakdown.txt          workflow.txt</programlisting>

    <para>The file <filename>breakdown.txt</filename>, for example, has min,
    max, and mean runtimes for each transformation:</para>

    <programlisting>$ <emphasis role="bold">more </emphasis> <emphasis
        role="bold">submit/pegtrainXX/pegasus/population/run0001/statistics/breakdown.txt 
</emphasis>
# Transformation - name of the transformation.
# Count          - the number of times the invocations corresponding to
#                  the transformation was executed.
# Succeeded      - the count of the succeeded invocations corresponding
#                  to the transformation.
# Failed         - the count of the failed invocations corresponding to
#                  the transformation.
# Min(sec)       - the minimum invocation runtime value corresponding
#                  to the transformation.
# Max(sec)       - the maximum invocation runtime value corresponding
#                  to the transformation.
# Mean(sec)      - the mean of the invocation runtime corresponding
#                  to the transformation.
# Total(sec)     - the cumulative of invocation runtime corresponding
#                  to the transformation.

# 579b97fd-c51f-4170-9213-903a27f74759 (population)
Transformation           Count     Succeeded Failed  Min       Max       Mean           Total     
animate                  1         1         0       2.268     2.268     2.268          2.268     
county_population_raster 4         4         0       5.098     5.716     5.434          21.738    
dagman::post             27        27        0       0.0       1.0       0.296          8.0       
full_res_pop_raster      4         4         0       2.041     3.297     2.815          11.258    
pegasus::cleanup         1         1         0       2.0       2.0       2.0            2.0       
pegasus::dirmanager      1         1         0       2.0       2.0       2.0            2.0       
pegasus::rc-client       4         4         0       0.344     0.369     0.352          1.41      
pegasus::transfer        9         9         0       2.152     7.229     4.063          36.571    
raster_to_png            4         4         0       2.261     2.363     2.325          9.298     


# All (All)
Transformation           Count     Succeeded Failed  Min       Max       Mean           Total     
animate                  1         1         0       2.268     2.268     2.268          2.268     
county_population_raster 4         4         0       5.098     5.716     5.434          21.738    
dagman::post             27        27        0       0.0       1.0       0.296          8.0       
full_res_pop_raster      4         4         0       2.041     3.297     2.815          11.258    
pegasus::cleanup         1         1         0       2.0       2.0       2.0            2.0       
pegasus::dirmanager      1         1         0       2.0       2.0       2.0            2.0       
pegasus::rc-client       4         4         0       0.344     0.369     0.352          1.41      
pegasus::transfer        9         9         0       2.152     7.229     4.063          36.571    
raster_to_png            4         4         0       2.261     2.363     2.325          9.298     </programlisting>
  </section>

  <section id="tutorial_clustering">
    <title>Clustering Short Running Tasks</title>

    <para>Often, users have lots of short running single processor jobs in
    their workflow, that if submitted individually to the compute
    infrastructure take a long time to execute, as each job sits in the job
    management system's queue. For example in our previous example, each job
    in the population modelling workflow actually runs for less than a minute
    each. This is not ideal, as each job is submitted individually to OSG and
    needs to be matched to a node before it can be executed. In order to
    alleviate this, it makes sense to cluster the short running jobs together.
    Pegasus allows users to cluster tasks in their workflow into larger
    chunks, and then execute them using a sequential tool called <emphasis
    role="bold">pegasus-cluster</emphasis> or MPI based master worker tool
    called <emphasis><emphasis
    role="bold">pegasus-mpi-cluster</emphasis></emphasis> .</para>

    <para>In this exercise we will re-run the split workflow with job
    clustering turned on and use pegasus-cluster to run the clustered job on
    the remote node. We will cluster parallel jobs on the same level of the
    workflow into larger chunks.</para>

    <para>In order to tell Pegasus to cluster the jobs we have to do the
    following</para>

    <orderedlist>
      <listitem>
        <para>Tell Pegasus how many clustered jobs per level of the workflow
        to create. We usually do it by setting a profile in the site
        catalog.</para>

        <programlisting>   &lt;site handle="condorpool" arch="x86_64" os="LINUX"&gt;
        &lt;!-- These profiles tell Pegasus that the site is a plain Condor pool --&gt;
        &lt;profile namespace="pegasus" key="style"&gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt;

       &lt;!-- This ensures that when a job runs enough memory and 
             diskspace available to stage and run the container --&gt;
        &lt;profile namespace="condor" key="request_memory" &gt;2  GB&lt;/profile&gt;
        &lt;profile namespace="condor" key="request_disk" &gt;5 GB&lt;/profile&gt;
 
       <emphasis role="bold">&lt;!-- This profile tells Pegasus to create two clustered jobs
            per level of the workflow, when horizontal clustering is
            enabled --&gt;</emphasis>
       &lt;profile namespace="pegasus" key="clusters.num" &gt;2&lt;/profile&gt;
    &lt;/site&gt;

</programlisting>
      </listitem>

      <listitem>
        <para>Tell pegasus that it has to do job clustering</para>

        <para>To do this, while planning the workflow we add <emphasis
        role="bold">--cluster </emphasis>option to pegasus-plan. That is what
        we have in plan_cluster_dax.sh file.</para>

        <programlisting>$ <emphasis role="bold">cat plan_cluster_dax.sh</emphasis>

#!/bin/sh

set -e
...

# This command tells Pegasus to plan the workflow contained in 
# dax file passed as an argument. The planned workflow will be stored
# in the "submit" directory. The execution # site is "".
# --input-dir tells Pegasus where to find workflow input files.
# --output-dir tells Pegasus where to place workflow output files.
pegasus-plan --conf pegasus.properties \
    --dax $DAXFILE \
    --dir $DIR/submit \
    --input-dir $DIR/input \
    --output-dir $DIR/output \
    --cleanup leaf \
    <emphasis role="bold">--cluster horizontal \</emphasis>
    --force \
    --sites osg \
    --staging-site isi_workflow \
    --submit
</programlisting>
      </listitem>
    </orderedlist>

    <para><emphasis role="bold">Let us now plan and run the
    workflow.</emphasis></para>

    <programlisting><emphasis role="bold">$  ./plan_cluster_dax.sh pop.dax </emphasis>
2018.07.06 12:16:42.464 PDT:    
2018.07.06 12:16:42.470 PDT:   ----------------------------------------------------------------------- 
2018.07.06 12:16:42.475 PDT:   File for submitting this DAG to HTCondor           : population-0.dag.condor.sub 
2018.07.06 12:16:42.480 PDT:   Log of DAGMan debugging messages                 : population-0.dag.dagman.out 
2018.07.06 12:16:42.486 PDT:   Log of HTCondor library output                     : population-0.dag.lib.out 
2018.07.06 12:16:42.491 PDT:   Log of HTCondor library error messages             : population-0.dag.lib.err 
2018.07.06 12:16:42.496 PDT:   Log of the life of condor_dagman itself          : population-0.dag.dagman.log 
2018.07.06 12:16:42.501 PDT:    
2018.07.06 12:16:42.507 PDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: 
2018.07.06 12:16:42.517 PDT:   ----------------------------------------------------------------------- 
2018.07.06 12:16:43.230 PDT:   Your database is compatible with Pegasus version: 4.8.3dev 
2018.07.06 12:16:43.328 PDT:   Submitting to condor population-0.dag.condor.sub 
2018.07.06 12:16:43.357 PDT:   Submitting job(s). 
2018.07.06 12:16:43.362 PDT:   1 job(s) submitted to cluster 992488. 
2018.07.06 12:16:43.367 PDT:    
2018.07.06 12:16:43.373 PDT:   Your workflow has been started and is running in the base directory: 
2018.07.06 12:16:43.378 PDT:    
2018.07.06 12:16:43.383 PDT:     /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0003 
2018.07.06 12:16:43.388 PDT:    
2018.07.06 12:16:43.393 PDT:   *** To monitor the workflow you can run *** 
2018.07.06 12:16:43.399 PDT:    
2018.07.06 12:16:43.404 PDT:     pegasus-status -l /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0003 
2018.07.06 12:16:43.409 PDT:    
2018.07.06 12:16:43.414 PDT:   *** To remove your workflow run *** 
2018.07.06 12:16:43.419 PDT:    
2018.07.06 12:16:43.425 PDT:     pegasus-remove /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0003 </programlisting>

    <para>This is what the population workflow looks like after Pegasus has
    finished planning the DAX:</para>

    <figure>
      <title>Clustered Population DAG</title>

      <mediaobject>
        <imageobject>
          <imagedata contentwidth="70%"
                     fileref="images/tutorial-clustered-population-dag.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>You can see that instead of 4 jobs for county_population_raster, we
    two merge_county_population_raster jobs. Each of these jobs runs two
    county_population_raster invocations one after the other using tool called
    pegasus-cluster. Similarly for the other jobs full_res_pop_raster and
    raster_to_png jobs.</para>
  </section>

  <section id="tutorial_failure_recovery">
    <title>Recovery from Failures</title>

    <para>Executing workflows in a distributed environment can lead to
    failures. Often, they are a result of the underlying infrastructure being
    temporarily unavailable, or errors in workflow setup such as incorrect
    executables specified, or input files being unavailable.</para>

    <para>In case of transient infrastructure failures such as a node being
    temporarily down in a cluster, Pegasus will automatically retry jobs in
    case of failure. After a set number of retries (usually once), a hard
    failure occurs, because of which workflow will eventually fail.</para>

    <para>In most of the cases, these errors are correctable (either the
    resource comes back online or application errors are fixed). Once the
    errors are fixed, you may not want to start a new workflow but instead
    start from the point of failure. In order to do this, you can submit the
    rescue workflows automatically created in case of failures. A rescue
    workflow contains only a description of only the work that remains to be
    done.</para>

    <section>
      <title>Submitting Rescue Workflows</title>

      <para>In this example, we will take our previously run workflow and
      introduce errors such that workflow we just executed fails at
      runtime.</para>

      <para>First we will "hide" the input file to cause a failure by renaming
      it:</para>

      <programlisting><emphasis role="bold">$ mv scripts/geospatial.py scripts/geospatial.py.bak
      </emphasis></programlisting>

      <para>Now submit the workflow again:</para>

      <programlisting><emphasis role="bold">$ </emphasis>.<emphasis
          role="bold">/plan_dax.sh pop.dax </emphasis>
2018.07.06 13:48:20.806 PDT:    
2018.07.06 13:48:20.812 PDT:   ----------------------------------------------------------------------- 
2018.07.06 13:48:20.817 PDT:   File for submitting this DAG to HTCondor           : population-0.dag.condor.sub 
2018.07.06 13:48:20.822 PDT:   Log of DAGMan debugging messages                 : population-0.dag.dagman.out 
2018.07.06 13:48:20.827 PDT:   Log of HTCondor library output                     : population-0.dag.lib.out 
2018.07.06 13:48:20.832 PDT:   Log of HTCondor library error messages             : population-0.dag.lib.err 
2018.07.06 13:48:20.838 PDT:   Log of the life of condor_dagman itself          : population-0.dag.dagman.log 
2018.07.06 13:48:20.843 PDT:    
2018.07.06 13:48:20.848 PDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: 
2018.07.06 13:48:20.858 PDT:   ----------------------------------------------------------------------- 
2018.07.06 13:48:21.593 PDT:   Your database is compatible with Pegasus version: 4.8.3dev 
2018.07.06 13:48:21.690 PDT:   Submitting to condor population-0.dag.condor.sub 
2018.07.06 13:48:21.724 PDT:   Submitting job(s). 
2018.07.06 13:48:21.729 PDT:   1 job(s) submitted to cluster 992607. 
2018.07.06 13:48:21.734 PDT:    
2018.07.06 13:48:21.740 PDT:   Your workflow has been started and is running in the base directory: 
2018.07.06 13:48:21.745 PDT:    
2018.07.06 13:48:21.750 PDT:     /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0004 
2018.07.06 13:48:21.756 PDT:    
2018.07.06 13:48:21.761 PDT:   *** To monitor the workflow you can run *** 
2018.07.06 13:48:21.766 PDT:    
2018.07.06 13:48:21.771 PDT:     pegasus-status -l /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0004 
2018.07.06 13:48:21.776 PDT:    
2018.07.06 13:48:21.782 PDT:   *** To remove your workflow run *** 
2018.07.06 13:48:21.787 PDT:    
2018.07.06 13:48:21.792 PDT:     pegasus-remove /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0004 
2018.07.06 13:48:21.797 PDT:    
2018.07.06 13:48:21.857 PDT:   Time taken to execute is 2.054 seconds </programlisting>

      <para>We will now monitor the workflow using the pegasus-status command
      till it fails. We will add -w option to pegasus-status to watch
      automatically till the workflow finishes:</para>

      <programlisting><emphasis role="bold">$ </emphasis><emphasis role="bold">pegasus-status -w submit/pegtrainXX/pegasus/population/run0004</emphasis>
(no matching jobs found in Condor Q)
UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
     22       0       0       0       0       5       1  17.9
Summary: 1 DAG total (Failure:1)
[pegtrainXX@workflow population]$ 
</programlisting>

      <para>Now we can use the pegasus-analyzer command to determine what went
      wrong:</para>

      <programlisting><emphasis role="bold">$ </emphasis> <emphasis
          role="bold">pegasus-analyzer submit/pegtrainXX/pegasus/population/run0004</emphasis>


************************************Summary*************************************

 Submit Directory   : submit/pegtrainXX/pegasus/population/run0004
 Total jobs         :     28 (100.00%)
 # jobs succeeded   :      5 (17.86%)
 # jobs failed      :      1 (3.57%)
 # jobs held        :      0 (0.00%)
 # jobs unsubmitted :     22 (78.57%)

******************************Failed jobs' details******************************

===========================stage_in_remote_local_0_0============================

 last state: POST_SCRIPT_FAILED
       site: local
submit file: 00/00/stage_in_remote_local_0_0.sub
output file: 00/00/stage_in_remote_local_0_0.out.001
 error file: 00/00/stage_in_remote_local_0_0.err.001

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : workflow.isi.edu
executable  : /local-scratch/vahi/software/install/pegasus/pegasus-4.8.3dev/bin/pegasus-transfer
arguments   :   --threads   2  
exitcode    : 1
working dir : /local-scratch/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0004

------------------Task #1 - pegasus::transfer - None - stdout-------------------

2018-07-06 13:52:16,216    INFO:  Reading URL pairs from stdin
2018-07-06 13:52:16,219    INFO:  10 transfers loaded
2018-07-06 13:52:16,219    INFO:  PATH=/usr/bin:/bin
2018-07-06 13:52:16,219    INFO:  LD_LIBRARY_PATH=
2018-07-06 13:52:16,259    INFO:  --------------------------------------------------------------------------------
2018-07-06 13:52:16,259    INFO:  Starting transfers - attempt 1
2018-07-06 13:52:18,270    INFO:  /bin/cp -f -R -L '/local-scratch/home/pegtrainXX/population/scripts/config/county_cohort_pop_config.ini' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./county_cohort_pop_config.ini'
2018-07-06 13:52:18,280    INFO:  /bin/cp -f -R -L '/local-scratch/home/pegtrainXX/population/scripts/county_population_raster.py' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./county_population_raster'
2018-07-06 13:52:18,289   ERROR:  Expected local file does not exist: /local-scratch/home/pegtrainXX/population/scripts/geospatial.py
2018-07-06 13:52:18,289    INFO:  /bin/cp -f -R -L '/scitech/home/pegtrainXX/population/input/SouthSudan_CountyPopulation.cpg' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./SouthSudan_CountyPopulation.cpg'
2018-07-06 13:52:18,298    INFO:  /bin/cp -f -R -L '/scitech/home/pegtrainXX/population/input/SouthSudan_CountyPopulation.dbf' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./SouthSudan_CountyPopulation.dbf'
2018-07-06 13:52:18,331    INFO:  /bin/cp -f -R -L '/scitech/home/pegtrainXX/population/input/SouthSudan_CountyPopulation.sbn' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./SouthSudan_CountyPopulation.sbn'
2018-07-06 13:52:18,340    INFO:  /bin/cp -f -R -L '/scitech/home/pegtrainXX/population/input/SouthSudan_CountyPopulation.sbx' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./SouthSudan_CountyPopulation.sbx'
2018-07-06 13:52:18,349    INFO:  /bin/cp -f -R -L '/scitech/home/pegtrainXX/population/input/SouthSudan_CountyPopulation.shp' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./SouthSudan_CountyPopulation.shp'
2018-07-06 13:52:18,365    INFO:  /bin/cp -f -R -L '/scitech/home/pegtrainXX/population/input/SouthSudan_CountyPopulation.shx' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./SouthSudan_CountyPopulation.shx'
2018-07-06 13:52:18,373    INFO:  /bin/cp -f -R -L '/scitech/home/pegtrainXX/population/input/SouthSudan_CountyPopulation.prj' '/local-scratch/home/pegtrainXX/population/scratch/pegtrainXX/pegasus/population/run0004/./SouthSudan_CountyPopulation.prj'
2018-07-06 13:53:00,296    INFO:  --------------------------------------------------------------------------------
2018-07-06 13:53:00,296    INFO:  Starting transfers - attempt 2
2018-07-06 13:53:02,306   ERROR: <emphasis role="bold"> Expected local file does not exist: /local-scratch/home/pegtrainXX/population/scripts/geospatial.py</emphasis>
2018-07-06 13:55:24,342    INFO:  --------------------------------------------------------------------------------
2018-07-06 13:55:24,342    INFO:  Starting transfers - attempt 3
2018-07-06 13:55:26,353   ERROR:  E<emphasis role="bold">xpected local file does not exist: /local-scratch/home/pegtrainXX/population/scripts/geospatial.py</emphasis>
2018-07-06 13:55:26,353    INFO:  --------------------------------------------------------------------------------
2018-07-06 13:55:26,353    INFO:  Stats: Total 12 transfers, 309.2 KB transferred in 190 seconds. Rate: 1.6 KB/s (13.0 Kb/s)
2018-07-06 13:55:26,353    INFO:         Between sites local-&gt;local : 12 transfers, 309.2 KB transferred in 190 seconds. Rate: 1.6 KB/s (13.0 Kb/s)
2018-07-06 13:55:26,353 CRITICAL:  Some transfers failed! See above, and possibly stderr.
</programlisting>

      <para>The above listing indicates that it could not transfer
      geospatial.py. Let's correct that error by restoring the pegasus.html
      file:</para>

      <programlisting><emphasis role="bold">$ mv scripts/geospatial.py.bak scripts/geospatial.py
      </emphasis></programlisting>

      <para>Now in order to start the workflow from where we left off, instead
      of executing pegasus-plan we will use the command pegasus-run on the
      directory from our previous failed workflow run:</para>

      <programlisting>$  <emphasis role="bold">pegasus-run submit/pegtrainXX/pegasus/population/run00024</emphasis>
Rescued /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0004/population-0.log as /scitech/home/pegtrainXX/population/submit/pegtrainXX/pegasus/population/run0004/population-0.log.000
Submitting to condor population-0.dag.condor.sub
Submitting job(s).
1 job(s) submitted to cluster 992625.

Your workflow has been started and is running in the base directory:

  submit/pegtrainXX/pegasus/population/run0004

*** To monitor the workflow you can run ***

  pegasus-status -l submit/pegtrainXX/pegasus/population/run0004

*** To remove your workflow run ***

  pegasus-remove submit/pegtrainXX/pegasus/population/run0004</programlisting>

      <para>The workflow will now run to completion and succeed.</para>

      <programlisting><emphasis role="bold">$ pegasus-status -l </emphasis><emphasis
          role="bold">submit/pegtrainXX/pegasus/population/run00024</emphasis>
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    0     0     0     0     0    28     0 100.0 Success *population-0.dag   
[pegtrainXX@workflow population]$ 

                     
</programlisting>
    </section>
  </section>

  <section id="tutorial_conclusion">
    <title>Conclusion</title>

    <para>This brings you to the end of the Pegasus tutorial. The tutorial
    should have given you an overview of how to compose a simple workflow
    using Pegasus and running it on the Open Science Grid. The tutorial
    examples, should provide a good starting point for you to port your
    application to a Pegasus workflow. If you need help in porting your
    application to Pegasus contact us on the following support channels</para>

    <para>public mailman list : pegasus-users@isi.edu</para>

    <para>private support list: pegasus-support@isi.edu</para>

    <para>Detailed Pegasus Documentation can be found <ulink
    url="http://pegasus.isi.edu/wms/docs/latest/">here</ulink>.</para>
  </section>
</chapter>
